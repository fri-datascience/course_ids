[
["index.html", "Introduction to data science Preface", " Introduction to data science Slavko Žitnik and Erik Štrumbelj 2020-09-21 Preface These are the course notes for the Introduction to data science course of the Data Science Master’s at University of Ljubljana, Faculty of computer and information science. The authors would like to thank the people who contributed to these notes with their reviews, comments and suggestions: Tomaž Curk, Janez Demšar, Jure Demšar, Dejan Lavbič, Matjaž Pančur, Gregor Pirš, Marko Robnik Šikonja. "],
["what-is-data-science-anyway.html", "What is data science anyway? 0.1 The role of this course", " What is data science anyway? There is no generally accepted definition of what data science is. We would probably all agree that it is a relatively new term and that it is field that requires a mix of IT, analytics and soft skills. Our view is that data science is synonymous with empirical science and that this is a result of two major changes. First, the evolution of empirical science to a level that now requires a much broader skill set. And second, the evolution of business, industry and other non-scientific areas to a level where more and more processes require treatment at the level of empirical science. Examples of empirical science go back as far as recorded history itself. From censuses in ancient Egypt that were used to scale the pyramid labor force to weather forecasting to profit from olive oil futures in ancient Greece. Historically, even as little as 100 years ago, empirical science was much more difficult than it is today (not that it is not still very difficult if done properly). Data were recorded and manipulated by hand. If one was not an expert mathematician and well-equipped with logarithm and quantile tables, it was near-impossible to do even the most basic statistical analyses. And to spread the results, one would write their report and plot their graphs by hand. Over time, this skill set has evolved, with computers playing the leading role. Pens were replaced by typsetting software. Graphing paper was replaced by visualization packages. And logarithm tables were replaced by computer code. Indeed, a lot of the brain-intensive mathematics gave way to computationally intensive numerical approaches, both out of convenience and out of necessity. On the other hand, progress also brought new challenges and raised standards in data management, software development, reproducible research and visualization. The names that describe people with this skill set have also evolved. From scientist, empirical scientist, researcher, and statistician to more cool names, such as quantitative analyst, quant, predictive analyst, data miner, machine learning expert and now data scientist. Tomorrow, data science will probably be replaced by another even catchier term. But the core skills will remain the same: analytical thinking, attention to detail, communication and domain-specific knowledge. Mathematics and computation. Science. Like the skill set itself the demand for it has also always been present. And growing. To the point where we now live in an increasingly data-driven world: all empirical sciences, finance, marketing, sports training, politics, HR and medicine, to name just a few, rely on data. Today, the practical reality is that there is a big shortage of people with such skills or even just some of them. This is great opportunity for those considering to become data scientists, but also a challenge for educational institutions on how to best prepare students for a career in this data-driven world. 0.1 The role of this course This course aims at breadth not depth. The goal is to familiarize ourselves with all key ingredients of data science from a practitioners perspective. We will learn enough methods, techniques and tools to deal with the vast majority of data science scenarios we encounter in real life. However, only with practice and through more academic courses will we be able to develop a fundamental understanding of some of these methods and refine their use to a professional standard. That is, this course focuses on doing. With time, study, practice and through mistakes, we will learn how to do better. So what do we think are the key ingredients of a data science practitioner? The first and most important is computer programming. Data science requires data manipulation and computation that can not be done without a computer. Working with data also represents most of the workload in a typical data science scenario. And if we are not able to tell the computer exactly what to do, we can’t be a professional data scientist. So, it should come as no surprise that Chapter 1 is dedicated to the Python programming language. Python and R are the two most common languages in data science and we advocate that a professional data scientist should learn both. Python is the more versatile of the two and thus more suitable for an introductory course such as this one. Once we are able to do something, it becomes very important that we do it in a transparent and easily repeatable way. We put a lot of emphasis on reproducibility and the tools that help us achieve it. Why? First, because we need to be true to the word science in data science. If it is not clear how something was done, if some steps cannot be reproduced, then we cannot have confidence in the results and that is not science! And second, from a purely practical perspective, if our work is easily reproducible, we ourselves will have a much easier job of repeating our analyses on different data or with slightly modified methodology. What time we invest into reproducibility we will recieve 10-fold savings later on. We will cover source code control in Chapter 2, Docker for portability in Chapter 6 and Jupyter notebooks and other dynamic reporting tools in Chapter 3. In practice, data are in most cases not in tabular format and stored in a csv file. It is important to know how to retrieve and deal with less structured data and how to store data in a more systematic and efficent way. We will cover web scraping in Chapter 4 and relational databases and SQL in Chapter 9. While most of the work is in getting, preprocessing and storing the data, most of the added value in data science is in extracting valuable information from data. First, we need to learn how to efficiently and effectively summarize data and do exploratory data analysis. In most cases, these techniques combined with some method of quantifying uncertainty in our summaries will be all that is required to successfully complete our analysis! This broad topic will be covered in three chapters: basic numerical summaries for univariate and bivariate data are covered in Chapter 5, standard visualization techiques are covered in Chapter 7 and techniques for multivariate data are covered in Chapter 8. Predictive modelling, the family of tasks that includes a vast majority of the analytic tasks we encounter in practice, is covered in Chapter 10. And finally, dealing with missing data - a very important topic and a topic that requires us to combine all the summarization, exploratory and predictive techniques - is covered in Chapter 11. "],
["python-introduction-chapter.html", "Chapter 1 Python programming language 1.1 Basic characteristics 1.2 Why Python? 1.3 Setting up the environment 1.4 Installing dependencies 1.5 Jupyter notebooks 1.6 A short introduction to Python 1.7 Python ecosystem for Data Science 1.8 Further reading and references 1.9 Learning outcomes 1.10 Practice problems", " Chapter 1 Python programming language 1.1 Basic characteristics Python is an interpreted, dynamically typed, object-oriented programming language. Advantages: Simple. Easy to learn. Extensive packages. Cross-platform. Free and open source. Large user base. Disadvantages: Being interpreted results in slower execution. Dynamic typing can lead to errors that only show up at runtime. Higher memory consumption, less suitable for memory-intensive tasks. 1.2 Why Python? The Python language is one of the two most popular languages for data science (the other being R). The three main reasons are: The advantages of Python fit the typical data science workflow and its disadvantages are not that deterimental to the data science workflow. Python has a large ecosystem of packages, libraries and tools for data science, some of which are discussed later in this chapter. Often libraries and software developed in other languages provide Python API or bindings. The typical data science workflow consists of acquiring and manipulating data and applying standard machine learning or statistical methods. In essence, the data flows through different methods. The emphasis is on obtaining results - extracting meaningful information from data. The advantages of Python are extremely beneficial to such a workflow: Being simple, easy to learn and free, it is accessible to a larger user base, including users with little or no prior programming experience. Being an interpreted language (and straightforward piecewise execution through read-eval-print loop shells or REPL) makes Python very flexible - multiple alternatives can be explored and quick decisions made on how to procede, depending on intermediary results. The disadvantages of Python are of minor consequence: The data science workflow is not time-critical - even an order-of-magnitude slowdown typically makes little difference. Memory and time consumption can be significantly decreased by using popular libraries such as numpy and scipy. Code maintainability is less important - data science scripts are often short and discarded after use. Specific platforms development or enterprise-level applications development are also not part of the typical data science workflow. Data science products used in such applications are normally rewritten as final models in a production-ready code. That being said, Python is in most cases much more suitable than R in terms of development, maintainability, speed and other considerations that are relevant to in-production use. Python dominates R as the language of choice in industry projects where solutions have to be deployed, such as typical machine learning tasks. On the other hand, R dominates Python as the language of choice for data science consultants, government and not-for-profic organizations and academia. 1.3 Setting up the environment Before using Python, we need to select and install a Python distribution. We can choose to install a pure Python distribution or an Anaconda Python distribution. Some advantages of using an Anaconda distribution are: Anaconda makes it easy for the user to install the Python version of choice. Anaconda will also resolve issues with administrator privileges if a user does not have administrative rights for his system. Anaconda Accelerate can provide the user with high performance computing and several other components. Anaconda removes bottlenecks involved in installing the right packages while taking into considerations their compatibility with various other packages as might be encountered while using the standard pip package manager. There is no risk of breaking required system libraries. There are also many open source packages available for Anaconda, which are not within the pip repository. We encourage you to use the Anaconda Python distribution. 1.3.1 Anaconda distribution installation Install the desired Anaconda Python distribution. A useful way of managing multiple Python projects is to use Conda environments. An environment enables you to use a specific version of Python along with specific dependencies completely separately on a single system. To create and use an environment named ids, issue the following command: $ conda create -n ids $ conda activate ids At the beginning of a line in the console you can see currently active environment. To run Python within this evironment, issue the python command in the console. You should see something similar to the following: (ids)$ python Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; To exit the Python interpreter, enter and commit the command exit(); To exit the environment, use conda deactivate. To show existing environments and their locations, issue conda info --envs. 1.3.2 Pure Python distribution installation You can also install a pure Python distribution directly to your system from the official Python Downloads web page. To run Python, issue the python command in the console (there may be more interpreters installed on your machine and Python 3.5 might be run also using python3.5). After running the command, you should see something similar to the following: $ python Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 26 2016, 10:47:25) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; When using the distribution directly, all packages and settings are changed system-wide, which may lead to problems when maintaining multiple Python projects (e.g. problems in having installed different versions of a specific library). Similar to an Anaconda distribution, one can use virtualenv virtual environments. First, we need to install virtualenv via pip: $ pip3 install virtualenv $ virtualenv --version 16.6.1 To set up a virtual environment for a project first create a project folder and set up a new environment in that folder. The latter will create Python executables within that folder and a copy of the pip library that is used to install libraries local to the environment (parameter p is optional). $ cd ids_project $ virtualenv -p /usr/local/bin/python2 ids To activate the environment, run the script venv/bin/activate from the project folder and use project specific Python: $ source ids/bin/activate (ids)$ python Python 2.7.14 (default, Mar 9 2018, 23:57:12) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; After you finish working on your project, deactivate the current virtual environment: (ids)$ deactivate $ 1.4 Installing dependencies An Anaconda distribution provides its own package repository and Anaconda packages can be installed as follows: $ conda install nltk If a package is not available in the official repository, it may be available from some private or community-led channels, for example conda-forge: $ conda install -c conda-forge pyspark Many useful libraries are available online, mostly in the Python Package Index (PyPI) repository, which can also be used directly in a conda environment. Well-built libraries consist of installation instructions and a setup.py installation file. The common location for installing libraries is the folder %PYTHON_BASE%/lib, %PYTHON_BASE%/site-packages or %PYTHON_BASE%/Lib. Packages can be installed using the pip command. For example, to install the NLTK library, we issue the following command: $ pip install nltk In some cases packages will be prebuilt for a specific OS, because the installation of its dependencies can be tedious. In such cases, wheel packages can be provided and installed using the following command: $ pip install YOUR_DOWNLOADED_PACKAGE.whl 1.5 Jupyter notebooks The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning and much more. To add support for Anaconda environments to Jupyter, issue the command below. This will add Conda extensions to Jupyter. The feature will be installed for currently active environment. When running Jupyter, you will notice a Conda tab in the file browser, which will enable the listing and selection of existing Anaconda environments, overview of the installed packages in the environment, installing new packages from the available package list, checking for updates packages and updating packages in the environment. $ conda install nb_conda 1.5.1 Running a Jupyter notebook Prior to running a Jupyter notebook we first need to start a Jupyter web server by issuing a command: $ jupyter notebook By default the server is started in the current folder and accessible via the web interface: http://localhost:8888. The root web page shows a file browser where we can create a new Jupyter notebook or open an existing one: To get a similar view, save the provided Jupyter notebook into the folder where you run jupyter notebook command. Click the filename of the notebook and it will open in a new window: As you notice, a notebook consists of linearly ordered blocks of types Markdown, Code, Raw NBConvert and Heading. If you double click a text block you can edit it using Markdown syntax. To evaluate the block again and show rendered view, click the Run button or press Alt+Enter. The same holds for other types of blocks. Sometimes we just want to view or execute a notebook online. There exist multiple services offering this functionality, for example https://gke.mybinder.org/. 1.6 A short introduction to Python All the examples presented in this section are also provided in a Jupyter notebook. Basics Let’s first say hello: print(&quot;Hello Data science!&quot;) ## Hello Data science! Now we know how to print something to the output. Before we dive into details, let’s first check how we can easily output literal values and variables. Let’s have a variable name and age with specific values and form a form a final string to show to a user. name = &quot;Mark&quot; age = 42 # Basic old-style string concatenation print(&quot;Hi, I am &quot; + name + &quot; and I am &quot; + str(age) + &quot; years old.&quot;) # %-formatting print(&quot;Hi, I am %s and I am %d years old.&quot; % (name, age)) # Cleaner syntax using format function print(&quot;Hi, I am {} and I am {} years old.&quot;.format(name, age)) # Format function with extended features of parameter naming and output formatting print(&quot;Hi, I am {name} and I am {age:3d} years old.&quot;.format(age=age, name=name)) # Same features as format function with evaluations directly within curly braces print(f&quot;Hi, I am {name} and I am {age:3d} years old&quot;) # Another example f = 91 print(f&quot;{f:.2f} Fahrenheit is {(f - 32) * 5 / 9:.2f} Celsius&quot;) ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old ## 91.00 Fahrenheit is 32.78 Celsius The last example seems the most readable and we will use it from now on. It is also called f-string and you can read more about its features in the official documentation about literal string interpolation. 1.6.0.1 Variables and types Python defines whole numbers (int, long) and real numbers (float). Whole numbers are integers (\\(\\pm 2^{31}\\) or \\(\\pm 2^{63}\\)) and long numbers, limited by the memory size. Long is a number with a trailing L added at the end (Python 2, in Python 3, long is merged with int). Complex numbers are also supported using a trailing j to the imaginary part. Bool type is based on integer - value of 1 is True and value of 0 is False. For the boolean expressions, integer value of 1 will be interpreted as True and all other values as False. String values are represented as sequence of characters within \" or '. A constant None is defined to represent the nonexistence of a value. a = 2864 print(f&quot;Type of a is {type(a)}&quot;) b = 18+64j print(f&quot;Type of c is {type(b)}&quot;) c = False print(f&quot;Type of d is {type(c)}&quot;) d = &quot;I&#39;m loving it!&quot; print(f&quot;Type of e is {type(d)}&quot;) e = None print(f&quot;Type of f is {type(e)}&quot;) ## Type of a is &lt;class &#39;int&#39;&gt; ## Type of c is &lt;class &#39;complex&#39;&gt; ## Type of d is &lt;class &#39;bool&#39;&gt; ## Type of e is &lt;class &#39;str&#39;&gt; ## Type of f is &lt;class &#39;NoneType&#39;&gt; Numbers and basic operators Basic data manipulations: a = 3 b = 2.5 c = a + b print(f&quot;Addition: {c}&quot;) c = a * b print(f&quot;Multiplication: {c}&quot;) c = a / b print(f&quot;Division: {c}&quot;) c = True + 5 print(f&quot;Addition to Boolean: {c}&quot;) c = &quot;5&quot; * 5 print(f&quot;String multiplication: {c}&quot;) ## Addition: 5.5 ## Multiplication: 7.5 ## Division: 1.2 ## Addition to Boolean: 6 ## String multiplication: 55555 1.6.0.2 Strings, concatenation and formatting Basic strings manipulations: a = &quot;Data science&quot; b = &#39;a multi-disciplinary field&#39; # we can use double or single quotes c = a + &quot; &quot; + b print(f&quot;Concatenated string: &#39;{c}&#39;&quot;) first = c[:4] last = c[-5:] print(f&quot;First word: &#39;{first}&#39; and last word: &#39;{last}&#39;.&quot;) firstLower = first.lower() lastUpper = last.upper() print( (f&quot;First word lowercased: &#39;{firstLower}&#39;&quot; f&quot;and last word uppercased: &#39;{lastUpper}&#39;.&quot;) ) management = c.replace(&quot;science&quot;, &quot;management&quot;) print(f&quot;Substring replacement: &#39;{management}&#39;&quot;) ## Concatenated string: &#39;Data science a multi-disciplinary field&#39; ## First word: &#39;Data&#39; and last word: &#39;field&#39;. ## First word lowercased: &#39;data&#39;and last word uppercased: &#39;FIELD&#39;. ## Substring replacement: &#39;Data management a multi-disciplinary field&#39; Explore more about strings in the official Python 3 documentation for strings. # string package import string print(f&quot;Punctuation symbols: &#39;{string.punctuation}&#39;&quot;) ## Punctuation symbols: &#39;!&quot;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~&#39; String manipulation is essential for parsing and providing machine readable outputs when needed. Below we show additional two examples of how to set print length of a variable using f-string method. For more sophisticated examples, see https://pyformat.info/ - f-string method is the new equivalent of the format method. number = 6/.7 text = &quot;dyslexia&quot; format0 = f&quot;Number: {round(number*100)/100.0}, Text: {&#39; &#39;*(15-len(text))} {text}&quot; print(format0) format1 = f&quot;Number: {number:5.2f}, Text: {text:&gt;15}&quot; print(format1) ## Number: 8.57, Text: dyslexia ## Number: 8.57, Text: dyslexia 1.6.0.3 Data stuctures: Lists, Tuples, Sets, Dictionaries Below we create some of the data structures available in Python. Explore more of their functionality in the official Python documentation. l = [1, 2, 3, &quot;a&quot;, 10] # List t = (1, 2, 3, &quot;a&quot;, 10) # Tuple (immutable) s = {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;} # Set dict = { &quot;title&quot;: &quot;Introduction to Data Science&quot;, &quot;year&quot;: 1, &quot;semester&quot;: &quot;fall&quot;, &quot;classroom&quot;: &quot;P02&quot; } dict[&quot;classroom&quot;] = &quot;P03&quot; We often use inline functions to map, filter or calculate values on an iterable data structure. For example, to apply a function to all values (map), filter out unnecessary values or use all values in a calculation: # Python 3 import for reduce (not needed for Python 2) from functools import reduce l = [6, 8, 22, 4, 12] doubled = map(lambda x: x*2, l) print(f&quot;Doubled: {doubled}&quot;) filtered = filter(lambda x: x &gt; 10, l) print(f&quot;Filtered: {filtered}&quot;) sum = reduce(lambda x, y: x+y, l) print(f&quot;Sum value: {sum}&quot;) ## Doubled: &lt;map object at 0x103dd2f50&gt; ## Filtered: &lt;filter object at 0x103dd2cd0&gt; ## Sum value: 52 Functions filter, reduce and map create a generator since Python 3. Generators enable declaring functions that behaves like an iterator. To print all the values of the aobve generator objects, we need to evaluate them, for example transform them into a list. print(f&quot;Doubled: {list(doubled)}&quot;) print(f&quot;Filtered: {list(filtered)}&quot;) ## Doubled: [12, 16, 44, 8, 24] ## Filtered: [22, 12] In comparison to generators we propose to use list comprehension on iterable types, which is more readable and also faster. l = [6, 8, 22, 4, 12] newList = [x**2 for x in l if x &gt;= 5 and x &lt;= 10] print(f&quot;Squared values between 5 and 10: {newList}&quot;) Squared values between 5 and 10: [36, 64] Sometimes we would like to generate a repeated sequence of select (i.e. slice) only specific values from a string, bytes, tuple, list or range. Slice object represents the indices specified by range(start, stop, step): l = list(range(10)) print(f&quot;List: {l}&quot;) slice_indexes = slice(2,8,2) print(f&quot;Sliced list: {l[slice_indexes]}&quot;) List: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Sliced list: [2, 4, 6] 1.6.1 Flow control Many operations can be written inline or using multiple lines. Let’s check how to use if statements and loops. a = 2 if a &gt; 1: print(&#39;a is greater than 1&#39;) elif a == 1: print(&#39;a is equal to 1&#39;) else: print(&#39;a is less than 1&#39;) ## a is greater than 1 # Inline if statement a = 2 print(&#39;a is greater than 1&#39; if a &gt; 1 else &#39;a is lower or equal to 2&#39;) ## a is greater than 1 Loops: for i in range(4, 6): print(i) ## 4 ## 5 people_list = [&#39;Ann&#39;, &#39;Bob&#39;, &#39;Charles&#39;] for person in people_list: print(person) ## Ann ## Bob ## Charles i = 1 while i &lt;= 3: print(i) i = i + 1 ## 1 ## 2 ## 3 1.6.2 Functions We organize (encapsulate) our code into logical units, so we can reuse them and reduce the amount of duplicate, boilerplate code. Below write the function greetMe that takes one argument (name) as input and prints some string. The function’s code will be executed when we call the function. def greetMe(name): print(f&quot;Hello my friend {name}!&quot;) greetMe(&quot;Janez&quot;) ## Hello my friend Janez! Sometimes our functions will have many arguments, some of which might be optional or have a default value. In the example below we add a argument with a default value. If there are multiple optional arguments we can set their values by naming them. def greet(name, title = &quot;Mr.&quot;): print(f&quot;Hello {title} {name}!&quot;) greet(&quot;Janez&quot;) greet(&quot;Mojca&quot;, &quot;Mrs.&quot;) greet(&quot;Mojca&quot;, title = &quot;Mrs.&quot;) ## Hello Mr. Janez! ## Hello Mrs. Mojca! ## Hello Mrs. Mojca! A function can also call itself and return a value. def sumUpTo(value): if value &gt; 0: return value + sumUpTo(value-1) else: return 0 print(f&quot;Sum of all positive integers up to 50 is: {sumUpTo(50)}&quot;) ## Sum of all positive integers up to 50 is: 1275 Functions can also return multiple values in a tuple. Tuple can be then also automatically unpacked into separate values: def calculateHealth(height_cm, weight_kg, age, gender = &#39;male&#39;): # Body mass index bmi = weight_kg/(height_cm/100)**2 # Basal metabolic rate (Revised Harris-Benedict Equation) bmr = 0 # Ideal body weight ibw = 0 if gender == &#39;male&#39;: bmr = 13.397*weight_kg + 4.799*height_cm - 5.677*age + 88.362 ibw = 50 + (0.91 * (height_cm - 152.4)) else: bmr = 9.247*weight_kg + 3.098*height_cm - 4.330*age + 447.593 ibw = 45.5 + (0.91 * (height_cm - 152.4)) return (bmi, bmr, ibw) janez_health = calculateHealth(184, 79, 42) (bmi, bmr, ibw) = calculateHealth(178, 66, 35, &#39;female&#39;) print(f&quot;Janez:\\n\\tBMI: {janez_health[0]}\\n\\tBMR: {janez_health[1]}\\n\\tIBW: {janez_health[2]}&quot;) print(f&quot;Mojca:\\n\\tBMI: {bmi}\\n\\tBMR: {bmr}\\n\\tIBW: {ibw}&quot;) Janez: BMI: 23.334120982986768 BMR: 1791.3070000000002 IBW: 78.756 Mojca: BMI: 20.830703194041156 BMR: 1457.7890000000002 IBW: 68.79599999999999 Python encapsulates variables within functions - they are not accessible outside the function. When we want variables to be accessible globally, we can use the global keyword. This can result in some difficulties to predict behaviour and interactions, so use with caution! def playWithVariables(value1, list1): global globVal globVal = 3 value1 = 10 list1.append(22) print(f&quot;Within function: {value1} and {list1} and {globVal}&quot;) value1 = 5 list1 = [3, 6, 9] print(f&quot;Before function: {value1} and {list1}&quot;) playWithVariables(value1, list1) print(f&quot;After function: {value1} and {list1} and {globVal}&quot;) ## Before function: 5 and [3, 6, 9] ## Within function: 10 and [3, 6, 9, 22] and 3 ## After function: 5 and [3, 6, 9, 22] and 3 In some cases we can also define functions that accept an arbitrary number of unnamed (args) and/or named (kwargs) arguments. def paramsWriter(*args, **kwargs): print(f&quot;Non-named arguments: &#39;{args}&#39;\\nNamed arguments: &#39;{kwargs}&#39;&quot;) paramsWriter(1, &quot;a&quot;, [1,5,6], studentIds = [234, 451, 842], maxScore = 100.0) ## Non-named arguments: &#39;(1, &#39;a&#39;, [1, 5, 6])&#39; ## Named arguments: &#39;{&#39;studentIds&#39;: [234, 451, 842], &#39;maxScore&#39;: 100.0}&#39; When naming functions, classes, objects, packages, etc. we need to be careful not to overwrite existing objects. Bugs such as this one can be difficult to find: def greeter(): print(&quot;Hello to everyone!&quot;) greeter() greeter = &quot;Mr. John Hopkins&quot; greeter() # Error - greeter is now string value Hello to everyone! Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; TypeError: &#39;str&#39; object is not callable 1.6.3 Classes and objects Python is also object-oriented and therefore enables us to encapsulate data and functionality into classes. Instances of classes are objects. The class below consists of one class variable, one class method, three object methods and two object variables. All class-based variables are accessible using the class name directly without having instantiated the object. Object-based methods are accessible only through an instantiated object and can also directly modify object properties (self.*). All object methods accept self as an implicit parameter, which points to the current object. Below we show an example of object declaration and its use. A detailed explanation can be found in the official Python documentation. class Classroom: class_counter = 0 def num_classes(): return Classroom.class_counter def __init__(self, name): Classroom.class_counter += 1 self.name = &quot;Best of Data Science class &quot; + name self.students = [] def enroll(self, student): self.students.append(student) def __str__(self): return f&quot;Class: &#39;{self.name}&#39;, students: &#39;{self.students}&#39;&quot; class1 = Classroom(&quot;best of millenials&quot;) class2 = Classroom(&quot;old sports&quot;) print(f&quot;Num classes: {Classroom.class_counter}&quot;) print(f&quot;Num classes: {Classroom.num_classes()}&quot;) class2.enroll(&quot;Slavko Žitnik&quot;) class2.enroll(&quot;Erik Štrumbelj&quot;) class2.enroll(&quot;Tomaž Curk&quot;) print(class2) ## Num classes: 2 ## Num classes: 2 ## Class: &#39;Best of Data Science class old sports&#39;, students: &#39;Slavko Žitnik, Erik Štrumbelj, Tomaž Curk&#39; 1.6.4 Reading and writing files Sometimes the data is stored in a well-formatted file which we must read. Also, it is expected that we output results into a file, so let’s check how to read and write to files using Python. To open a file, call function open(FILENAME, MODE). The function returns handle to work with a file. We must select the mode type which can be one of: r - reading only, w - writing to a file (previous content will be deleted), x - creating a new file (function fails of a file already exists), a - appending to a file, t - opening a file in text mode, b - opening a file in binary mode and + - opening a file for reading and writind (updating). Writing content to a file: file = open(&quot;ids.txt&quot;,&quot;w+&quot;) for i in range(10): file.write(f&quot;This is line {i}.\\r\\n&quot;) file.close() Reading content (output omitted): #Open the file and read the contents file = open(&quot;ids.txt&quot;, &quot;r&quot;) # Reading the whole content into a variable contents = file.read() # Reading file into a list of lines file.seek(0) #Move to the beginning as we already readt the whole file lines = file.readlines() # Reading line by line file.seek(0) for line in file: print(line) It is also useful to read and write JSON data directly. Python can automatically write and read JSON files for built-in objects: import json json_obj = {&#39;name&#39;: &#39;Janez&#39;, &#39;age&#39;: &#39;Novak&#39;, &#39;marks&#39;: [{&#39;OPB&#39;: 8, &#39;IDS&#39;: 6, &#39;WIER&#39;: 10}]} # Write json to a file json.dump(json_obj, open(&#39;json_output.json&#39;, &#39;w&#39;)) # Read json to a variable from file janez = json.load(open(&#39;json_output.json&#39;, &#39;r&#39;)) print(janez) ## {&#39;name&#39;: &#39;Janez&#39;, &#39;age&#39;: &#39;Novak&#39;, &#39;marks&#39;: [{&#39;OPB&#39;: 8, &#39;IDS&#39;: 6, &#39;WIER&#39;: 10}]} 1.6.5 Python IDE’s and code editors An IDE (Integrated Development Environment) is software dedicated to software development. As the name implies, IDEs integrate several tools specifically designed for software development. These tools usually include: An editor designed to handle code with features such as syntax highlighting and auto-completion. Build, execution and debugging tools. Some form of source control support. IDEs are generally large and take time to download and install. You may also need advanced knowledge to use them properly. In contrast, a dedicated code editor can be as simple as a text editor with syntax highlighting and code formatting capabilities. Most good code editors can execute code and control a debugger. The very best ones interact with source control systems as well. Compared to an IDE, a good dedicated code editor is usually smaller and quicker, but often less feature rich. Below we list some popular Python IDEs/code editors that are available for major operating systems (Windows, Linux and Mac OS): IDLE - the default code editor that installs together with the Python distribution. It includes a Python shell window (interactive interpreter), auto-completion, syntax highlighting, smart indentation and a basic integrated debugger. We do not recommend it for larger projects. Sublime Text, Atom, Visual Studio Code - highly customizable code editors with rich features of an IDE. They support installation of additional extensions and also provide intelligent code completion, linting for potential errors, debugging, unit testing and so on. These editors are becoming quite popular among Python and web developers. PyCharm - an IDE for professional developers. There are two versions available: a free Community version and a paid Professional version which is free for students only. PyCharm provides all major features of a good IDE: code completion, code inspections, error-highlighting and fixes, debugging, version control system and code refactoring, etc.. 1.7 Python ecosystem for Data Science The Python ecosystem of libraries, frameworks and tools is large and ever-growing. Python can be used for web scraping, machine learning, general scientific computing and many other computing and scripting uses. We list some of the most widely used libraries in the field of data science. NumPy - NumPy is the fundamental package for scientific computing with Python. The tools that we commonly use are: a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code and most importantly, linear algebra, Fourier transform and random number capabilities. NumPy can also be used as an efficient multi-dimensional container of generic data, where arbitrary data-types can be defined. Matplotlib - A 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers. We can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc., with just a few lines of code. It provides a MATLAB-like interface, particularly when combined with IPython. It gives users full control of line styles, font properties, axes properties, etc. SciPy - “Sigh Pie”\" is a Python-based ecosystem of open-source software for mathematics, science and engineering. In particular, it connects the following core packages: NumPy, SciPy library (fundamentals for scientific computing), Matplotlib, IPython, Sympy (symbolic mathematics) and Pandas. scikit-learn - a Machine Learning (ML) library in Python. It provides simple and efficient tools for data analysis. It offers a framework and many algorithms for classification, regression, clustering, dimensionality reduction, model selection and preprocessing. The library is open source and build on NumPy, SciPy and matplotlib. Pandas - Python Data Analysis Library (pandas) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Some of the library highlights are a fast and efficient DataFrame object for data manipulation with integrated indexing, tools for reading and writing data between in-memory data structures and different formats, intelligent data alignment and integrated handling of missing data, flexible reshaping and pivoting of data sets, high performance merging and joining of data sets, an intuitive way of working with high-dimensional data in a lower-dimensional data structure, time series-functionality. TensorFlow - TensorFlow is an end-to-end open source platform for machine learning in the field of deep learning. It has a comprehensive and flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers to easily build and deploy ML powered applications. Keras - Compared with TensorFlow, Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK or Theano backends. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay. Keras allows for easy and fast prototyping (through user friendliness, modularity and extensibility), supports both convolutional networks and recurrent networks, as well as combinations of the two and runs seamlessly on CPU and GPU. 1.8 Further reading and references Here is a list of more comprehensive guides to Python programming: Fluent Python: Clear, Concise, and Effective Programming Official Python Tutorials - Tutorial accompanying official Python documentation. See this resource for latest features. Beginning Python by Magnus Hetland - The book is written for beginners but last chapters are useful also for more experienced programmers. Non-Programmer’s Tutorial for Python - a well organized Wikibook. Think Python - similar book to the previous one, but a bit more intermediate. [Wikibook: Programming Python] - similar to the above two with more advanced topics. How to Think Like a Computer Scientist - well organized sections for self-paced learning. Python za programerje by Janez Demšar - Well known Slovene book, written by the professor at our Faculty. You can find electronic versions online, but printed version is accessible to buy at publishing house FRI (at the entrance). 1.9 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use the Python programming language for simple programming tasks, data manipulation and file I/0. Identify the Python IDE(s) that best fit their requirements. Find suitable Python packages for the task at hand and use them. Recognize when Python is and when it is not a suitable language to use. 1.10 Practice problems Install Anaconda Python, run the provided Jupyter notebook within a new conda environment and then export all the installed dependencies into an environment.yml file (see reference). Check the file, remove not needed data (location, library versions, libraries in lower dependency trees), create a new environment based on the exported file and run the notebook again (it should work without the need to install additional packages manually). Try different Python IDEs and form a personal opinion of their advantages and disadvantages. Download, explore and run some scripts from the Keras examples repository. Download the CMU Seminar Announcements dataset and uncompress it. The dataset consists of multiple files, whereas each file represents a seminar anncouncement. Write a program that reads every file and tries to extract the speaker, title, start-time, end-time and the location of the seminar. Help yourself with regular expressions and libraries mentioned above. Store all the extracted data into a Pandas data frame and lastly, export all the data into a single CSV file. In addition, compute your success rate in extraction of specific fields based on the manual tags from the documents. "],
["git.html", "Chapter 2 Source code control 2.1 Overview 2.2 Why should I know about Git 2.3 Basic Git terminology 2.4 A beginner Git example 2.5 Beyond the basics 2.6 Git tools 2.7 Further reading and references 2.8 Learning outcomes 2.9 Practice problems", " Chapter 2 Source code control 2.1 Overview Source code control systems, version control systems or revision control systems are systems that ease software development in groups. Wikipedia defines them as follows: A component of software configuration management, version control, also known as revision control, source control, or source code management systems are systems responsible for the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the “revision number”, “revision level”, or simply “revision”. For example, an initial set of files is “revision 1”. When the first change is made, the resulting set is “revision 2”, and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged. There exist a number of different source control systems such as Subversion, Mercurial, Microsoft Visual SourceSafe, SourceDepot, Git, etc.. Git is currently the most widely used version control system in the world. It is the most recognized and popular approach to contribute to a project, whether it be open source or commercial, in a distributed and collaborative manner. Beyond being a distributed version control system, Git has been designed with performance, security and flexibility in mind. If you are new to Git it is important for you to differentiate between Git and GitHub: Git is a widely used Version Control System (VCS) that lets you keep track of all the modifications you make to your code. This means that if a new feature is causing any errors, you can easily roll back to a previous version. GitHub is a widely used platform for version control that uses Git at its core. It lets you host the remote version of your project from where all the collaborators can have access to it. GitHub is like a social platform where you can find a plethora of open-source projects with their codes. All the new and emerging technologies can be found on this platform. You can collaborate on many projects and have discussions on your contributions. There exist also other platforms that enable you hosting of remote Git repositories such as GitLab and BitBucket. 2.2 Why should I know about Git Where people are often working in groups that are small enough they feel comfortable just keeping their code on Dropbox and trying to make sure that collaborators never work on the same file at the same time (in the hope of never changing the same file). But Git has much to offer to the modern data scientist, even if they only work on small teams and don’t want to contribute to larger software projects or even for personal projects. Some major benefits of using Git are: Keep an archive of every version of your project: Git works by logging the work you do on your project into a series of discrete sets of changes called “commits”. Crucially, it remembers all of your commits, making it possible at any time to go back to a previous version of the project. If you discover that you or your collaborator deleted part of your code and you did not notice, can easily recover the version of that file that existed before. Systematically keeping track of the history of your work is also an important component of modern reproducible research, which we discuss in Chapter 3. All you and your co-authors to work at the same time: Git treats each line in a text document separately, so if your collaborator is editing the introduction of your paper (assuming it’s in a text format such as LaTeX) while you’re editing the conclusion, Git can easily integrate your simultaneous edits. Moreover, if you do both edit the same line of code or text, then Git will help you resolve those conflicting edits in a very efficient manner instead of what Dropbox does, and leave you to figure out what changes conflicted and how to integrate both authors’ changes. You can easily see what changes were made and by whom: Git is organized around keeping track of commits. When your or your collaborator make changes to a file, Git allows you to easily see just the changes your collaborator has made. This makes it much easier for colleagues to be aware of how their project is changing and to watch out of potential issues. For example, you can easily see if your collaborator recoded a variable in a way that is problematic for code you wrote later. Allows you to contribute to open source projects: This may not be something you’re planning to do, but learning Git will make this an option. If you find that a package you use doesn’t have a feature you need, the ability to use Git will make it possible for you to add that feature to the package, not only allowing you to do what you want to do, but also making that fix available to the broader community. Allows you to make your project open source so others can contribute to your project: If you know Git, you can also share that code in a way that makes it easy for other people to contribute code and improve that package. Also, being active in the open-source community or showing code of your hobby projects in public source code repositories may help you getting a better job. 2.3 Basic Git terminology 2.3.1 Repository A Repository or Repo is a folder that contains all the project files and the history of the revisions made to each file. The repo stores its data within a .git hidden folder in your project. It contains all the history related to your repository. In general you will work with at least two repositories throughout the lifetime of your project – a Remote repo and a Local repo: A Remote repo contains your project that can be accessed from anywhere and by anyone that has access and sufficent rights to the git server (e.g. GitHub). In general you can have multiple remote repositories and push changes selectively to each of them. When you clone a repository for the first time from a remote repo, that remote repo will be named origin and you will be able to push or pull code from it. You can manage connections to the remote repositories using git remote commands. Local repo is a copy of the remote repo that resides on your local machine. All the changes you make are saved in your local repo. Your collaborators will not be able to see them until you push them to the remote repo (and after they browse the remote repo or pull the code locally). 2.3.2 Clone $ git clone &lt;Repo URL&gt; Cloning means creating a copy of the remote repo on your local machine. After that you can make changes to the local copy of the files, commit them to the local repo and push them back to the remote repo. If you would like to create a new repository locally without cloning an existing one, you can use the git init command and an empty repository will be created in you current folder. If you would like to publish your repository remotely, you will need to add a connection to the empty remote Git repository (git remote command) and then push commits to it. CAUTION: You cannot merge two different locallrepositories. So, at project start we create a single repository and all collaborators start by using that repository. 2.3.3 Adding $ git add &lt;filename or folder&gt; By default, Git monitors the project folder for changes in files. When you add new files or change existing ones, you need to tell git, which files would you like to include in the next commit using git add command. To check which files will already be included or which files are changed from the last repository state, you can use command git status. 2.3.4 Commit $ git commit -m &quot;&lt;commit message&gt;&quot; When you commit changes, you save the changes you made to files in the repo. The changes will consist only of files you had previously added using git add command and will be saved to your local Git repository. To make changes available to others, you need to send changes to the remote repo using the push command. HINT: If your commit would consist of only the files that Git is already tracking (i.e. no new files), you can use command git commit -a -m \"&lt;commit message&gt;\". 2.3.5 Push $ git push &lt;remote name&gt; &lt;branch name&gt; Push command allows you to copy all the changes from the remote repo to your local repo. Now all the collaborators will have access to the changes and will be able to update their local repositories. You can sync code with multiple remote repositories which is defined by the first parameter (by default it is named origin). The second parameter defines which branch we would like our changes to be copied (by default it is named master - we will cover more about branching in 2.5.2). 2.3.6 Pull $ git pull &lt;remote name&gt; &lt;branch name&gt; The pull command is analogous to the push command but is intended to copy changes from the remote repository to the local repository. 2.4 A beginner Git example Step 0: First we need to install Git. Visit https://git-scm.com/downloads, download the package for your system and install. Depending on your OS there might also exist package managers that provide Git. After you install Git, the ‘git’ command should be available to you in the terminal: $ git usage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;] [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path] [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare] [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;] &lt;command&gt; [&lt;args&gt;] .... Later in the example we use GitHub and if you wish to use SSH links, you need to generate a local SSH key and add it to your GitHub account as described in the documentation. Step 1: Let’s say you have started with a hobby mini project and wrote a simple script in a file greeter.py: def main(): print(&quot;Ehlo World!&quot;) main() Now you would like to save it in a Git repository. Step 2: Move to the folder where greeter.py is located and create an empty Git repository using a command git init. You can observe that a new hidden folder named .git was created in that directory and this is where your local repository resides. $ git init Initialized empty Git repository in /Users/slavkoz/Downloads/ds_repo/.git/ $ ls -la total 8 drwxr-xr-x 4 slavkoz staff 128 Aug 10 13:34 . drwxr-xr-x@ 154 slavkoz staff 4928 Aug 10 13:28 .. drwxr-xr-x 9 slavkoz staff 288 Aug 10 13:34 .git -rw-r--r-- 1 slavkoz staff 44 Aug 10 13:29 greeter.py To check the status of your repo, issue git status: $ git status On branch master No commits yet Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) greeter.py nothing added to commit but untracked files present (use &quot;git add&quot; to track) Step 3: Let’s tell Git to track our greeter.py file and add it to the list for the next commit. $ git add greeter.py $ git status On branch master No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: greeter.py Now the current version of the greeter.py is staged for commit and we can add it to our repository. $ git commit -m &quot;Initial version of greeter function.&quot; [master (root-commit) e2f5820] Initial version of greeter function. 1 file changed, 4 insertions(+) create mode 100644 greeter.py $ git status On branch master nothing to commit, working tree clean After all the changes are committed, we double-check there are no uncommited changes in the project. If we do not provide message parameter ‘-m’ to the git commit command, our default editor is opened to write a commit message. Commit messages should contain a description of the changes we made or and ID of the issue we were working on. Step 4: In the greeter script we have a typo, so we change the text “Ehlo World!” to “Hello Data Science!”. We must save the file in our text editor so the changes are written to disk, before we proceed! If we check the status of the repo, we will see our file has changed. All the changed files can be commited directly using the git commit -a -m \"...\" command. If we do not use the -a parameter, we need to first add files to a commit and then issue the commit command. $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: greeter.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) $ git commit -a -m &quot;Hello text update.&quot; [master e597a5d] Hello text update. 1 file changed, 1 insertion(+), 1 deletion(-) $ git status On branch master nothing to commit, working tree clean Step 5: We would now like to provide a more personal script, so we change the greeter.py as follows: def main(): student = input(&quot;Please enter your name: &quot;) print(f&quot;Dear {student}, we are happy to have you in the Data Science track!&quot;) main() Again, add the file to stage and commit it to the repo. $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: greeter.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) $ git commit -a -m &quot;Personal update.&quot; [master 29710cc] Personal update. 1 file changed, 3 insertions(+), 1 deletion(-) $ git status On branch master nothing to commit, working tree clean The commit command returns how many files were changes and how many lines were added or deleted. Still, it is a good practice to use command git status to make sure that we are not forgetting something. Step 6: Now our project has some history, $ git log commit 29710cc16bcaca476d35456fa7453a4f9b422ea6 (HEAD -&gt; master) Author: Slavko &lt;slavko@zitnik.si&gt; Date: Mon Aug 10 13:56:38 2020 +0200 Personal update. commit e597a5d1930af50f5ad5d5a6039e4767b8a8c4b4 Author: Slavko &lt;slavko@zitnik.si&gt; Date: Mon Aug 10 13:50:56 2020 +0200 Hello text update. commit e2f5820364419fa29a17d84392367b90a14d7214 Author: Slavko &lt;slavko@zitnik.si&gt; Date: Mon Aug 10 13:42:35 2020 +0200 Initial version of greeter function. As we can see, each commit is identified using a GUID which can be used to move our repo to a specific commit or refer to it. Git by default includes a graphical interface gitk to visually browse the history. There also exist a number of tools work with Git projects (see 2.6). We can upload our project to GitHub or some other host, so that it is stored in the cloud and, if we make the repository public, that others can contribute. To create a remote Git repository on GitHub, create a new GitHub account and then create a new public repository, named “DS-Project” (DO NOT select to add README or .gitignore!). When creating a public repository it may be important to define a license for your files in the repository. If a repository has no license, then all rights are reserved and it is not Open Source or Free. You cannot modify or redistribute this code without explicit permission from the copyright holder. It is therefore advisable that you select appropriate license for your source (check GitHub licensing help or Licensing choice helper). We already have a local repository so we refer to the second set of commands. The first of those commands adds a link to the remote repository and names it “origin” - we could also use any other name. The second command copies all the changes (three commits) to the remote repository to branch named master (default branch). The parameter ‘-u’ sets “origin” as the default remote repository, so we can later use commands git push/git pull without parameters. $ git remote add origin https://github.com/szitnik/DS-Project.git $ git push -u origin master Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 4 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (9/9), 861 bytes | 861.00 KiB/s, done. Total 9 (delta 0), reused 0 (delta 0) To https://github.com/szitnik/DS-Project.git * [new branch] master -&gt; master Branch &#39;master&#39; set up to track remote branch &#39;master&#39; from &#39;origin&#39;. $ git status On branch master Your branch is up to date with &#39;origin/master&#39;. nothing to commit, working tree clean Step 7: You can now browse your repository on GitHub (see commits, update security, add a description, etc.). The public Web site should be accessible at https://github.com/YOUR-GITHUB-USERNAME/DS-Project. It is good practice to include a short description of your project, installation instructions or other useful data to your repository landing page. This is done by creating a README.md file in the root of your project (read more about this in Chapter 3). Sometimes we don’t want certain files to be uploaded to the repository. For example, sensitive files, such as passwords, temporary files created by our IDES, compiled programs etc. To ignore files, we can create a .gitignore file and add names of files or folders we would like to ignore (one line per file). Commit and push your file to the repo and Git will automatically ignore them (they will not appear in Git status listing). Note that we can ignore not just individual files but files that match a specific folder structure or pattern, such as extension. Check some examples of ignore files at https://github.com/github/gitignore. 2.5 Beyond the basics In this section we describe three essential uses of Git: conflict resolution, branching and forking. We continue with our project from the previous section. 2.5.1 Conflict resolution Let’s say that two developers A and B are working on a project and have 3 synced repositories (local repo at devevoper A, local repo at developer B and public GitHub repo). Developer B now changes the text in code to “… you in the Data Science track at the University of Maribor!”, commits and pushes the change to the remote repo. Step 7: Developer A later changes the text in the code to “… you in the Data Science track at the University of Ljubljana!”, commits file and tries to push the file to the remote repo, after the change by developer B has already been pushed: $ git commit -a -m &quot;Uni. Ljubljana added.&quot; [master d263b01] Uni. Ljubljana added. 1 file changed, 1 insertion(+), 1 deletion(-) $ git push To https://github.com/szitnik/DS-Project.git ! [rejected] master -&gt; master (fetch first) error: failed to push some refs to &#39;https://github.com/szitnik/DS-Project.git&#39; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., &#39;git pull ...&#39;) before pushing again. hint: See the &#39;Note about fast-forwards&#39; in &#39;git push --help&#39; for details. Developer A’s push is rejected, because the remote repository contains further commits before his change. A needs to issue git pull to first integrate changes locally and then push them to the remote repo. $ git pull remote: Enumerating objects: 5, done. remote: Counting objects: 100% (5/5), done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. From https://github.com/szitnik/DS-Project 29710cc..11483e3 master -&gt; origin/master Auto-merging greeter.py CONFLICT (content): Merge conflict in greeter.py Automatic merge failed; fix conflicts and then commit the result. Had A and B worked on separate parts of the project, the changes would be automatically merged. In this case they were working on the same line and Git is not able to automatically merge the code. It needs to be done manually in the greeter.py file: In the conflicted file we are presented both conflicting changes. To resolve the conflict we need to remove lines added by Git and keep the final code (for example, A’s code). Then a new commit and push can be made with merged changes. # Manually resolve conflict $ nano greeter.py $ git commit -a -m &quot;Merged changes.&quot; [master e6deaf6] Merged changes. $ git push Enumerating objects: 10, done. Counting objects: 100% (10/10), done. Delta compression using up to 4 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (6/6), 547 bytes | 547.00 KiB/s, done. Total 6 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 1 local object. To https://github.com/szitnik/DS-Project.git 11483e3..e6deaf6 master -&gt; master Git includes the tool git mergetool which simplifies conflict resolution. The developer just selects which lines to include in the final decision. Git tools provide further useful features for conflict resolution (see 2.6). 2.5.2 Branching We have already mentioned that the default branch of a Git repository is a branch named master. When working on a larger project we would like to have the stable code always accessible and separate from development code. Git branches in essence allow us to create multiple repositories in a single Git repository. Therefore we can have a master branch with the tested code that is available to download and other branches which are based on the master branch and contain new features in development. Step 8: Let’s say our project is mature and many people use it daily. We would like to add parameters to the application and when we complete development and are satisfied with the solution, merge it into the master branch. First, we need to create a new branch using the command git branch &lt;branch-name&gt;. The command without the name parameter lists branches in the local repo and shows which branch is currently active. $ git branch parameters-addition $ git branch * master parameters-addition $ git checkout parameters-addition Switched to branch &#39;parameters-addition&#39; $ git branch master * parameters-addition Everything we do with Git is done locally. To make this branch available to others, we need to upload it to the remote repo using command git push origin parameters-addition. As we are now in a new branch, we update the greeter.py code as follows: def main(): print(&quot;###################################\\nSTUDENT ENROLLMENT FORM\\n###################################&quot;) student = input(&quot;Please enter your name: &quot;) uni = input(&quot;Please enter your University name: &quot;) track = input(&quot;Please enter your study programme: &quot;) print(f&quot;Dear {student}, we are happy to have you in the {track} track at the {uni}!&quot;) main() Then we commit the change and push it to the remote repo. The testers can now test the code and then we decide to merge the changes into master to make them available to our users. We move back to the master branch (our changes above are not available there) using the command git checkout &lt;branch-name&gt;. We can now merge the parameters-addition branch nto master using the git merge &lt;branch name&gt; -m \"message\" command. $ git checkout master Switched to branch &#39;master&#39; Your branch is up to date with &#39;origin/master&#39;. $ git merge parameters-addition -m &quot;Track and uni parameters feature added&quot; Updating e6deaf6..7983f71 Fast-forward (no commit created; -m option ignored) greeter.py | 6 +++++- 1 file changed, 5 insertions(+), 1 deletion(-) If other changes were made to master while we were developing new features in our branch, they would be merged together. In the case of conflicting changes, we would have resolve them using the procedures we described in the previous section. A good practice when merging branches is to first merge the main branch into the feature branch, then resolve conflicts, test and then finally merge back into the main branch. Command git log and Git GUIs in particular (see 2.6) implement nice visualizations of connections between branches. 2.5.3 Forking Forking is not a Git feature but a feature that platforms like GitHub offer to developers. Let’s say a Korean developer finds our repository and would like to translate our project and then publish his addition to our repository. Forking enables him to create a separate clone of our repository under his username. He has all the rights to that repository, but any changes he makes will not affect our repository. After he completes his translation he can send us a pull request to merge his changes into our repository. We can review his changes, ask him to improve the code, and finally reject or accept his changes. In the latter case the changes are then merged with the work in our repository. Step 9: Let’s say there is Korean developer that is interested in our application and he forks our repo on GitHub. He then implements additional features and creates a pull request. We are notified of his pull request and can review it. We decide that his work would be a valuable addition to our project so we accept the change. Our repository is now updated with Korean changes. 2.6 Git tools In the beginning we advise to you to use Git commands manually, so that you are in complete control of what is going on and that you develop a fundamental understanding of what Git does and how it works. However, in practice, especially if your work includes many Git repositories, collaborators or advanced Git functionalities, you can benefit from using a Git GUI such as SourceTree, GitKraken and Tower. These tools provide a high level interface which makes it easier to visually keep track of changes, manage repositories, resolve conflicts, review merge pull requests, etc. Note that some IDEs such as VS Code and IntelliJ integrate Git functionalities. A typical example of a Git GUI (GitKraken): 2.7 Further reading and references Official Git source with extensive documentation: git-scm. They provide a free book called Pro Git. Git tutorials along with more advanced topics such as Git large file storage can be found here: Atlassian Git. There are many online Git resources and courses, such as A beginners guide. 2.8 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Create their own Git repositories and clone existing repositories. Commit, push and pull from Git repositories. Work in a collaborative environment, submit issues, create forks and submit pull requests. Students should also consciously use source control to systematically keep track of their work, including homework, projects and other coursework. 2.9 Practice problems Take one of your (finished) faculty projects, create an empty GitHub repository and add all of its files to the repository. In a separate commit create a .gitignore file (populate it with common ignores related to your project) and a README.md file. Push everything to remote GitHub repository. Create a separate branch named cleaning and check it out. Now, remove any un-needed files, update few files, update README.md, commit and push changes. Go back to the master branch and change the same line in README.md, commit and push. Then merge branch cleaning into master and resolve conflicts. After merge commit and push, remove cleaning branch locally and remotely. Try to find an error or potential improvement to this Introduction to datascience book. Clone the book’s Git repository, make the change and submit a pull request. Then make at least one suggestion of a more comprehensive change to the book (for example, what could be explained in more or less detail, is there an improtant topic that isn’t covered, etc.) by submitting an issue. "],
["reproducibility.html", "Chapter 3 Reproducible research 3.1 Scientific inquiry 3.2 From principles to practice 3.3 Reproducibility tools in R 3.4 Reproducibility tools in Python 3.5 Further reading and references 3.6 Learning outcomes 3.7 Practice problems", " Chapter 3 Reproducible research 3.1 Scientific inquiry The term data science contains the word science. It is our view that data science is more or less synonimous with with modern empirical science and as such it should adhere to the standards for scientific inquiry. Ideally, scientific inquiry would always be objective and correct. However, these are unattainable ideals! Science will always be subjective at least in the sense of what we investigate and how we choose to investigate it. And scientists, like all people, even with the best intentions and practices, make mistakes. Instead, the realistic standard is lower, albeit still surprisingly difficult to attain - research should be reproducible. We will borrow two definitions from the American Statistical Association: Reproducibility: A study is reproducible if you can take the original data and the computer code used to analyze the data and reproduce all of the numerical findings from the study. Replicability: This is the act of repeating an entire study, independently of the original investigator without the use of original data (but generally using the same methods). In other words, reproducibilty refers to providing a complete and unambiguous description of the entire process from the original raw data to the final numerical results. Reproducibility does not concern itself with the correctness of the results or the process. Our research might even have major bugs in the code or completely flawed methodology, but if it is reproducible, those bugs and those flaws are transparent and will be found by those trying to reproduce or replicate our results. That is, as long as our research is reproducible, scientific critique and progress can be made! However, if our research is not reproducible, then it is of much less value! Reproducibility will be our minimum standard for scientific enquiry and is also a prerequisite for replicability. Replicability, on the other hand, does concern itself with the correctness of the original research. If we independently repeat a study, we expect to get the same results or at least within some margin of error due to sampling and other sources of variability. Sometimes we might replicate our own study to validate it, but in most cases, replication of our studies is done by others. 3.2 From principles to practice Perfect reproducibility is also unattainable, because not all factors can be feasibly accounted for (time, location, hardware used, etc.). Instead, we will strive for a high but practically feasible level of reproducibility. We will also introduce the practical dimension of how easy to reproduce research is. For example, if we precisely describe all data manipulations in a text document, the data manipulation part of our research will be reproducible. However, it will be much more difficult to reproduce than, for example, if we provided well-documented code that did the same manipulations. The modern standard for reproduciblity from data to results is to use computer code for everything. This includes even minimal changes to the original data (for example, renaming a column), generating and including plots and tables and even single-number data in the text of our report or paper. Computer code is, by definition, clear and unambiguous - there is no room for misinterpretation of what has been done. And, if we have everything in code, we completely avoid the possiblity of introducing errors or undetectable changes through manual manipulation of the data or copying the results Such code together with the original data and a precise description of the platform and software used (relevant hardware, operating system, packages, including versions) constitutes as highly and easily reproducible research. This process can be made much easier with the use of standard tools. In particular, integrated analyses and reports, such as Jupyter notebooks and R markdown and notebooks, which we introduce later in the chapter. 3.2.1 Preregistration - the future standard As we already discussed above, reproduciblity is concerned only with the completeness and unambiguity of the description of the study. All studies should be reproducible, but we expect that some studies will not replicate. Some due to the inherent risk of statistical procedures, some due to researchers’ mistakes and some due to accidental or deliberate manipulation of the researcher’s degrees of freedom or data dredging. These refer to the choices that we make during a study (some of which should strictly be made before the study). For example: the researcher chooses the method or statistical test that confirms the desired hypothesis, the researcher selects the hypotheses only after seeing the data, the researcher splits the data into subgroups that lead to desired results, and many others. In order to deal with this problem and enforce a higher standard of research (and subsequently fewer studies that do not replicate) preregistration is very slowly but surely becoming the norm for professional research. In essence, preregistration is the practice of registering all relevant aspects of a scientific study (how the data will be collected, hypotheses, methods used, etc.) before we begin with the study itself. Preregistration is made with some central authority, such as the Open science framework and many reputable scientific journals already promote and support preregistered research. 3.3 Reproducibility tools in R In this section we introduce four different approaches to integrating R code and text to produce dynamic reports, fully reproducible reports that can automatically be updated if the data change. All four approaches are integrated into RStudio. R Markdown R Markdown (knitr) is the most popular approach to dynamic reporting with R. It combines the lightweight Markdown language with R code chunks. R Markdown documents can be rendered as html, pdf, word, etc. Here is an example of a R markdown file (.Rmd extension) that features R code, inline R code and an image: --- output: html_document --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE) ``` ## Data ```{r} set.seed(0) x &lt;- rnorm(100) y &lt;- rnorm(100) ``` ```{r fig.width=4, fig.height=4, fig.cap = paste(&quot;Relationship between high jump and long jump performance (n =&quot;, length(x), &quot;).&quot;)} plot(x,y) ``` We can also use inline R core: there are `r length(y)` observations in our data. R Markdown is very easy to use and can produce nice documents. However, it lacks the control of more specialized typesetting languages such as LaTeX. A good starting point for learning R Markdown is the free online book R Markdown: The Definitive Guide. Note that the textbook you are reading was also produced with R Markdown and the Bookdown extension. R Markdown can also be used with chunks from other programming languages, for example, Python, with the help of the reticulate package. R Notebook A R Notebook is identical to a R Markdown document, except that we replace the output to html_notebook: --- output: html_notebook --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE) ``` ## Data ```{r} set.seed(0) x &lt;- rnorm(100) y &lt;- rnorm(100) ``` ```{r fig.width=4, fig.height=4, fig.cap = paste(&quot;Relationship between high jump and long jump performance (n =&quot;, length(x), &quot;).&quot;)} plot(x,y) ``` We can also use inline R core: there are `r length(y)` observations in our data. R Notebooks are more interactive - code chunk results are shown inline and are rendered in real time. That is, we can render individual chunks instead of the entire document. Sweave When we require more control over the typsetting, for example, when we are trying to produce an elegant pdf document or adhere to the typsetting standards of an academic journal, we might prefer Sweave. Sweave is an integration of LaTeX and R code - in essence, a LaTeX document with R code chunks. R code is executed when we compile the Sweave document with Sweave and LaTeX. Here is an example of a Sweave file (.Rnw extension) that demonstrates the use of R code, inline R code and an image: \\documentclass{article} \\begin{document} \\section*{Data} &lt;&lt;&gt;&gt;= set.seed(0) x &lt;- rnorm(100) y &lt;- rnorm(100) @ Look at the nice plot in Figure \\ref{fig1}. \\begin{figure}[htb] &lt;&lt;fig=T&gt;&gt;= plot(x,y) @ \\caption{Relationship between high jump and long jump performance (n = \\Sexpr{length(x)}).}\\label{fig1} \\end{figure} \\end{document} Sweave gives us all the typsetting control of LaTeX at the expense of more code and having to compile the LaTeX as well. A good starting point for Sweave is this compact Sweave manual. Shiny web apps and dashboards Shiny is an R package that makes it easy to build interactive applications with R. Shiny can be used to produce standalone web apps and dashboards or it can be embedded into R Markdown documents. Shiny is useful for rapid development of user-friendly interfaces to interactive analyses of our data. These Shiny tutorials are a good starting point for further study. 3.4 Reproducibility tools in Python Python offers a variety of tools similar to R. It provides Jupyter notebooks with multiple options to create reproducible Python code with dynamic visualizations and presentations and exports to different formats. We already showed how to install Jupyter notebooks in Chapter 1. The NBConvert tool allows you to convert a Jupyter .ipynb notebook document file into another format. The nbconvert documentation contains a complete description of this tool’s capabilities. It allows for: Presentation of information in familiar formats, such as PDF. Pblishing of research using LaTeX and opens the door for embedding notebooks in papers. Collaboration with others who may not use the notebook in their work. Sharing content with many people via the web using HTML. Overall, notebook conversion and the nbconvert tool give scientists and researchers the flexibility to deliver information in a timely way across different formats. Primarily, the nbconvert tool allows you to convert a Jupyter .ipynb notebook document file into another static format including HTML, LaTeX, PDF, Markdown, reStructuredText, and more. The nbconvert tool can also add productivity to your workflow when used to execute notebooks programmatically. If used as a Python library (import nbconvert), nbconvert adds notebook conversion within a project. For example, nbconvert is used to implement the “Download as” feature in the Jupyter notebook web application. When used as a command line tool (jupyter nbconvert), users can conveniently convert just one or a batch of notebook files to another format. Jupyter Dashboards A jupyter dashboard is a Jupyter notebook with the dashboards layout extension, where we can arrange our notebook outputs (text, plots, widgets, etc.) in a grid - or report-like layouts. The layouts are saved in the notebook document. When in a Jpyter notebook, we should see the dashboard view extension to control the dashboard settings: Jupyter notebooks and dashboards are also widely used, support a large number of programming languages and they provide: Documentation and literate programming by combining rich-text narrative concepts &amp; machine-readable code. The notebeook itself is a data-structure with metadata that can be easily read and parsed. Exploration &amp; development: Intermediate steps are saved in a clean, well documented format. Communication/Collaboration: sharing research with peers, collaborators, reviewers, public. Publishing: It is simple and quick switch between the development &amp; publishing stage In combination with jupyter widgets, dynamic dashboards can be created easily. Below we show an example of a general dashboard, where a user selects a location and weather data is extracted from the Web. Along with the data the location of a weather station is also visualized. import ipywidgets as widgets from ipywidgets import interact import urllib.request from lxml import etree import folium w = widgets.Dropdown( options=[(&quot;Ljubljana&quot;, &quot;LJUBL-ANA_BEZIGRAD&quot;), (&quot;Celje&quot;, &quot;CELJE&quot;), (&quot;Kočevje&quot;, &quot;KOCEVJE&quot;), (&quot;Novo mesto&quot;, &quot;NOVO-MES&quot;)], value=&quot;LJUBL-ANA_BEZIGRAD&quot;, description=&#39;Location:&#39;, ) @interact(location=w) def f(location): url = f&quot;http://meteo.arso.gov.si/uploads/probase/www/observ/surface/text/en/observation_{location}_latest.xml&quot; xml = etree.XML(urllib.request.urlopen(url).read()) lat = etree.XPath(&quot;//domain_lat/text()&quot;)(xml)[0] lon= etree.XPath(&quot;//domain_lon/text()&quot;)(xml)[0] station_name = etree.XPath(&quot;//domain_title/text()&quot;)(xml)[0] last_updated = etree.XPath(&quot;//tsUpdated/text()&quot;)(xml)[0] weather = etree.XPath(&quot;//nn_shortText/text()&quot;)(xml)[0] temp = etree.XPath(&quot;//t/text()&quot;)(xml)[0] humidity = etree.XPath(&quot;//rh/text()&quot;)(xml)[0] print(f&quot;Location: {station_name}\\nLast updated: {last_updated}\\n\\n&quot;) print(f&quot;Weather info: {weather}\\nTemperature: {temp}°C\\nHumidity: {humidity}%&quot;) slovenia_map = folium.Map(location=[46, 14.5], zoom_start=9, tiles=&#39;Stamen Terrain&#39;) folium.Marker([46.0658,14.5172], icon=folium.Icon(color=&#39;green&#39;, icon=&#39;ok-sign&#39;)).add_to(slovenia_map) display(slovenia_map) There exist many possible options for graph visualizations (e.g. Plotly) and integrations of common libraries such as matplotlib. from matplotlib import pyplot as plt import numpy as np import math %matplotlib inline def showGraph(scale): x = np.arange(0, math.pi*scale, 0.05) y = np.sin(x) fig = plt.figure() ax = fig.add_axes([0, 0, 1, 1]) ax.plot(x, y) ax.set_xlabel(&quot;Angle&quot;) ax.set_ylabel(&quot;sine&quot;) interact(showGraph, scale = widgets.IntSlider(value=4, description=&#39;Scale&#39;, max=10, min=1)) 3.5 Further reading and references Science is in a replication and reproducibility crisis. In some fields more than half of published studes fail to replicate and most studies are not reproducible: Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature News, 533(7604), 452. Preregistration will eventually become the standard for publication of empirical research: Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600-2606. 3.6 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Produce reproducible data science analyses. Identify reproducibility flaws in own and other peoples’ research. Distinguish between reproducibility and replication. 3.7 Practice problems Create a short R Markdown, Sweave and/or Jupyter Notebook that loads some data, draws a plot, prints a table and contains some text with an example of inline use of code. Consider the following experiment of drawing \\(m &gt; 1\\) samples from a distribution (standard normal or uniform) and taking their average, repeating this process for \\(n &gt; 1\\) times and plotting the histogram of these \\(n\\) averages: n &lt;- 5000 m &lt;- 10 set.seed(0) dist &lt;- &quot;normal&quot; all &lt;- c() for (i in 1:n) { if (dist == &quot;normal&quot;) { x &lt;- rnorm(m) } else { x &lt;- runif(m) } all &lt;- c(mean(x), all) } hist(all, breaks = sqrt(n)) Create a Shiny App, Shiny Dashboard or Jupyter Dashboard that draws such a histogram and allows you to interactively change \\(n\\), \\(m\\) and which distribution is used (support standard normal, uniform and a third distribution of choice). Use the Dashboard to visually explore if the sample average tend to a normal distribution as \\(m\\) grows larger. "],
["scraping.html", "Chapter 4 Web scraping 4.1 Introduction to Web data extraction 4.2 Web wrapper 4.3 Further reading and references 4.4 Learning outcomes 4.5 Practice problems", " Chapter 4 Web scraping Today there are more than 3.7 billion Internet users, which almost 50% of the entire population (Internet World Stats 2017). Taking into account all the existing Internet-enabled devices, we can estimate that approximatelly 30 billion devices are connected to the internet (Deitel, Deitel, and Deitel 2011). In this chapter we focus on Web data extraction (Web scraping) - automatically extracting data from websites and storing it in a structured format. However, the broader field of Web information extraction also requires the knowledge of natural language processing techniques such as text pre-processing, information extraction (entity extraction, relationship extraction, coreference resolution), sentiment analysis, text categorization/classification and language models. Students that are interested in learning more about these techniques are encouraged to enrol in a Natural language processing course. For introduction to natural language techniques please see the Further reading references (Liu 2011) (Chapter 11), (Christopher, Prabhakar, and Hinrich 2008) (Chapters 12, 13, 15-17, 20), (Aggarwal and Zhai 2012) (Chapters 1-8, 12-14) or other specialized books on natural language processing. 4.1 Introduction to Web data extraction Web data extraction systems (Ferrara et al. 2014) are a broad class of software applications that focus on extracting data from Web sources. A Web data extraction system usually interacts with a Web source and extracts data stored in it: for example, if the source is an HTML Web page, the extracted content could consist of elements in the page as well as the full-text of the page itself. Eventually, extracted data might be post-processed, converted to the most convenient structured format and stored for further usage. The design and implementation of Web data extraction systems has been discussed from different perspectives and it leverages on scientific methods from different disciplines including machine learning, logic and natural language processing. Web data extraction systems find extensive use in a wide range of applications including the analysis of text-based documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence, crawling of Social Web platforms, Bioinformatics and so on. The importance of Web data extraction systems depends on the fact that a large (and steadily growing) amount of data is continuously produced, shared and consumed online: Web data extraction systems allow us to efficiently collect these data with limited human effort. The availability and analysis of collected data is vital to understanding complex social, scientific and economic phenomena which generate the data. For example, collecting digital traces produced by users of social Web platforms such as Facebook, YouTube or Flickr is the key step to understand, model and predict human behavior. In commercial fields, the Web provides a wealth of public information. A company can probe the Web to acquire and analyze information about the activity of its competitors. This process is known as Competitive Intelligence and it is crucial to quickly identify the opportunities provided by the market, to anticipate the decisions of the competitors as well as to learn from their faults and successes. The challenges of Web data extraction In its most general formulation, the problem of extracting data from the Web is very hard because it is constrained by several requirements. The key challenges we can encounter in the design of a Web Data Extraction system can be summarized as follows: Web data extraction techniques implemented in a Web data extraction system often require human input. The first challenge consists of providing a high degree of automation by reducing human efforts as much as possible. Human feedback, however, may play an important role in raising the level of accuracy achieved by a Web data extraction system. A related challenge is to identify a reasonable tradeoff between building highly automated Web data extraction procedures and the requirement of achieving accurate performance. Web data extraction techniques should be able to process large volumes of data in relatively short time. This requirement is particularly stringent in the field of Business and Competitive Intelligence because a company needs to perform timely analysis of market conditions. Applications in the field of Social Web or, more in general, those dealing with personal data, must provide solid privacy guarantees. Therefore, potential (even if unintentional) attempts to violate user privacy should be timely and adequately identified and counteracted. Approaches relying on machine learning often require a large training set of manually labeled Web pages. In general, the task of labeling pages is time comnsuming and error prone and, therefore, in many cases we cannot assume the existence of labeled pages. Often, a Web data extraction tool has to routinely extract data from a Web Data source which can evolve over time. Web sources are continuously evolving and structural changes happen with no forewarning and are thus unpredictable. In real-world scenarios such systems have to be maintained, because they are likely to eventually stop working correctly, in particular if they lack the flexibility to detect and face structural modifications of the target Web sources. The first attempts to extract data from the Web date back to the early 90s. In the early days, this discipline borrowed approaches and techniques from Information Extraction (IE) literature. In particular, two classes of strategies emerged: learning techniques and knowledge engineering techniques – also called learning-based and rule-based approaches, respectively. These classes share a common rationale: the former was thought to develop systems that require human expertise to define rules (for example, regular expressions) to successfully accomplish the data extraction. These approaches require specific domain expertise: users that design and implement the rules and train the system must have programming experience and a good knowledge of the domain in which the data extraction system will operate; they will also have the ability to envisage potential usage scenarios and tasks assigned to the system. On the other hand, also some approaches of the latter class involve strong familiarity with both the requirements and the functions of the platform, so human input is essential. 4.2 Web wrapper In the literature, any procedure that aims at extracting structured data from unstructured or semi-structured data sources is usually referred to as a wrapper. In the context of Web data extraction we provide the following definition: Web wrapper is a procedure, that might implement one or many different classes of algorithms, which seeks and finds data required by a human user, extracting them from unstructured (or semi-structured) Web sources, and transforming them into structured data, merging and unifying this information for further processing, in a semi-automated or fully automated way. Web wrappers are characterized by a 3-step life-cycle: Wrapper generation: the wrapper is defined and implemented according to one or more selected techniques. Wrapper execution: the wrapper runs and extracts data continuously. Wrapper maintenance: the structure of data sources may change and the wrapper should be adapted accordingly. The first two steps of a wrapper life-cycle, generation and execution, might be implemented manually, for example, by defining and executing regular expressions over the HTML documents. Alternatively, which is the aim of Web data extraction systems, wrappers might be defined and executed by using an inductive approach – a process commonly known as wrapper induction (Kushmerick, Weld, and Doorenbos 1997). Web wrapper induction is challenging because it requires high-level automation strategies. Induction methods try to uncover structures from an HTML document to form a robust wrapper. There exist also hybrid approaches that make it possible for users to generate and run wrappers semi-automatically by means of visual interfaces. The third and final step of a wrapper’s life-cycle is maintenance: Web pages change their structure continuously and without forewarning. This might affect the correct functioning of a Web wrapper, whose definition is usually tightly bound to the structure of the Web pages adopted. Defining automated strategies for wrapper maintenance is vital to the correctness of extracted data and the robustness of Web data extraction platforms. Wrapper maintenance is especially important for long-term extractions. For the purposes of acquiring data in a typical data-science project, wrappers are mostly run once or over a short period of time. In such cases it is less likely that a Web site would change, so automation is not a priority and is typicaly not implemented. 4.2.1 HTML DOM HTML is the predominant language for implementing Web pages and it is largely supported by the World Wide Web consortium. HTML pages can be regarded as a form of semi-structured data (even if less structured than other sources like XML documents) in which information follows a nested structure. HTML structure can be exploided in the design of suitable wrappers. While semi-structured information is often also available in non-HTML formats (for example, e-mail messages, code, system logs), extracting information of this type is the subject of more general information extraction and not the focus of this chapter. The backbone of a Web page is a Hypertext Markup Language (HTML) document which consists of HTML tags. According to the Document Object Model (DOM), every HTML tag is an object. Nested tags are called children of the enclosing tag. Generally we first parse a web page into a DOM tree representation. Then we specify extraction patterns as paths from the root of the DOM tree to the node containing the values to extract. Special languages such as XPath or XQuery support searching and querying elements of a DOM tree. Below we show a simple HTML Web page and its representation as a HTML DOM tree. &lt;html&gt; &lt;head&gt; &lt;title&gt;My Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;&quot;&gt;My link&lt;/a&gt; &lt;h1&gt;My header&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; Figure 4.1: DOM tree representation of the above HTML Web page. The HTML DOM is an Object Model for HTML and it defines: HTML elements as objects. Properties for all HTML elements. Methods for all HTML elements. Events for all HTML elements. Interactive Web pages mostly consist of Javascript code which is executed directly in the Web browser. The HTML DOM is also an API (Programming Interface) for JavaScript that allows it to dynamically: Add/change/remove HTML elements/attributes/events. Add/change/remove CSS styles. React to HTML events. 4.2.2 XPath One of the main advantage of the adoption of the Document Object Model for Web Content Extraction is the possibility of exploiting the tools for XML languages (and HTML is to all effects a dialect of the XML). In particular, the XML Path Language (XPath) provides with a powerful syntax to succintly address specific elements of an XML document. XPath was also defined by the World Wide Web consortium, as was DOM. Below we provide an XML example to explain how XPath can be used to address elements of a Web page. &lt;persons&gt; &lt;person&gt; &lt;name&gt;John&lt;/name&gt; &lt;height unit=”cm”&gt;191&lt;/height&gt; &lt;sport&gt;Running&lt;/sport&gt; &lt;sport&gt;Cycling&lt;/sport&gt; &lt;/person&gt; &lt;person&gt; &lt;name&gt;Mandy&lt;/name&gt; &lt;height&gt;140&lt;/height&gt; &lt;sport&gt;Swimming&lt;/sport&gt; &lt;/person&gt; &lt;/persons&gt; There exist two possible ways to use XPath: (A) to identify a single element in the document tree, or (B) to address multiple occurrences of elements. We show some XPath queries against the above XML document: /persons/person/name - Extract all elements in the provided path. The result is &lt;name&gt;John&lt;/name&gt; and &lt;name&gt;Mandy&lt;/name&gt;. /persons/person/name/text() - Get contents of all the elements that match the provided path. The result is John and Mandy. //person/height[@unit=\"cm\"]/text() - Extract contents of all height objects that have set attribute unit to cm and have their parent element named as person. Result is 191. //sport - Extract all elements that appear at any level of XML and are named sport. The result is &lt;sport&gt;Running&lt;/sport&gt;, &lt;sport&gt;Cycling&lt;/sport&gt; and &lt;sport&gt;Swimming&lt;/sport&gt;. //person[name=\"Mandy\"]/sport/text() - Extract contents of sport objects that are nested directly under person objects which contain a name object with value Mandy. The result is Swimming. The major weakness of XPath is its lack of flexibility: each XPath expression is strictly related to the structure of the Web page. However, this limitation has been partially mitigated with the introduction of relative path expressions. In general, even minor changes to the structure of a Web page might corrupt the correct functioning of an XPath expression defined on a previous version of the page. Still, due to the ease of use, many Web extraction libraries support the use of XPath in to addition of their own extraction API. Let us consider Web pages generated by a script (e.g. the information about a book in an e-commerce Web site). Now assume that the script undergoes some changes. We can expect that the tree structure of the HTML page generated by that script will change accordingly. To keep the Web data extraction process functional, we should update the expression every time the underlying page generation model changes. Such an operation would require a substantial human effort and would therefore be very costly. To this end, the concept of wrapper robustness was introduced. From the perspective of XPath the idea is to find, among all the XPath expressions capable of extracting the same information from a Web page, the one that is least influenced by potential changes in the structure of the page and such an expression identifies the more robust wrapper. In general, to make the entire Web data extraction process robust, we need tools allowing for measuring document similarity. Such a task can be accomplished by detecting structural variations in the DOM trees associated with the documents. Techniques called tree-matching strategies are a good candidate to detect similarities between two trees (Tai 1979). The discussion of these techniques is outside the scope of this chapter. 4.2.3 Modern Web sites and JS frameworks Modern Web sites still use HTML to render their data within Web browsers. Below we show an example of a Web page (left side) and extracted content from it (right side). When all pages follow the same structure, we can easily extract all the data. e-Commerce Web page - product description Extracted data in a structured format (JSON) However, Web sites are becoming more dynamic - loading data in the background, not refreshing the entire view, etc. These functionalities require dynamic code to be executed directly on the client, that is, inside the Web browser. The main language that can be interpreted by a Web browser is Javascript. Although best practices instruct the programmers to support non-Javascript browsers, there are many Web pages that malfunction if the browser does not support Javascript. With the advent of Single page application (SPA) Web sites the content does not even partially load as the whole Web page is driven by the Javascript. Popular frameworks that enable SPA development are, for example Angular, Vue.js or React. Below we show some examples of rendering Web pages when a browser runs with Javascript enabled or disabled: Javascript enabled Javascript disabled When we develop a Web extraction system we should first review how the target Web site is built and which frontend technologies are used. Then we can also more efficiently use a library to implement a final Web wrapper to extract the desired data. When we need to execute Javascript, our extraction library needs to implement headless browser functionality. This functionality runs a hidden browser to construct a final HTML content which is then used for further manipulation. Libraries that support such functionality are, for example: Selenium, phantomJS and HTMLUnit. Running a headless browser and executing Javascript can be time consuming and prone to errors. So, whenever we can, we should rather use just an HTML parsing library. There exist many of such libraries and we mention just a few: HTML Cleaner, HTML Parser, JSoup (Java) or BeautifulSoup (Python), Jaunt API and HTTP Client. 4.2.4 Crawling, resources and policies Crawling is a process of automatic navigation through Web pages within defined Web sites. When we deal with continuous retrieval of content from a large amount of Web pages, there are many aspects we need to take care of. For example, we need to track which pages were already visited, we need to decide how to handle HTTP redirects or HTTP error codes in case of a delayed retry, we must follow the rules written in robots.txt for each domain or should follow general crawling ethics so that we not send too many request to a specific server and we need to track changes on Web pages to identify approximate change-rate, etc. Generally, a crawler architecture will consist of the following components (Figure 4.2): HTTP downloader and renderer: To retrieve and render a web page. Data extractor: Minimal functionalities to extract images and hyperlinks. Duplicate detector: To detect already parsed pages. URL frontier: A list of URLs waiting to be parsed. Datastore: To store the data and additional metadata used by the crawler. Figure 4.2: Web crawler architecture. As we already mentioned before, we need to understand all the specifics how Web pages are built and generated. To make sure that we correctly gather all the needed content placed into the DOM by Javascript, we should use headless browsers. Google’s crawler Googlebot implements this as a two-step process or expects to retrieve dynamically built web page from an HTTP server. A session on crawling modern web sites built using JS frameworks, link parsing and image indexing was a part of Google IO 2018 and we recommend it to get an impression of problems that we can encounter: A crawler needs to identify links, which can be encoded in several different ways. They can be explicilty given within href attributes, as onclick Javascript events (e.g. location.href or document.location), etc. Similarly, images can be generated in different formats, shown dynamically, etc. While in some circumstances it might be better to implement a custom crawler, a crawler package or suite is typically a better choice. Here are two examples: Apache Nutch, Scrapy. The Web consists also of other files that web pages point to, for example PDF files, Word/OpenOffice documents, Excel spreadsheets, presentations, etc. They may also include some relevant information. We recommend the following tools for parsing such content: Apache Tika toolkit detects and extracts metadata and text from over a thousand different file types (such as PPT, XLS, and PDF). All of these file types can be parsed through a single interface, making Tika useful for search engine indexing, content analysis, translation, and much more. Apache Poi focus on manipulating various file formats based upon the Office Open XML standards (OOXML) and Microsoft’s OLE 2 Compound Document format (OLE2). In short, you can read and write MS Excel files using Java. Apache PDFBox library is an open source Java tool for working with PDF documents. This project allows creation of new PDF documents, manipulation of existing documents and the ability to extract content from documents. It also includes several command-line utilities. 4.3 Further reading and references Practical Web Scraping for Data Science, Best Practices and Examples with Python (2018), Seppe vanden Broucke and Bart Baesens. Web Scraping with Python (2015), Ryan Mitchell Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data. 2nd ed. (2011), Bing Liu XPath W3schools tutorial 4.4 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Understand the architecture and different technologies used in the World Wide Web. Use or build a standalone Web crawler to gather data from the Web. Identify and automatically extract the Web page content of interest. 4.5 Practice problems Explore the Web and find some dynamic Web sites. Inspect the structure of a few Web pages with your favorite browser and try to load the same Web pages with Javascript disabled. Implement and run a crawler that will target https://fri.uni-lj.si/sl/o-fakulteti/osebje and extract the following information about every employee: name, email, phone number, title/job position and the number of courses they teach and number of projects they have listed. Save the data in CSV format. An example project: Recently, gov.si Web site has been launched. Build a crawler that will retrieve semi-structured data from the site only. Also, you need to obey crawler ethics, take into account robots.txt, etc. Store rendered HTML data and also extracted data in a database. References "],
["summarizing-data-basics.html", "Chapter 5 Summarizing data - the basics 5.1 Descriptive statistics for univariate distributions 5.2 Descriptive statistics for bivariate distributions 5.3 Further reading and references 5.4 Learning outcomes 5.5 Practice problems", " Chapter 5 Summarizing data - the basics Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis. In this chapter we focus on the basic techniques for univariate and bivariate data. Visualization and more advanced data summarization techniques will be covered in later chapters. We will be using R and ggplot2 but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools! 5.1 Descriptive statistics for univariate distributions We humans are not particularly good at thinking in multiple dimensions, so, in practice, there will be a tendency to look at individual variables and dimensions. That is, in practice, we will most of the time be summarizing univariate distributions. Univariate distributions come from various sources. It might be a theoretical distribution, an empirical distribution of a data sample, a probabilistic opinion from a person, a posterior distribution of a parameter from a Bayesian model, and many others. Descriptive statistics apply to all of these cases in the same way, regardless of the source of the distribution. Before we proceed with introducing the most commonly used descriptive statistics, we discuss their main purpose. The main purpose of any sort of data summarization technique is to (a) reduce the time and effort of delivering information to the reader in a way that (b) we lose as little relevant information as possible. That is, to compress the information. All summarization methods do (a) but we must be careful to choose an appropriate method so that we also get (b). Summarizing out relevant information can lead to misleading summaries, as we will illustrate with several examples. 5.1.1 Central tendency The most common first summary of a distribution is its typical value, also known as the location or central tendency of a distribution. The most common summaries of the location of a distribution are: the mean (the mass centre of the distribution), the median or 2nd quartile (the value such that half of the mass is on one and half on the other side), the mode (the most probable value or the value with the highest density). Given a sample of data, the estimate of the mean is the easiest to compute (we compute the average), but the median and mode are more robust to outliers - extreme and possibly unrepresentative values. In the case of unimodal approximately symmetrical distributions, such as the univariate normal distribution, all these measures of central tendency will be similar and all will be an excellent summary of location. However, if the distribution is asymmetrical (skewed), they will differ. In such cases it is our job to determine what information we want to convey and which summary of central tendency is the most appropriate, if any. For example, observe the Gamma(1.5, 0.1) distribution and its mean, median and mode: x &lt;- seq(0, 50, 0.01) a &lt;- 1.5 b &lt;- 0.1 y &lt;- dgamma(x, a, b) library(ggplot2) ggplot(data.frame(x,y), aes(x = x, y = y)) + geom_line() + ylab(&quot;p(x)&quot;) + geom_vline(xintercept = (a-1) / b, colour = &quot;orange&quot;, lty = &quot;dashed&quot;) + geom_vline(xintercept = a / b, colour = &quot;red&quot;, lty = &quot;dashed&quot;) + geom_vline(xintercept = qgamma(0.5, a, b), colour = &quot;blue&quot;, lty = &quot;dashed&quot;) + annotate(geom = &quot;text&quot;, x = (a-1) / b - 1.7, y = 0.01, colour = &quot;orange&quot;, label = &quot;mode&quot;, angle = 90) + annotate(geom = &quot;text&quot;, x = a / b - 1.7, y = 0.01, colour = &quot;red&quot;, label = &quot;mean&quot;, angle = 90) + annotate(geom = &quot;text&quot;, qgamma(0.5, a, b) - 1.7, y = 0.01, colour = &quot;blue&quot;, label = &quot;median&quot;, angle = 90) In the case of multi-modal distributions, no single measure of central tendency will adequately summarize the distribution - they will all be misleading. For example, look at this bimodal distribution: x &lt;- seq(-10, 20, 0.01) y &lt;- 0.6 * dnorm(x, 2, 1) + 0.4 * dnorm(x, 12, 2) z &lt;- c(rnorm(600, 2, 1), rnorm(400, 12, 2)) # approximating the median library(ggplot2) ggplot(data.frame(x,y), aes(x = x, y = y)) + geom_line() + ylab(&quot;p(x)&quot;) + geom_vline(xintercept = 0.6* 2 + 0.4 * 15, colour = &quot;red&quot;, lty = &quot;dashed&quot;) + geom_vline(xintercept = 2, colour = &quot;orange&quot;, lty = &quot;dashed&quot;) + geom_vline(xintercept = median(z), colour = &quot;blue&quot;, lty = &quot;dashed&quot;) + annotate(geom = &quot;text&quot;, x = 2 - 1, y = 0.025, colour = &quot;orange&quot;, label = &quot;mode&quot;, angle = 90) + annotate(geom = &quot;text&quot;, x = 0.6* 2 + 0.4 * 15 - 1, y = 0.025, colour = &quot;red&quot;, label = &quot;mean&quot;, angle = 90) + annotate(geom = &quot;text&quot;, x = median(z) + 0.5, y = 0.025, colour = &quot;blue&quot;, label = &quot;median&quot;, angle = 90) 5.1.2 Dispersion Once location is established, we are typically interested in whether the values of the distribution cluster close to the location or are spread far from the location. The most common ways of measuring such dispersion (or spread or scale) of a distribution are: variance (mean of quadratic distances from mean) or, more commonly, standard deviation (root of variance, so we are on the same scale as the measurement) median absolute deviation (median of absolute distances from mean), quantile-based intervals, in particular the inter-quartile range (IQR) (interval between the 1st and 3rd quartiles, 50% of the mass/density lies in this interval). Standard deviation is the most commonly used and median absolute deviation is more robust to outliers. In the case of distributions that are approximately normal, the mean and standard deviation will be the optimal choice for summarization, because they correspond directly to the two parameters of the normal distribution. That is, they completely summarize the distribution without loss of information. We also know that approximately 95% (99%) of the normal density lies within 2 (3) standard deviations from the mean. Standard deviation is useful even if the distribution is not approximately normal as it does provide some information, combined with the sample size (producing the standard error), on how certain we can be in our estimate of the mean. But, as before, the more we deviate from normality, the less meaningful standard deviation becomes and it makes more sense to use quantile-based intervals. For example, if we estimate the mean and \\(\\pm\\) 2 standard deviations for samples from the Gamma distribution from before, we get the following: set.seed(0) x &lt;- rgamma(1000, a, b) cat(sprintf(&quot;%.2f +/- %.2f\\n&quot;, mean(x), 2*sd(x))) ## 14.66 +/- 24.49 That is, the 95% interval estimated this way also includes negative values, which is misleading and absurd - Gamma distributed variables are positive. Computing the IQR or the 95% range interval provides a more sensible summary of this skewed distribution and, together with the mean also serve as an indicator that the distribution is skewed (the mean is not the centre of the intervals): set.seed(0) x &lt;- rgamma(1000, a, b) cat(sprintf(&quot;%.2f, IQR = [%.2f, %.2f], 95pct = [%.2f, %.2f]\\n&quot;, mean(x), quantile(x, 0.25), quantile(x, 0.75), quantile(x, 0.025), quantile(x, 0.975))) ## 14.66, IQR = [5.72, 20.76], 95pct = [1.18, 46.09] And, again, for multi-modal distributions, we can adequately summarize them only by identifying the modes visually and/or describing each mode individually. 5.1.3 Skewness and kurtosis As mentioned above, ranges can be used to indicate a distributions asymmetry (skewness) or fat-tailedness (kurtosis). Although less commonly used, there exist numerical summaries of skewness and kurtosis that can be used instead. The following example shows the kurtosis and skewness for a gamma, normal, logistic and bimodal distribution. Observe how the standard way of calculating kurtosis fails for the bimodal and assigns it the lowest kurtosis: library(moments) set.seed(0) tmp &lt;- NULL for (i in 1:4) { if (i == 1) x &lt;- rgamma(1000, a, b) if (i == 2) x &lt;- rnorm(1000, 0, 9) if (i == 3) x &lt;- rlogis(1000, 0, 9) if (i == 4) x &lt;- c(rnorm(700, 0, 9), rnorm(300, 35, 4)) s &lt;- paste0(&quot;kurtosis = &quot;, round(kurtosis(x), 2), &quot; skewness =&quot;, round(skewness(x), 2)) tmp &lt;- rbind(tmp, data.frame(x = x, name = s)) } ggplot(tmp, aes(x = x)) + geom_histogram(bins = 50) + facet_wrap(.~name) 5.1.4 Nominal variables Nominal variables are typically represented with the relative frequencies or probabilities, numerically or visually. Note that the methods discussed so far in this chapter apply to numerical variables (rational, interval and to some extent, ordinal) but not nominal variables, because the notions of location and distance (dispersion) do not exist in the nominal case. The only exception to this is the mode, which is the level of the nominal variable with the highest relative frequency or probability. One summary that is often useful for summarizing the dispersion or the uncertainty associated with a nominal variable is entropy. Observe the following example: entropy &lt;- function(x) { x &lt;- x[x != 0] -sum(x * log2(x)) } entropy(c(0.5, 0.5)) # fair coin ## [1] 1 entropy(c(0.8, 0.2)) # biased coin ## [1] 0.7219281 entropy(c(1.0, 0.0)) # coin with heads on both sides ## [1] 0 A fair coin has exactly 1 bit of entropy - we receive 1 bit of information by observing the outcome of a flip. This is also the maximum achievable entropy for a binary variable. A biased coin has lower entropy - we receive less information. In the extreme case of a coin with heads on both sides, the entropy is 0 - the outcome of a flip brings no new information, as we already know it will be heads. When we want to compare entropy across variables with different numbers of levels/categories, we can normalize it by dividing it with the maximum achieavable entropy. For example, observe a fair coin and a fair 6-sided die - in absolute terms, the 6-sided die has higher entropy due to having more possible values. However, relatively to the maximum achievable entropy, both represent maximally uncertain distributions: entropy(c(0.5, 0.5)) # fair coin ## [1] 1 entropy(rep(1/6, 6)) # fair 6-sided die ## [1] 2.584963 Note that entropy can easily be calculated for any discrete random variable. Entropy also has a continuous analogue - differential entropy, which we will not discuss here. 5.1.5 Testing the shape of a distribution Often we want to check if the distribution that underlies our data has the shape of some hypothesized distribution (for example, the normal distribution) or if two samples come from the same distribution. Here, we will present two of the most common methods used: the Kolmogorov-Smirnov test and the Chi-squared goodness-of-fit test. Both of these are Null-hypothesis significance tests (NHST), so, before we proceed, be aware of two things. First, do not use NHST blindly, without a good understanding of their properties and how to interpret their results. And second, if you are more comfortable with thinking in terms of probabilities of hypotheses as opposed to significance and p-values, there always exist Bayesian alternatives to NHST. The Kolmogorov-Smirnov test (KS) is a non-parametric test for testing the equality of two cumulative distribution functions (CDF). These can be two empirical CDFs or an empirical CDF and a theoretical CDF. The KS test statistic is the maximum distance between the two corresponding CDFs. That is, we compute the distribution of this statistic under the null-hypothesis that the CDFs are the same and then observe how extreme the maximum distance is on the sample. To illustrate the KS test, we use it to test the normality of the underlying distributions for two samples - one from a logistic distribution, one from a standard normal distribution. And then to test if the two samples come from the same distribution: set.seed(0) x1 &lt;- rlogis(80, 0, 1) x2 &lt;- rnorm(80, 0, 1) ks.test(x1, y = &quot;pnorm&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: x1 ## D = 0.1575, p-value = 0.03362 ## alternative hypothesis: two-sided ks.test(x2, y = &quot;pnorm&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: x2 ## D = 0.070067, p-value = 0.801 ## alternative hypothesis: two-sided ks.test(x1, x2) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x1 and x2 ## D = 0.175, p-value = 0.173 ## alternative hypothesis: two-sided So, with a 5% confidence level (95% confidence), we would reject the null hypothesis that our sample x1 is from a standard normal distribution. On the other hand, we would not reject that our sample x2 is from a standard normal distribution. Finally, we would not reject the null hypothesis that x1 and x2 come from the same distribution. The only guarantee that comes with these results is that we will in the long run falsely reject a true null-hypothesis at most 5% of the time. It says very little about our overall performance, because we do not know the ratio of cases when the null-hypothesis will be true. This example also illustrates the complexity of interpreting NHST results or rather all the tempting traps laid out for us - we might be tempted to conclude, based on the high p-value, that x2 does indeed come from a standard normal, but that then leads us to a weird predicament that we are willing to claim that x1 is not standard normal, x2 is standard normal, but we are less sure that x1 and x2 have different underlying distributions. Note that typical implementations of the KS test assume that the underlying distributions are continuous and ties are therefore impossible. However, the KS test can be generalized to discrete and mixed distributions (see R package KSgeneral). Differences in between distributions can also be assessed visually, through the QQ-plot, a plot that compares the quantiles of the two distributions. If the distributions have the same shape, their quantiles, plotted together, should lie on a line. The samples from the logistic distribution obviously deviate from the theoretical quantiles of a normal distribution: tmp &lt;- data.frame(y = x1) ggplot(tmp, aes(sample = y)) + stat_qq() + stat_qq_line() The Chi-squared goodness-of-fit (CHISQ) test is a non-parametric test for testing the equality of two categorical distributions. The CHISQ test can also be used on discrete or even continuous data, if there is a reasonable way of binning the data into a finite number of bins. The test statistic is based on a similar idea as the KS test statistic, but instead of observing just the maximum difference, we sum the squared difference between the relative frequency of the two distributions for a bin across all bins. We illustrate the CHISQ test by testing the samples for a biased coin against a theoretical fair coin and the samples from an unbiased 6-sided die against a theoretical fair 6-sided die. set.seed(0) x &lt;- table(sample(0:1, 30, rep = T, prob = c(0.7, 0.3))) x ## ## 0 1 ## 20 10 chisq.test(x, p = c(0.5, 0.5)) # the default is to compare with uniform theoretical, but we make it explicit here ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 3.3333, df = 1, p-value = 0.06789 x &lt;- table(sample(0:1, 40, rep = T, prob = c(0.7, 0.3))) x ## ## 0 1 ## 30 10 chisq.test(x, p = c(0.5, 0.5)) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 10, df = 1, p-value = 0.001565 x &lt;- table(sample(1:6, 40, rep = T)) x ## ## 1 2 3 4 5 6 ## 7 3 6 7 7 10 chisq.test(x) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 3.8, df = 5, p-value = 0.5786 So, with a 5% significance level (95% confidence), we would reject the null hypothesis that our coin is fair, but only in the case with 40 samples. Because fair or close-to fair coins have high entropy, we typically require a lot of samples to distinguish between their underlying probabilities. We would not reject the null-hypothesis that the die is fair. For a more real-world example, let us take the exit-poll data for the 2016 US Presidential election, broken down by gender, taken from here: n &lt;- 24588 male &lt;- round(0.47 * n * c(0.41, 0.52, 0.07)) # some rounding, but it should not affect results female &lt;- round(0.53 * n * c(0.54, 0.41, 0.05)) x &lt;- rbind(male, female) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) # this is also known as a contingency table and the subsequent test as a contingency test ## Clinton Trump other/no answer ## male 4738 6009 809 ## female 7037 5343 652 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 417.71, df = 2, p-value &lt; 2.2e-16 So, at any reasonable confidence level, we would reject the null-hypothesis and conclude that there is a difference in how men and women voted. In fact, we do not even need a test, because the difference is so obvious and the sample size so large. The differences between those who earned less or more than 100k$, however, appear smaller, so a test makes more sense: n &lt;- 24588 less100 &lt;- round(0.66 * n * c(0.49, 0.45, 0.06)) # some rounding, but it should not affect results more100 &lt;- round(0.34 * n * c(0.47, 0.47, 0.06)) x &lt;- rbind(less100, more100) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) ## Clinton Trump other/no answer ## less100 7952 7303 974 ## more100 3929 3929 502 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 9.3945, df = 2, p-value = 0.00912 Still, we can at most typical levels of confidence reject the null-hypothesis and conclude that that there is a pattern here as well. 5.2 Descriptive statistics for bivariate distributions When dealing with a joint distribution of two variables (that is, paired samples), the first thing we are typically interested in is dependence between the two variables or lack thereof. If two distributions are independent, we can summarize each separately without loss of information. If they are not, then the distributions carry information about eachother. The predictability of one variable from another is another (equivalent) way of looking at dependence of variables. The most commonly used numerical summary of dependence is the Pearson correlation coefficient or Pearson’s \\(\\rho\\). It summarizes the linear dependence, with \\(\\rho = 1\\) and \\(\\rho = - 1\\) indicating perfect colinearity (increasing or decreasing) and \\(\\rho = 0\\) indicating linear independence. As such, Pearson’s \\(\\rho\\) is directly related (the squared root) to the coefficient of determination \\(R^2\\), a goodness-of-fit measure for linear models and the proportion of variance in one explained by the other variable. An important consideration is that the statement that linear independence implies independence is not true in general (the converse implication is). One notable exception where this implication is true is the multivariate Normal distribution, where the dependence structure is expressed through linear dependence only. Two of the most popular alternatives to Pearson’s \\(\\rho\\) are Spearman’s \\(\\rho\\) and Kendalls \\(\\tau\\). The former measures the degree to which one variable can be expressed as monotonic function of the other. The latter measures the proportion of concordant pairs among all possible pairs (pairs (x1,y1) and (x2, y2), where if x1 &gt; x2 then y1 &gt; y2). As such, they can capture non-linear dependence and are more appropriate for data with outliers or data where distance might have no meaning, such as ordinal data. Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) are more robust but do not have as clear an interpretation as Pearson’s \\(\\rho\\). Kendall’s tau is also computationally more expensive. Below are a few examples of bivariate samples that illustrate the strengths and limitations of the above correlation coefficients: set.seed(0) library(MASS) m &lt;- 100 dat &lt;- NULL # data 1 sigma &lt;- matrix(c(1, 0, 0, 1), nrow = 2) x &lt;- mvrnorm(m, mu = c(0, 0), Sigma = sigma ) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 2 sigma &lt;- matrix(c(1, -0.5, -0.5, 1), nrow = 2) x &lt;- mvrnorm(m, mu = c(0, 0), Sigma = sigma ) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 3 sigma &lt;- matrix(c(1, 0.95, 0.95, 1), nrow = 2) x &lt;- mvrnorm(m, mu = c(0, 0), Sigma = sigma ) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 4 x &lt;- rnorm(m, 0, 1) y &lt;- 2 * pnorm(x) + rnorm(m, 0, 0.05) x &lt;- cbind(x, y) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 5 sigma1 &lt;- matrix(c(1, -0.5, -0.5, 1), nrow = 2) sigma2 &lt;- matrix(c(0.1, 0, 0, 0.1), nrow = 2) x &lt;- rbind(mvrnorm(m, mu = c(0, 0), Sigma = sigma ), mvrnorm(50, mu = c(3, -2), Sigma = sigma2 )) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 6 z &lt;- runif(m, 0, 2*pi) x &lt;- cbind(cos(z), sin(z)) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) ggplot(dat, aes(x = x, y = y)) + geom_point() + facet_wrap(.~example, ncol = 3, scales = &quot;free&quot;) Like similar questions about other parameters of interest, the question Is a strong correlation? is a practical question. Unless the correlation is 0 (no correlation) or 1/-1 (perfectly correlated, can’t be more correlated than this), the meaning of the magnitude of correlation depends on the practical setting and its interpretation depends on some reference level. Even a very low correlation, such as 0.001 (if we are reasonably sure that it is around 0.001) can be practically meaningful. For example, if it is correlation between neighboring numbers in sequences generated by a uniform random number generator (RNG), that would be more than enough correlation to stop using this RNG. 5.3 Further reading and references For a more comprehensive treatment of the most commonly used summarization techniques see: Holcomb, Z. C. (2016). Fundamentals of descriptive statistics. Routledge. More on the practice of summarization techniques and hypothesis testing: Bruce, P., &amp; Bruce, A. (2017). Practical statistics for data scientists: 50 essential concepts. \" O’Reilly Media, Inc.\". (Chapters 1 and 3) 5.4 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Recognize when a type of summary is appropriate and when it is not. Apply data summarization techiques to obtain insights from data. Once introduced to the bootstrap and other estimation techniques, to be able to combine descriptive statistics with a quantification of uncertainty, such as confidence intervals. 5.5 Practice problems Download the Football Manager Players dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the summarization techniques from this chapter. Find one or more real-world examples (data sets) where a standard summary of univariate or bivariate data fails. That is, where important information is lost in the summary. "],
["docker.html", "Chapter 6 Docker container platform 6.1 Why Docker? 6.2 Setting up the environment 6.3 Short introduction to Docker 6.4 Further reading and references 6.5 Learning outcomes 6.6 Practice problems", " Chapter 6 Docker container platform For extreme portability and scalability of our applications there exist tools that can package together the entire required application environment, such as library dependencies or language runtime. Docker is a set of tools that use operating system-level virtualization to develop and deliver software in packages called containers. The software that hosts the containers is called Docker Engine and was first started in 2013 by Docker, Inc. Docker products are available both in Community and Enterprise editions. Docker containers are isolated from each other and they bundle their own software, libraries and configuration files. If our application consists of multiple services, containers can communicate with each other through internal or external network configuations. All containers are run by a single host operating system kernel and are thus more lightweight than virtual machines. They are instantiated from Docker images that specify their precise contents. We often create images by combining and modifying existing standard images downloaded from public repositories, also known as image registries. An instance of an image (a container) thus contains configured networking, storage, logging, etc. Furthermore, Docker defines an abstraction for these machine-specific settings and the exact same Docker container can run without any changes on many different machines with many different configurations. There exist other tools similar to Docker, which may not be that well known or are used for different scenarios, such as Singularity, Nanobox or LXC. Another approach of bundling applications into separate environment is using virtual machines, where each virtual machine contains its own operating system. Most common virtualization technologies are VMWare products or Virtualbox. A technology in-between both worlds is Vagrant, which enables a user to script the entire environment but when running the app, Vagrant creates a separate virtual machine (e.g. using Virtualbox) and runs it. 6.1 Why Docker? Author of a Linux.com article (Noyes 2008) described Docker as follows: Docker is a tool that can package an application and its dependencies in a virtual container that can run on any Linux server. This helps enable flexibility and portability on where the application can run, whether on premises, public cloud, private cloud, bare metal, etc. Virtual machines and containers have similar resource isolation and allocation benefits, but containers virtualize the operating system whereas virtual machines virtualize hardware. Therefore, containers are more portable and efficient. Multiple containers can run on the same machine and share the operating system kernel with other containers, each running as isolated processes in user space. Containers images are typically tens of MBs in size, which is much than typical virtual machines. Containers can handle more applications and require fewer operating systems. Virtual machines as an abstraction of physical hardware turn one server into many servers. The hypervisor allows multiple virtual machines to run on a single machine. Each virtual machine contains a full copy of an operating system, the application, necessary binaries and libraries. This requires GBs of data - virtual machines require more disk space, physical resources and are slower to boot. The Figure below shows a comparison between multiple running Docker containers (left hand side) and multiple virtual machines (right hand side): Figure 6.1: Dockerized applications vs. VM-virtualized applications (image courtesy of docker.com). 6.2 Setting up the environment To install Docker on your machine, you need to download the appropriate Docker Engine Community distribution from the official download website. There also exists a Docker Machine installation package, which is a provisioning and installation software for Docker - we do not need it at this point as it helps us manage multiple remote Docker hosts. After first installation we can explore different installation settings and tools. Kitematic is a tool which provides a handy GUI to overview downloaded images, containers, volumes and their settings. We can use it as a side-tool for command-line commands we introduce below. We can check our Docker installation by running docker info to view some status details. Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 18.09.2 Storage Driver: overlay2 ... If we experience permission problems, we need to allow our user to work with Docker. Follow the post-installation guidelines. Now we are ready to test the Docker installation by running the command docker run hello-world. If we see a similar message to the one below, we are ready to begin. Unable to find image &#39;hello-world:latest&#39; locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:6540fc08ee6e6b7b63468dc3317e3303aae178cb8a45ed3123180328bcc1d20f Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. ... 6.3 Short introduction to Docker 6.3.1 Basics A Docker container is a runtime instance of an image - that is, an image with state, or a user process. You can see a list of available images using the command docker image ls: REPOSITORY TAG IMAGE ID CREATED SIZE docker-app latest 4b727c80cf90 4 minutes ago 475MB application latest cd497ca7013b 19 hours ago 538MB database latest 428448bc5e2d 19 hours ago 373MB ubuntu 18.04 4c108a37151f 4 weeks ago 64.2MB mysql 5 a1aa4f76fab9 5 weeks ago 373MB hello-world latest fce289e99eb9 6 months ago 1.84kB A container is launched by running an image. An image is an executable package that includes everything needed to run an application - the code, a runtime, libraries, environment variables, and configuration files. We have seen the docker run command that takes image name as a parameter. Image name looks like [username/]repository[:tag], where tag is latest by default. To see all the containers on your machine, issue the command docker container ls --all (the parameter –all will also list stopped containers): CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f6a55e492493 docker-app &quot;/bin/sh -c &#39;python3…&quot; 14 seconds ago Up 13 seconds 0.0.0.0:8787-&gt;8787/tcp kind_blackwell 42b33bc532ab hello-world &quot;/hello&quot; 22 minutes ago Exited (0) 22 minutes ago unruffled_morse 6.3.2 Docker application example To better understand everything, let’s develop a simple web application and run it from within a docker container (solution of this part is available in folder app_only in the GitHub repository). We have a simple Python web server implementation (server.py) from flask import Flask from flask import json from flask import request from flask import Response app = Flask(__name__) @app.route(&#39;/&#39;, methods=[&#39;GET&#39;]) def index(): content = open(&quot;index.html&quot;).read() return Response(content, mimetype=&quot;text/html&quot;) if __name__ == &#39;__main__&#39;: app.run(host=&#39;0.0.0.0&#39;, port=8787) and accompanying HTML web template (index.html) &lt;html&gt; &lt;head&gt; &lt;title&gt;..:: Sample application ::..&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css&quot; integrity=&quot;sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO&quot; crossorigin=&quot;anonymous&quot;&gt; &lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js&quot; integrity=&quot;sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://code.jquery.com/jquery-3.4.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; $( document ).ready(function() { employees = [ {&quot;name&quot;: &quot;John Doe&quot;, &quot;hobbies&quot;: &quot;I like cycling, mountain biking and skiing!!!&quot;, &quot;role&quot;: &quot;Director of operations&quot;}, {&quot;name&quot;: &quot;Melanie Oesch&quot;, &quot;hobbies&quot;: &quot;As the best jodeling singer, the love of my life is singing all day long. I come from Switzerland and have achieved many prizes. Maybe I also visit your hometown and get to know you.&quot;, &quot;role&quot;: &quot;Chairman of kids programme&quot;}, {&quot;name&quot;: &quot;Vladimir Zookeeper&quot;, &quot;hobbies&quot;: &quot;Animals are the nicest and very polite creatures in our world. I have observed many species already and I have not found an animal that would harm me without a reason (in comparison to a human being). My dream is to play with animals every day.&quot;, &quot;role&quot;: &quot;Animal feeder&quot;} ]; function addEmployee(employee) { $(&quot;#personnelListing&quot;).append(` &lt;a href=&quot;#&quot; class=&quot;list-group-item list-group-item-action flex-column align-items-start&quot;&gt; &lt;div class=&quot;d-flex w-100 justify-content-between&quot;&gt; &lt;h5 class=&quot;mb-1&quot;&gt;${employee.name}&lt;/h5&gt; &lt;/div&gt; &lt;p class=&quot;mb-1&quot;&gt;${employee.hobbies}&lt;/p&gt; &lt;small&gt;${employee.role}&lt;/small&gt; &lt;/a&gt; `); } $.each(employees, function( index, employee ) { addEmployee(employee); }); function processForm() { employee = { &quot;name&quot;: $(&quot;#name&quot;).val(), &quot;hobbies&quot;: $(&quot;#hobbies&quot;).val(), &quot;role&quot;: $(&quot;#role&quot;).val() }; addEmployee(employee); $(&quot;#name&quot;).val(&quot;&quot;), $(&quot;#hobbies&quot;).val(&quot;&quot;), $(&quot;#role&quot;).val(&quot;&quot;) } // Fetch all the forms we want to apply custom Bootstrap validation styles to var forms = document.getElementsByClassName(&#39;needs-validation&#39;); // Loop over them and prevent submission var validation = Array.prototype.filter.call(forms, function(form) { form.addEventListener(&#39;submit&#39;, function(event) { event.preventDefault(); event.stopPropagation(); if (form.checkValidity() === true) { processForm(); form.classList.remove(&#39;was-validated&#39;); } else { form.classList.add(&#39;was-validated&#39;); } }, false); }); }); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;h1&gt;Personnel listing&lt;/h1&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;h2&gt;Add an employee&lt;/h2&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;form id=&quot;newEmployeeForm&quot; class=&quot;needs-validation&quot; novalidate&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;name&quot;&gt;Name&lt;/label&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;Enter your name&quot; required&gt; &lt;div class=&quot;invalid-feedback&quot;&gt; Please provide a valid city. &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;hobbies&quot;&gt;Hobbies&lt;/label&gt; &lt;textarea rows=&quot;3&quot; class=&quot;form-control&quot; id=&quot;hobbies&quot; placeholder=&quot;Describe your hobbies&quot; required&gt;&lt;/textarea&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;role&quot;&gt;Role&lt;/label&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; id=&quot;role&quot; placeholder=&quot;Role in the organization&quot; required&gt; &lt;/div&gt; &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Save&lt;/button&gt; &lt;/form&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;h2&gt;Employees&lt;/h2&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;div class=&quot;list-group&quot; id=&quot;personnelListing&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; To run the application above, install the Flask library in your Python 3 environment and run the server as python server.py: * Serving Flask app &quot;server&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:8787/ (Press CTRL+C to quit) Your web application is now accessible at http://localhost:8787. It is a simple JS-based we page, where you can add new employees into a local JS list. 6.3.2.1 Dockerfiles To define a container and be able to create an image, we must write a special file, called Dockerfile. Dockerfile defines what goes on in the environment inside your container. Access to resources like networking interfaces and disk drives is virtualized inside this environment, which is isolated from the rest of our system, so we need to map ports to the outside world, and be specific about what files we want to “copy in” to that environment. After doing that, we can expect that the build of our application defined in this Dockerfile behaves identically wherever it runs. Let’s create a file named Dockerfile with the following content: # Use an official Ubuntu runtime as a parent image FROM ubuntu:18.04 # Set the current working directory to /work WORKDIR /work # Copy the current directory contents into the container at /work ADD ./ . # Install and configure your environment RUN apt-get update \\ &amp;&amp; apt-get install -y python3 python3-pip \\ &amp;&amp; pip3 install flask # Make port 8787 available to the world outside this container (i.e. docker world) EXPOSE 8787 # Run server.py when the container launches ENTRYPOINT python3 server.py A full list of Dockerfile commands is described at the official Dockerfile documentation. If there is a need to prepare some specifics in the environment, it can be tedious to manually write all the RUN commands and check whether the application runs as expected. We can therefore always directly run the parent image and enter it’s shell to manually prepare the environment. We then copy the working commands into a Dockerfile and build our image. To create a container from a specific image and access it’s shell, run the following command: docker run -it ubuntu:18.04 /bin/bash. 6.3.2.2 Adding everything together Now we have the files server.py, index.html and Dockerfile in the same folder. If we move to the same folder, we can build a Docker image named docker-app using the following command docker build -t docker-app .: Sending build context to Docker daemon 9.728kB Step 1/6 : FROM ubuntu:18.04 ---&gt; 4c108a37151f .... Successfully built 4b727c80cf90 Successfully tagged docker-app:latest Now the image is available in our Docker system (verify that using docker image command as we did above). To create a running container based on our image, run docker run -p 8787:8787 docker-app: * Serving Flask app &quot;server&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:8787/ (Press CTRL+C to quit) With this command we also map the host’s port 8787 to the Docker container’s port 8787. For additional options of docker run command, see the official reference. Our web application is now accessible at the same address as before, but is now running in a Docker container! Sometimes we would still like to directly access the shell of a running Docker container and check something (another option would be to install an ssh server into the container and map a port to the host). First, we get the ID of our container using the command docker ps, then we execute docker exec -it CONTAINER_ID /bin/bash. Now we are connected directly to the “machine” that hosts our web application. If our running application generates logs, we can easily access them using docker logs CONTAINER_ID. 6.3.3 Volumes By default all files created inside a container are stored on a writable container layer. Note that: The data doesn’t persist when that container no longer exists and it can be difficult to get the data out of the container if another process needs it. A container’s writable layer is tightly coupled to the host machine on which the container is running. You can’t easily move the data somewhere else. Writing into a container’s writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. This extra abstraction reduces performance as compared to using data volumes, which write directly to the host filesystem. As a result, best practices are to always create containers read-only. Docker thus provides two solutions to store files, which are persisted also after the container removal - volumes and bind mounts. Volumes are the best way to persist data in Docker. Commands to manage Docker volumes start with docker volume (add –help parameter to list all options). After the volume is created, we can map it to a specific mount point when running the container (see docker run command reference for more). 6.3.4 Docker application example with multiple services When developing an application that consist of multiple services, we can also create one “fat” container hosting all the services. This is not the Docker best practice as therefore each service should be run within a separate Docker container. Let’s upgrade our web application and add database support to it (solution of this part is available in folder app_and_database in the GitHub repository). 6.3.4.1 Web application service The web application service is similar to the example above. The differences are the installation of an additional mysql-connector-python library into the container (Dockerfile), implementation of REST calls to the backend (index.html) and server endpoints implementation (server.py). In the implementation we should notice the connection settings from the application to the database. As we said, both services will run separately in a Docker container, so we should provide a connection. When we run the services, we should name the containers and create a network among running services. Connection settings look like: config = { &#39;user&#39;: &#39;root&#39;, &#39;password&#39;: &#39;BeReSeMi.DataScience&#39;, &#39;host&#39;: &#39;database&#39;, &#39;database&#39;: &#39;data_science&#39;, &#39;raise_on_warnings&#39;: False } Note that the host setting and port 3306 are used to connect to a MySQL database. In the MySQL container that port will also needed to be exposed within the Dockerfile. Docker services can be connected using separate virtual networks and by default, bridge networks are created to interconnect running containers. For more sophisticated examples, see the official network setting guidelines. 6.3.4.2 Database service For the database service, we will use already created image, which is available from the public MySQL image repository (for more info on repositories, see section 6.3.4.4). Along with the image, there are also instructions of how to use it with the parameters, where to put initial script files, versions, Dockerfile sources and other references, which ease the deployment process without creating complicated Dockerfiles by ourselves. In our case the database dockerfile looks as follows: # Parent image FROM mysql:5 # Copying all files in curent dir to container ADD ./ /docker-entrypoint-initdb.d # Updating permissions for the copied files RUN /bin/bash -c &#39;chmod a+rx /docker-entrypoint-initdb.d/*&#39; Observe that we only copied files into a special folder in the container (/docker-entrypoint-initdb.d) which was previously set as init folder, where all the .sql scripts are run at startup. To test our Dockerfile, we can also run it separately using the command (from the database folder) docker build -t docker-db . &amp;&amp; docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=BeReSeMi docker-db. We set the database password of the root user using an environment variable, which can be used to connect to the database. Now we can connect to the database from the host machine - e.g. use MySQL Workbench and connection settings to localhost:3306. A database named data_science should now be created along with the employees table and few entries. 6.3.4.3 Docker compose Now we know how to create docker containers separately. What remains is to connect them and run them more easily. We will utilize the Docker compose tool. Docker Compose is a tool for defining and running multi-container Docker applications. It uses YAML files to configure the application’s services and performs the creation and start-up process of all the containers with a single command. The docker-compose CLI utility allows us to run commands on multiple containers at once, for example, building images, scaling containers, running containers that were stopped, and more. The docker-compose.yml file is used to define an application’s services and includes various configuration options. Before proceeding, we’ll stop all running containers and move to the app_and_database folder. First, let’s check the docker-compose.yml file: version: &#39;3.7&#39; services: database: image: database build: ./database command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: BeReSeMi.DataScience ports: - 3306:3306 volumes: - ds_databases:/var/lib/mysql networks: - dsnet application: image: application build: ./application depends_on: - database restart: always ports: - &quot;8787:8787&quot; networks: - dsnet networks: dsnet: driver: bridge volumes: ds_databases: In the docker-compose configuration we define two containers, one named database and other application. Parameter build defines the folder with an appropriate Dockerfile or directly an already built image name. Parameter restart instructs the container to automaticallt restart its service if it suddenly stops due to crash or error. Parameter depends_on created dependencies between containers, so the application container will be started after the database container is running. Be aware that this is based on the time when service is initializing, so if database script initialization takes a long time, the database service may run later than the application service. With the volumes parameter we map a specific folder in the container to a named volume db_databases - this folder will contain the database data. We also define a network named dsnet with the bridge configuration and therefore both containers will be able to communicate with each other. To run the docker-compose configuration, run docker-compose up (for detached mode add parameter -d at the end). Both containers should be created and running - to verify, navigate to http://localhost:8787 and add some employees. Data should be persistent and stored into the database. To shutdown the containers, press CTRL+C (or run docker-compose down if you started containers in a detached mode). 6.3.4.4 Image registries We have already mentioned that there exist prepared images, which can be retried from public or private repositories. The two main public registries are Docker Hub and Docker Cloud. Docker Hub is the default registry where Docker looks for images. Docker clients connect to Docker repositories to download (pull) images for use or upload (push) images that they have built. We have published the image of the first example in the repository szitnik/docker-ds-app (link). You can pull the image (docker pull command) or run it directly with the command: docker run -p 8787:8787 szitnik/docker-ds-app The functionality should be the same as with the image you created during this tutorial. To publish images to Docker Hub, we first need to create and account. For example, we could take the built image docker-app from the first example and push it to the Docker Hub using the following commands: # Provide credentials and login docker login # Tag your local image docker tag docker-app USERNAME/PUBLIC_IMAGE_NAME:TAG # Publish your image docker push USERNAME/PUBLIC_IMAGE_NAME:TAG If we do not define the tag, it will be latest by default. We can access our image via Web interface at https://hub.docker.com/r/USERNAME/PUBLIC_IMAGE_NAME, where we can also add a description, instructions, publish the Dockerfile or connect its content to your Git repository. Now anybody can pull &amp; run our image using the above procedure. 6.3.5 Dockerfile optimizations The Dockerfile for the image running the Flask server we created above is not written completely according to good practices: Copying project files as a first step invalidates Docker cache and causes all others step to execute again. This means that it will install all further requirements, which causes a significant delay as it has to download and install all of them. In Dockerfiles, a proper ordering of commands in very important. Files (as they change frequently) have to be copied into the container as late as possible. For Python it is a good practice to use virtualenv even in the container. But using virtualenv in the container may introduce some adaptations in how to activate the environment and run an application in it. Python modules in the official Ubuntu repository are old. It is a good practice (very important in production environments!) to lock dependency versions in a requirements.txt file. If you do not lock the versions, the build is not deterministic as it can brake later when a new version of a package is introduced. Below we show an updated version of a Dockerfile: FROM ubuntu:18.04 # Install Python RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends python3 python3-virtualenv # Create virtualenv ENV VIRTUAL_ENV=/opt/venv RUN python3 -m virtualenv --python=/usr/bin/python3 $VIRTUAL_ENV ENV PATH=&quot;$VIRTUAL_ENV/bin:$PATH&quot; # Install dependencies: COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt # Make port 8787 available to the world outside this container (i.e. docker world) EXPOSE 8787 # Copy the application: COPY server.py . COPY index.html . # Run the application CMD [&quot;python3&quot;, &quot;server.py&quot;] and a requirements.txt file: Flask==1.1.1 6.4 Further reading and references Check the Docker official web page and the Docker tutorial. For more advanced topics get to know container orchestration solutions like Docker Swarm or Kubernetes. Docker in Action book teaches readers how to create, deploy, and manage applications hosted in Docker containers. The Docker Book is inteded for complete beginners offering 268 pages of demos and live tutorials. Docker Cookbook presents more advanced Docker techniques like mounting data across multiple servers, distributed containers, detailed monitoring, networking across multiple hosts, accessing Docker in the cloud and merging with other platforms like Kubernetes. 6.5 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use and extend existing Docker image. Package an application consisting of multiple services into one or more containers and manage them. Publish reproducible algorithms to Docker Hub. 6.6 Practice problems Create a simple Python program or web application, write a Dockerfile to compile an image an publish it on Docker Hub. If you are more proficient with some other language, use a framework of your choice. Write a service which needs multiple servers deployed and run everything using Docker Compose. Dockerize your Introduction to Data Science project and publish it to Docker Hub. Add instructions of how to use your image. References "],
["summarize2.html", "Chapter 7 Summarizing data - visualization 7.1 Histograms and density plots 7.2 Bar plot 7.3 Pie chart 7.4 Scatterplot 7.5 2D density plot 7.6 Boxplot 7.7 Violin plot 7.8 Correlogram 7.9 A comprehensive summary 7.10 Further reading and references 7.11 Learning outcomes 7.12 Practice problems", " Chapter 7 Summarizing data - visualization Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis. In this chapter we will focus on the basic visualization techniques for univariate and bivariate data. Advanced data summarization techniques will be covered in a later chapter. We will be using R and ggplot2 but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools! In most examples in this section we’ll be using the NBA players dataset that contains some basic information about NBA players in the period up to year 2009. library(ggplot2) dat &lt;- read.csv(&quot;./data/NBAplayers.csv&quot;) dat &lt;- dat[complete.cases(dat),] dat$height &lt;- dat$h_feet * 30.48 + dat$h_inches * 2.54 # in cm dat$weight &lt;- dat$weight * 0.4536 # in kg summary(dat) ## ilkid firstname lastname position ## Length:3906 Length:3906 Length:3906 Length:3906 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## firstseason lastseason h_feet h_inches ## Min. :1946 Min. :1946 Min. :5.000 Min. :-6.000 ## 1st Qu.:1967 1st Qu.:1970 1st Qu.:6.000 1st Qu.: 3.000 ## Median :1980 Median :1985 Median :6.000 Median : 6.000 ## Mean :1979 Mean :1983 Mean :6.022 Mean : 5.581 ## 3rd Qu.:1995 3rd Qu.:2001 3rd Qu.:6.000 3rd Qu.: 8.000 ## Max. :2009 Max. :2009 Max. :7.000 Max. :11.500 ## weight college birthdate height ## Min. : 60.33 Length:3906 Length:3906 Min. :160.0 ## 1st Qu.: 83.92 Class :character Class :character 1st Qu.:190.5 ## Median : 92.99 Mode :character Mode :character Median :198.1 ## Mean : 93.91 Mean :197.7 ## 3rd Qu.:102.06 3rd Qu.:205.7 ## Max. :149.69 Max. :231.1 Visual summaries are often a more informative, faster and more concise alternative to numerical summaries. This will also be our guideline for improving our visualizations. Can we convey the same information in substantially less time/space? Can we convey more information without using more time/space? If the answer is yes, then we should! We’ll illustrate this point with an example that features some common mistakes or inefficiencies people do when visualizing data. Our goal will be to summarize how the average height of NBA Centers, Forwards, and Guards has changed over time. We’ll focus on rookies - for each year we’ll look at the players that started their NBA career that year. Let’s plot the averages over time: tmp &lt;- dat[dat$position == &quot;G&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean)) tmp &lt;- dat[dat$position == &quot;F&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean)) tmp &lt;- dat[dat$position == &quot;C&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean)) The plots reveal information that the average height of rookies has been increasing over time for all three groups of players. We should immediately recognize that we are wasting space by plotting each plot separately. We are also wasting the reader’s time, having to jump back and forth from one plot to another when making comparisons. So, by plotting all three groups together, we save space and allow the reader to make inter-group comparisons more efficiently. Remember that how we present our results is how we treat our reader. These kind of plots say to the reader that our time and convenience are more important than theirs. Let’s remedy this mistake: tmp &lt;- dat[dat$position == &quot;G&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean), col = &quot;green&quot;, ylim = c(180, 220)) tmp &lt;- dat[dat$position == &quot;F&quot;,] points(tapply(tmp$height, tmp$firstseason, mean), col = &quot;red&quot;) tmp &lt;- dat[dat$position == &quot;C&quot;,] points(tapply(tmp$height, tmp$firstseason, mean), col = &quot;black&quot;) This plot uses up only a third of the space and simplifies comparison. This reveals new information, such as that centers (black) are higher than forwards (red) who are higher than guards (green), that there was a more steep increase in the beginning of the period and that forwards these days are as high on average as centers were in the past. However, there are several things that we can still improve on. The first will be one of the fundamental rules of statistical plotting - always label your axes! The reader should never look elsewhere for information about what is plotted. We will also take this opportunity to reorganize our data: library(reshape2) tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) plot(tmp$year, tmp$height, col = tmp$position, xlab = &quot;year&quot;, ylab = &quot;average rookie height (cm)&quot;) This is starting to look better. However, we should also include the legend - if we describe the meaning of the colors in the figure caption (or worse, in text), the reader will have to jump from text to figure, wasting time. Additionally, some people (and publications) prefer to add a title to their plot, explaining concisely what is in it, therefore making it more self containes. Others prefer to explain the plot in the caption. We’ll add a title: tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) plot(tmp$year, tmp$height, col = tmp$position, xlab = &quot;year&quot;, ylab = &quot;average rookie height (cm)&quot;) legend(1997, 190, legend=c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;), col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;), lty = 1, cex=0.8) title(&quot;The average height of NBA rookies by playing position over time.&quot;) This is now a quite decent and self-contained plot. Next, we’ll add a bit of polish. Pleasing aesthetics might not add much to the informativeness of a plot, but they do make our work look more professional. They are also indicate that we put in the extra effort. Of course, we should never let aesthetics get in the way of efficiency and informativeness (see pie-chart example in the following section): tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) levels(tmp$position) &lt;- c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;) ggplot(tmp, aes(x = year, y = height, colour = position)) + geom_point() + xlab(&quot;year&quot;) + ylab(&quot;average rookie height (cm)&quot;) + ggtitle(&quot;The average height of NBA rookies by playing position over time.&quot;) + theme_bw() By using ggplot2 we can make our visualizations look better, but it is also very convenient for adding some extra layers to our plots. Additional layers are draw on top of eachother, in order. For example, let’s add a smoothed line with standard errors to help us focus on the trend and not the individual data points: tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) levels(tmp$position) &lt;- c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;) ggplot(tmp, aes(x = year, y = height, colour = position)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;year&quot;) + ylab(&quot;average rookie height (cm)&quot;) + ggtitle(&quot;The average height of NBA rookies by playing position over time.&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; There is one more thing that we can typically do in such cases - label the data directly and omit the legend. Always keep related objects visually close! This saves both space and user’s time, especially if we have several lines/colors in our plot: tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) levels(tmp$position) &lt;- c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;) ggplot(tmp, aes(x = year, y = height, colour = position)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;year&quot;) + ylab(&quot;average rookie height (cm)&quot;) + ggtitle(&quot;The average height of NBA rookies by playing position over time.&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + annotate(&quot;text&quot;, x = 1970, y = 212, label = &quot;centers&quot;, colour = &quot;red&quot;) + annotate(&quot;text&quot;, x = 1970, y = 203, label = &quot;forwards&quot;, colour = &quot;darkgreen&quot;) + annotate(&quot;text&quot;, x = 1970, y = 192, label = &quot;guards&quot;, colour = &quot;blue&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 7.1: The data show that the average heights of all three groups of players have been increasing. The difference between forwards and centers is approximately the same throughout the period while the average height of guards has been increasing at a slower pace. Note that the lines are loess smoothed lines with standard error estimates. We’ve equipped the above plot with a caption that states the information that we would like to point out to the reader (the plot serves as a visual summary and argument). We’ve also made the points transparent, to put more focus on the lines. There is always something we can tweak and improve in a plot, depending on the situation, but if all of our plots will be at least at the level of this plot, that will be of sufficient standards. 7.1 Histograms and density plots The most elementary way of summarizing data is to plot their density. Of course, the true density is unknown and we can only estimate it by using a model or a non-parametric (smoothing) kernel density estimation. A histogram (binning the data and plotting the frequencies) can be viewed as a more coarse or discrete way of estimating the density of the data. In both density plots and histograms we need to specify the amount of smoothing (smoothing kernel width or bin size) - most built-in functions do it for us, but there is no optimal way of doing it, so we can often improve the plot by selecting a more appropriate degree of smoothing. When we have more data, we can get away with less smoothing and reveal more characterisics of the underlying distribution. We illustrate these two plots by summarizing the NBA players’ weight: ggplot(dat, aes(x = weight)) + geom_histogram(aes(y=..density..), alpha=0.5, position=&quot;identity&quot;, binwidth = 7) + geom_density(lwd = 1, col = &quot;black&quot;) + theme_bw() Figure 7.2: Histogram and density estimation of NBA player weight 7.2 Bar plot Bar plots are the most common choice for summarizing the (relative) frequencies for categorical or ordinal data with a manageable number of unique values. It is similar to a histogram, except that the categories/values provide a natural way of binning the data: set.seed(0) tmp &lt;- data.frame(University = dat$college) x &lt;- table(tmp) x &lt;- x[x &gt;= 5] x &lt;- sample(x, 10, rep = F) x &lt;- sort(x) ggplot(data.frame(x), aes(x = tmp, y = Freq)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + xlab(&quot;University&quot;) + ylab(&quot;number of players&quot;) Figure 7.3: The number of NBA players that came from these 10 Universities. When the number of unique values is large, it will not be not possible to visualize all of them (try visualizing the frequencies for all universities in the above example). In such cases we may opt to group some values or show only certain values. 7.3 Pie chart Pie charts are quite possibly the easiest chart type to work with, because there is only one rule to using pie charts - don’t use pie charts. Let’s visualize the data from the bar chart example: y &lt;- x / sum(x) ggplot(data.frame(y), aes(x = &quot;&quot;, y = Freq, fill = tmp)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) + theme_bw() + labs(fill = &quot;University&quot;) Figure 7.4: The relative frequencies of NBA players that came from these Universities. The pie chart is a great example (and warning!) of how aesthetics can get in the way of function and effectiveness. It is well documented that people are poor at comparing areas and especially angles. Could you recognize quickly from the above pie chart that University of Washington gave approximately twice as many players as University of Texas? How quickly would you be able to judge these relationships from the bar chart? Angles can also play tricks on our eyes. Which color pie slice is the largest on the first plot below? Which on the second plot? y &lt;- data.frame(Name = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), Value = rep(1, 3)) ggplot(data.frame(y), aes(x = &quot;&quot;, y = Value, fill = Name)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) + theme_bw() + theme(legend.position = &quot;none&quot;) ggplot(data.frame(y), aes(x = &quot;&quot;, y = Value, fill = Name)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=pi/3) + theme_bw() + theme(legend.position = &quot;none&quot;) For more information on how people perceive visual objects and relationships between them (distances, angles, areas), we recommend the pioneering work of Cleveland and McGill (see Further reading). Note that while a pie chart (the same applies to donut charts) is arguably a poor chart from a statistical graphics perspective, it is visually appealing and more engaging. That is, the pie chart still has merit as a techinque for visual presentation of information as witnessed by its widespread use. 7.4 Scatterplot The scatterplot is the most common plot for summarizing the relationship between two numerical variables. In this chapter we’ve already seen several examples of scatterplots. Here, we use three of them to summarize the relationship between player weight and player height by position. ggplot(dat, aes(x = height, y = weight)) + geom_jitter(width = 3, alpha = 0.3) + theme_bw() + facet_wrap(.~position) Figure 7.5: Relationship between player weight and height by player position. Note that we introduced a bit of jitter - this is a common approach to dealing with numerical data where we have a limited number of unique values (such as rounded data) to reveal where we have more points. 7.5 2D density plot When individual points are of little interest and we just want to summarize the density of the joint distribution, a 2D density plot is a good alternative to the scatterplot: ggplot(dat, aes(x = height, y = weight, colour = position)) + geom_density_2d() + theme_bw() + theme_bw() + theme(legend.position = &quot;none&quot;) + annotate(&quot;text&quot;, y = 90, x = 212, label = &quot;centers&quot;, colour = &quot;red&quot;) + annotate(&quot;text&quot;, y = 80, x = 203, label = &quot;forwards&quot;, colour = &quot;darkgreen&quot;) + annotate(&quot;text&quot;, y = 73, x = 192, label = &quot;guards&quot;, colour = &quot;blue&quot;) Figure 7.6: Relationship between player weight and height by player position. Density plots should be read as maps - the contours represent points of equal height (in our case, density). Where contour lines lie closer together, we have a steeper hill. Note that drawing a density plot from a point cloud requires some sort of density estimation or smoothing. Statistical graphics packages typically use non-parameteric smoothing which has at least one relevant parameter with which we can control the amount of smoothing (for example, smoothing kernel width). The default or automatic selection of this parameter gives satisfactory results in most cases, but sometimes we can improve the plot by manually selecting the parameter value. 7.6 Boxplot The boxes (where the boxplot gets is name) typicaly summarize the quartiles of the data (1st, meadian and 3rd quartile). Sometimes, whiskers are used to represent the IQR (Inter-Quartile Range) and points beyond that range are plotted - the IQR is a common approach to defining what an outlier is. ggplot(dat, aes(x = position, y = weight)) + geom_boxplot(width = 0.1) + theme_bw() Figure 7.7: Summary of player weight by position. 7.7 Violin plot The boxplot shows only the quartiles so it can sometimes hide relevant information and mislead us. An alternative is to plot the entire density estimates. Such a plot is called a violin plot. ggplot(dat, aes(x = position, y = weight)) + geom_violin(fill = &quot;lightblue&quot;) + geom_boxplot(width = 0.05) + theme_bw() Figure 7.8: Summary of player weight by position. Note that we combined the violin plot with the boxplot to facilitate comparison. 7.8 Correlogram When we want to quickly inspect if there are any correlations between numerical variables, we can summarize correlation coefficients in a single plot. Such a plot is also known as a correlogram. Here we do it for the iris dataset: library(ggcorrplot) corr &lt;- round(cor(iris[,-5]),2) ggcorrplot(corr, hc.order = TRUE, type = &quot;lower&quot;, outline.col = &quot;white&quot;, lab = T) Figure 7.9: Summary of the numerical variables in the iris dataset. 7.9 A comprehensive summary Sometimes it will be useful to summarize the density/histogram of several numerical variables and the correlation between them. Here is an example of how we can combine histograms/density plots, scatterplots, and information about correlation: library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha pairs.panels(iris[,-5]) Figure 7.10: Summary of the numerical variables in the iris dataset. 7.10 Further reading and references This book is a must-read for anyone that aspires to be a professional data analyst: Tufte, E. R. (2001). The visual display of quantitative information (Vol. 2). Cheshire, CT: Graphics press. To learn more about ggplot2, the go-to plotting library for R users, see: ggplot2: Elegant Graphics for Data Analysis (Use R!) 1st ed. 2009. Corr. 3rd printing 2010 Edition. The go-to plotting library for Python users is Matplotlib. A great starting point for understanding human perception of graphical representations: Cleveland, W. S., &amp; McGill, R. (1984). Graphical perception: Theory, experimentation, and application to the development of graphical methods. Journal of the American statistical association, 79(387), 531-554. 7.11 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Produce visualizations that are aesthetic and without major techical flaws. Recognize when a type of summary is appropriate and when it is not. Apply data summarization techiques to obtain insights from data. Once introduced to the bootstrap and other estimation techniques, to be able to combine descriptive statistics with a quantification of uncertainty, such as confidence intervals. 7.12 Practice problems Read Chapter 1 of Tufte’s book (see Further reading). Gather 3-5 statistical plots from popular media and 3-5 statistical plots from scientific research papers. Comment on if and how each plot could be improved. Download the Football Manager Players dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the visualization techniques from this chapter. "],
["multivariate-data-chapter.html", "Chapter 8 Summarizing data - multivariate data 8.1 Principal Component Analysis (PCA) 8.2 Factor analysis (FA) 8.3 Multi-dimensional scaling (MDS) 8.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) 8.5 Clustering 8.6 Further reading and references 8.7 Learning outcomes 8.8 Practice problems", " Chapter 8 Summarizing data - multivariate data Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis. In this chapter we will focus on the some advanced techniques for multivariate data. We will be using R and ggplot2, but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools! As we already mentioned, we humans prefer low-dimensional representations of information. This makes sense, because we’ve adapted to living in a 3-dimensional world. The natural way (and the only way) of dealing with high-dimensional data is therefore to map it to fewer dimensions. Often, a lower-dimensional representation can offer useful information and sometimes it can even be done without substantial loss of information. Dimensionality reduction is often also used as a preprocessing step before prediction or inference - fewer dimensions reduce computation times and simplify interpretation of input variables’ importance. Here, we will discuss some of the most common techniques. 8.1 Principal Component Analysis (PCA) PCA is typicaly the first method we use and often the only method, due to its simplicity, interpretability and speed. PCA is based on an orthogonal transformation of possibly correlated variables into new linearly uncorrelated variables which we call principal components. The first principal component accounts for as much of the variability as possible and each next component in turn has the highest possible variance, conditional to being orthogonal to all the previous components. The proportion of variance explained serves as an indicator of the importance of that principal component. If some principal components explain only a small part of the variability, we can discard them and therefore reduce the dimensionality of the representation. Because PCA produces orthogonal (uncorrelated, linearly independent) variables, we can use it as a preprocessing step before linear modelling. If we can interpret the principal components this will simplify the interpretation of the linear model. Note that PCA is sensitive to the relative scales of the variables. That is, scaling a variable would increase its variance and make it a priority for the principal components to include. Before applying PCA we should scale the variables according to their practical scale or, if we have no preference, standardize them so that they have equal relative importance. We demonstrate PCA on a dataset of decathlon results. We hypothesize that decathlon results might be explained by a smaller set of dimensions that correspond to the athlete’s strength, explosiveness, and stamina. We’ll use the decathlon dataset that can be found in the FactoMineR R package. First, we load the data: dat &lt;- read.csv(&quot;./data/decathlon.csv&quot;) dat &lt;- dat[,2:11] print(summary(dat)) ## X100m Long.jump Shot.put High.jump X400m ## Min. :10.44 Min. :6.61 Min. :12.68 Min. :1.850 Min. :46.81 ## 1st Qu.:10.85 1st Qu.:7.03 1st Qu.:13.88 1st Qu.:1.920 1st Qu.:48.93 ## Median :10.98 Median :7.30 Median :14.57 Median :1.950 Median :49.40 ## Mean :11.00 Mean :7.26 Mean :14.48 Mean :1.977 Mean :49.62 ## 3rd Qu.:11.14 3rd Qu.:7.48 3rd Qu.:14.97 3rd Qu.:2.040 3rd Qu.:50.30 ## Max. :11.64 Max. :7.96 Max. :16.36 Max. :2.150 Max. :53.20 ## X110m.hurdle Discus Pole.vault Javeline ## Min. :13.97 Min. :37.92 Min. :4.200 Min. :50.31 ## 1st Qu.:14.21 1st Qu.:41.90 1st Qu.:4.500 1st Qu.:55.27 ## Median :14.48 Median :44.41 Median :4.800 Median :58.36 ## Mean :14.61 Mean :44.33 Mean :4.762 Mean :58.32 ## 3rd Qu.:14.98 3rd Qu.:46.07 3rd Qu.:4.920 3rd Qu.:60.89 ## Max. :15.67 Max. :51.65 Max. :5.400 Max. :70.52 ## X1500m ## Min. :262.1 ## 1st Qu.:271.0 ## Median :278.1 ## Mean :279.0 ## 3rd Qu.:285.1 ## Max. :317.0 Next, we prepare the data by standardizing the columns - we don’t want 1500m running to be more important just because it has a larger scale! We also take the negative value of the running events - we want all the variables to be of the type “larger is better” to simplify interpretation. dat[,c(1,4,5,6,10)] &lt;- -dat[,c(1,4,5,6,10)] dat &lt;- scale(dat) Now we are ready to do PCA: res &lt;- prcomp(dat) prop_explained &lt;- res$sdev^2 / sum(res$sdev^2) data.frame(prop_explained, cumsum(prop_explained)) ## prop_explained cumsum.prop_explained. ## 1 0.32719055 0.3271906 ## 2 0.17371310 0.5009037 ## 3 0.14049167 0.6413953 ## 4 0.10568504 0.7470804 ## 5 0.06847735 0.8155577 ## 6 0.05992687 0.8754846 ## 7 0.04512353 0.9206081 ## 8 0.03968766 0.9602958 ## 9 0.02148149 0.9817773 ## 10 0.01822275 1.0000000 We can see that the first two principal components explain half of the variability in the data. And if we keep half of the principal components, we lose only about 13% of the variability. We could now argue that the latent dimensionality of this data is lower than the original 10 dimensions. Of course, in order to produce a meaningful summary of the data, we must provide an explanation of what these principal components represent: round(res$rotation,2) ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 ## X100m 0.43 -0.14 0.16 -0.04 0.37 -0.30 0.38 -0.46 -0.10 0.42 ## Long.jump 0.41 -0.26 0.15 -0.10 -0.04 -0.31 -0.63 -0.02 0.48 -0.08 ## Shot.put 0.34 0.45 -0.02 -0.19 -0.13 0.31 0.31 -0.31 0.43 -0.39 ## High.jump -0.32 -0.27 0.22 -0.13 0.67 0.47 -0.09 -0.13 0.24 -0.11 ## X400m 0.38 -0.43 -0.11 0.03 -0.11 0.33 -0.12 -0.21 -0.55 -0.41 ## X110m.hurdle 0.41 -0.17 0.08 0.28 0.20 0.10 0.36 0.71 0.15 -0.09 ## Discus 0.31 0.46 0.04 0.25 0.13 0.45 -0.43 0.04 -0.15 0.45 ## Pole.vault 0.03 -0.14 0.58 -0.54 -0.40 0.26 0.10 0.18 -0.08 0.28 ## Javeline 0.15 0.24 -0.33 -0.69 0.37 -0.16 -0.11 0.30 -0.25 -0.09 ## X1500m 0.03 -0.36 -0.66 -0.16 -0.19 0.30 0.08 -0.01 0.31 0.43 As we described in the beginning, each principal component is a linear combination of the original variables. Because we standardized the variables, the corresponding coefficients serve as an indicator of importance. For example, pole vaulting and 1500m running have little importance in the first principal component, while these two disciplines have the highest weight in the fourth principal components. The meaning of principal components can more easily be discerend by plotting pairs of components, their relationship with the original variables, and individual observations. Typically, we first plot the first two principal components - components which explain most of the variance: biplot(res, cex = 0.8) This reveals two major axes in the data - (1) athletes that have high values in PC1 and PC2 are better at javeline, discus, and shotput (events that require strength) and worse at high jump (events where having a lot of muscle mass is detrimental) and (2) atletes that have high PC1 but low PC2 are better at all the shorter running events and long jump (events that depend on running speed). Pole vaulting and 1500m running results are not explained well by the first two principal components. 8.2 Factor analysis (FA) Another commonly used dimensionality reduction and exploratory data analysis technique is factor analysis. In application and interpretation it is similar to PCA, but it solves a different problem. PCA tries to explain the variability in the data with linear combinations of the variables. FA on the other hand assumes the existence of unmeasurable (latent) variables, also known as factors, and that the measured data can be explained as a linear combination of these factors. We demonstrate FA on the same dataset as PCA. In its basic form, FA requires us to specify the number of factors. We’ll assume from previous experience that there are two main factors that explain most of the variability in decathlon results: dat &lt;- read.csv(&quot;./data/decathlon.csv&quot;) dat &lt;- dat[,2:11] dat[,c(1,4,5,6,10)] &lt;- -dat[,c(1,4,5,6,10)] dat &lt;- scale(dat) res &lt;- factanal(dat, factors = 2) library(psych) library(GPArotation) res &lt;- fa(dat, nfactors = 2, rotate = &quot;varimax&quot;) biplot(res) We can see from the plot that the results are visually similar to PCA results only rotated. Rotation is an important concept in FA. Because we are simultaneously fitting latent factors and the coefficients (loadings) of the linear combination of these factors, the solution is invariant to rotation. That is, there are infinitely many (equivalent) solutions to the FA problem. Before we interpret the results, we have to pick one rotation. For easier interpretation, we prefer rotations that keep the factors orthogonal and reduce the number of variables that each factor relates to (loads on). One such rotation is Varimax, which we used in the above example. Before we interpret the results, let’s print the loadings: print(res$loadings, cutoff = 0.4) ## ## Loadings: ## MR1 MR2 ## X100m 0.638 ## Long.jump 0.717 ## Shot.put 0.879 ## High.jump -0.515 ## X400m 0.858 ## X110m.hurdle 0.631 ## Discus 0.715 ## Pole.vault ## Javeline ## X1500m ## ## MR1 MR2 ## SS loadings 2.157 1.967 ## Proportion Var 0.216 0.197 ## Cumulative Var 0.216 0.412 First, observe that not all factors have a substantial influence (loading) on each variable, whereas in PCA each component was moderately influenced by most variables. This is the result of using a rotation that minimizes the complexity of the model in this sense. The interpretation of the two factor is as follows (this can be derived from the above plot as well as the printed loadings). The first latent factor (MR1) correlates positively with all the shorter running events and long jump. We can interpret this factor as the explosiveness of the athlete. The second latent factor (MR1) correlates positively with shot put and discus and negatively with high jump. We can interpret this factor as the strength of the athlete. Note two more things regarding the interpretation of latent factors. First, the meaning of latent factors can only be inferred through their influence on measured variables and always relies on domain specific knowledge. And second, interpreting latend factors is not an exact science - in the above example we revealed some interesting and useful structure in the data, but more than half of the variance still remains unexplained (see the loadings output above) and the two factors also affect other variables, albeit less than the ones shown in the output. 8.3 Multi-dimensional scaling (MDS) MDS is a dimensionality reduction technique that is based on the idea of representing the original data in a lower (typically 2-dimensional) space in a way that best preserves the distances between observations in the original space. The results of MDS are of course sensitive to the distance (metric) that we choose and, similar to PCA, scaling of the variables. There are many variants of MDS: classic MDS uses Euclidean distance and a generalization of that to an arbitrary metric is called metric MDS. There are also non-metric variants of MDS that allow for monotonic transformations of distance. Unlike PCA, MDS does not assume that the high-dimensional structure of the input data can be reduced in a linear fashion and is not sensitive to outliers. That is, MDS can reduce dimensionality in a more robust way than PCA and can be used to detect outliers. Note that MDS, unlike PCA, can be used on data where only the relative distances are known (imagine knowing distances betwee cities but not their geographic locations). We’ll be using non-metric MDS and we’ll first apply it to the decathlon data from the PCA example: library(MASS) d &lt;- dist(dat) # compute Euclidean distances between observations res &lt;- isoMDS(d, trace = F)$points plot(res, xlab = &quot;MDS1&quot;, ylab = &quot;MDS2&quot;, col = &quot;white&quot;) text(res, labels = 1:nrow(dat)) In order to understand MDS1 and MDS2 dimensions, we would have to look at the characteristics of the athletes on the left-hand or right-hand side (higher and lower). Fortunately, we do not have to go through the effort, because we can readily compare this result to the result of PCA and we will quickly determine that they are very similar (up to rotation). To better illustrate where PCA might fail but MDS would give reasonable results, we’ll use a 3D ball. library(scatterplot3d) library(MASS) dataset &lt;- readRDS(&quot;./data/dataset.rds&quot;)$ball dataset &lt;- dataset[sample(1:nrow(dataset),1000, rep = F),] scatterplot3d(dataset[,1:3], color = dataset[,4], pch = 16) All three dimensions are almost identical, so PCA can only produce principal components that are a rotation of the ball. That is, the projection on the first two components does not add any value over just visualizing the first two original dimensions: # PCA res &lt;- prcomp(dataset[,1:3]) rot &lt;- as.matrix(dataset[,1:3]) %*% t(res$rotation) plot(rot[,1:2], col = dataset[,4], pch = 16, xlab = &quot;PCA1&quot;, ylab = &quot;PCA2&quot;) On the other hand, MDS works with distances (topology) and groups points that are on similar layers of the ball closer together, producing a more useful 2D projection of the layers of the ball: # MDS d &lt;- dist(dataset[,1:3]) # compute Euclidean distances between observations res &lt;- isoMDS(d, trace = F)$points plot(res, xlab = &quot;MDS1&quot;, ylab = &quot;MDS2&quot;, col = dataset[,4], pch = 16) 8.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is an advanced state-of-the-art non-linear technique for dimensionality reduction and visualization of high-dimensional data. The key idea of t-SNE is to minimize the divergence between a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. Note that t-SNE is mainly a data exploration and visualization technique - the input features are no longer identifiable in the embedding, so inference can not be done only with t-SNE output. The expressivenes of t-SNE makes it very useful for visualizing complex datasets that require non-linear transformations. On the other hand, as with all complex methods, there is the added computational complexity, additional parameters that need to be tuned, and fewer guarantees regarding convergence to a sensible solution. Due to its stochastic nature, t-SNE output can vary on the same data or, if we set the seed, we can get substantially different results for different seeds. We illustrate t-SNE on data known as the Swiss-roll: library(scatterplot3d) dataset &lt;- readRDS(&quot;./data/dataset.rds&quot;)$roll dataset &lt;- dataset[sample(1:nrow(dataset),1000, rep = F),] scatterplot3d(dataset[,1:3], color = dataset[,4], pch = 16) MDS and PCA preserve the x and y dimensions of the data: # MDS d &lt;- dist(dataset[,1:3]) # compute Euclidean distances between observations res &lt;- isoMDS(d, trace = F)$points plot(res, xlab = &quot;MDS1&quot;, ylab = &quot;MDS2&quot;, col = dataset[,4], pch = 16) # PCA res &lt;- prcomp(dataset[,1:3]) rot &lt;- as.matrix(dataset[,1:3]) %*% t(res$rotation) plot(rot[,1:2], col = dataset[,4], pch = 16) t-SNE on the other hand projects the data manifold into 2 dimensions, which produces an arguably more useful visualization of the characteristics of the data: library(Rtsne) set.seed(321) res &lt;- Rtsne(dataset[,1:3], perplexity = 50) plot(res$Y, col = dataset[,4], xlab = &quot;t-SNE1&quot;, ylab = &quot;t-SNE2&quot;) 8.5 Clustering Another common summarizatin technique is to group the observations into a finite number of groups or clusters. There are numerous clustering techniques, but they all have the same underlying general idea - we want to cluster the points in such a way that similar observations are in the same cluster and dissimilar observations are in different clusters. Instead of hard assignments to clusters, we can also probabilistically assign observations to clusters or assign some other type of assignment weight. This is known as soft clustering and we’ll not be discussing it in this text. 8.5.1 k-means clustering One of the most common and useful clustering methods is k-means clustering. It is based on the assumption that the data were generated by k multivariate normal distributions, each representing one cluster. An observation is assigned to the cluster that it was most likely to have been generated from - and, because we are assuming normal distributions, this reduces to assigning the observation to the mean (centroid) that is closest in terms of Euclidean distance. We’ll illustrate k-means clustering on the swiss dataset from R - data about 47 provinces of Switzerland in about 1888: summary(swiss) ## Fertility Agriculture Examination Education ## Min. :35.00 Min. : 1.20 Min. : 3.00 Min. : 1.00 ## 1st Qu.:64.70 1st Qu.:35.90 1st Qu.:12.00 1st Qu.: 6.00 ## Median :70.40 Median :54.10 Median :16.00 Median : 8.00 ## Mean :70.14 Mean :50.66 Mean :16.49 Mean :10.98 ## 3rd Qu.:78.45 3rd Qu.:67.65 3rd Qu.:22.00 3rd Qu.:12.00 ## Max. :92.50 Max. :89.70 Max. :37.00 Max. :53.00 ## Catholic Infant.Mortality ## Min. : 2.150 Min. :10.80 ## 1st Qu.: 5.195 1st Qu.:18.15 ## Median : 15.140 Median :20.00 ## Mean : 41.144 Mean :19.94 ## 3rd Qu.: 93.125 3rd Qu.:21.70 ## Max. :100.000 Max. :26.60 We’ll suppose that there are 2 clusters in the data (automatic determination of the number of clusters will be discussed later in this chapter): res &lt;- kmeans(swiss, centers = 2) print(res$cluster) ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy ## 2 1 1 2 2 1 ## Broye Glane Gruyere Sarine Veveyse Aigle ## 1 1 1 1 1 2 ## Aubonne Avenches Cossonay Echallens Grandson Lausanne ## 2 2 2 2 2 2 ## La Vallee Lavaux Morges Moudon Nyone Orbe ## 2 2 2 2 2 2 ## Oron Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## 2 2 2 2 2 2 ## Conthey Entremont Herens Martigwy Monthey St Maurice ## 1 1 1 1 1 1 ## Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel ## 1 1 2 2 2 2 ## Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## 2 2 2 2 2 Unless we understand the meaning of the observations (in this case, province names), cluster assignments carry little information. This is similar to latent factors - unless we have some domain specific knowledge, we will not be able to interpret the meaning of the clusters. In fact, clustering is only a special case of latent modeling. Instead of a set of numerical factors we have one factor that explains the data - the cluster assignment. To gain more insight, we can plot the clusters. This is typically done with some dimensionality reduction method that projects the data into 2 dimensions and preserves as much information as possible - the methods we’ve been discussing so far in this chapter. We’ll plot the clusters in the space determined by the first two principal components: library(factoextra) ## Loading required package: ggplot2 ## ## Attaching package: &#39;ggplot2&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## %+%, alpha ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa pca &lt;- prcomp(swiss, scale=TRUE) fviz_pca_biplot(pca, label=&quot;var&quot;, habillage=as.factor(res$cluster)) + labs(color=NULL) + theme(text = element_text(size = 15), panel.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;), legend.position=&quot;none&quot;) Now we can interpret the two clusters as predominately Catholic and predominately Protestant provinces. PCA also shows some other characteristics associated with the clusters - blue is associated with higher fertility and more agriculture, while red is associated with higher education. K-means clustering is simple and fast. It is, however, sensitive to outliers and, if the typical iterative algorithm is used, it might not converge to the same solution every time (solution might be sensitive to choice of initial centroids). K-means clustering also performs poorly in cases where the modelling assumption of k multivariate normal distributions does not hold. 8.5.2 Determining the number of clusters A very simple but often good enough way of determining the most appropriate number of clusters is the so-called elbow method. The idea is to find the number of clusters after which, if we furter increase the number of clusters, does no longer substantially decrease within-cluster variability. Within-cluster variability is decreasing in the number of clusters - more clusters will always lead to clusters being more similar, up to the point of assigning every point to its own cluster, which leads to 0 within-cluster variability. However, if adding a cluster decreases the within-cluster variability by a relatively small amount, that indicates that we might just be splitting already very homogeneous clusters. We demonstrate this technique by plotting the within-cluster variability for different numbers of clusters (also called a scree plot): wss &lt;- 0 maxk &lt;- 10 for (i in 1:maxk) { km &lt;- kmeans(swiss, centers = i) wss[i] &lt;- km$tot.withinss } # Plot total within sum of squares vs. number of clusters plot(1:maxk, wss, type = &quot;b&quot;, xlab = &quot;Number of Clusters&quot;, ylab = &quot;Within groups sum of squares&quot;) We see that after 2 clusters, the within-cluster variability decreases slowly. Therefore, according to this technique, k = 2 (or k = 3) is a good choice for the number of clusters. Sometimes there will be no distinct elbow in the scree plot and we will not be able to use this technique effectively. Another popular technique is to compute the silhouette index. The silhouette index measures how similar an observation is to its own cluster compared to other clusters ). It ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Because it takes into account within and between-cluster variability, the silhouette index, unlike within-cluster variability, does not necessarily increase with the number of clusters - at some point it typically starts decreasing. library(cluster) si &lt;- 0 maxk &lt;- 10 for (i in 2:maxk) { km &lt;- kmeans(swiss, centers = i) si[i] &lt;- mean(silhouette(km$cluster, dist(swiss))[,3]) # mean Silhouette } si ## [1] 0.0000000 0.6284002 0.5368920 0.4406436 0.4495773 0.4056314 0.3703828 ## [8] 0.3806726 0.3447439 0.3502419 In this case the Silhouette technique leads to the same conclusion - k = 2 is the best choice for the number of clusters. Alternatively, we can use clustering methods such as Affinity propagation and Mean shift clustering where the number of clusters is determined automatically. However, such methods introduce other parameters, such as kernel bandwidth. 8.5.3 Agglomerative hierarchical clustering Hierarchical clustering is an umbrella term for a host of clustering methods that build a hierarchy of clusters. Here we’ll talk about agglomerative clustering - hierarchical clustering that starts with each observation in its own cluster and procedes by joining the most similar clusters, until only one cluster remains. The steps of this procedure form a hierarchical cluster structure. Divisive hierarchical clustering approaches instead start with one cluster and procede by splitting the clusters. There are also several different criteria for determining the two most similar clusters A and B: smallest minimum distance between a point from A and a point from B (single linkage), smallest maximum distance (complete linkage), average distance, and many other, each with its own advantages and disadvantages. Here, we’ll apply hierarchical agglomerative clustering with joining clusters according to average distance. Of course, hierarchical clustering works for any distance (metric). We choose Euclidean distance. Here are the results for the swiss dataset: res_hk &lt;- hclust(dist(swiss), method = &quot;average&quot;) plot(res_hk) The above dendrogram visualizes the entire hierarchical clustering structure. We can see how a hierarchical clustering contains not just one, but a clustering for every number of clusters between 1 and the number of points. Comparing with k-means results for 2 clusters, we can see that the two methods return identical clusters: hk2 &lt;- cutree(res_hk, k = 2) res &lt;- kmeans(swiss, centers = 2) km2 &lt;- res$cluster rbind(hk2, km2) ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy Broye Glane ## hk2 1 2 2 1 1 2 2 2 ## km2 1 2 2 1 1 2 2 2 ## Gruyere Sarine Veveyse Aigle Aubonne Avenches Cossonay Echallens Grandson ## hk2 2 2 2 1 1 1 1 1 1 ## km2 2 2 2 1 1 1 1 1 1 ## Lausanne La Vallee Lavaux Morges Moudon Nyone Orbe Oron Payerne ## hk2 1 1 1 1 1 1 1 1 1 ## km2 1 1 1 1 1 1 1 1 1 ## Paysd&#39;enhaut Rolle Vevey Yverdon Conthey Entremont Herens Martigwy Monthey ## hk2 1 1 1 1 2 2 2 2 2 ## km2 1 1 1 1 2 2 2 2 2 ## St Maurice Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel Val de Ruz ## hk2 2 2 2 1 1 1 1 1 ## km2 2 2 2 1 1 1 1 1 ## ValdeTravers V. De Geneve Rive Droite Rive Gauche ## hk2 1 1 1 1 ## km2 1 1 1 1 However, for 3 clusters, the results are, at first glance, completely different: hk2 &lt;- cutree(res_hk, k = 3) res &lt;- kmeans(swiss, centers = 3) km2 &lt;- res$cluster rbind(hk2, km2) ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy Broye Glane ## hk2 1 2 2 1 1 2 2 2 ## km2 1 3 3 2 2 3 3 3 ## Gruyere Sarine Veveyse Aigle Aubonne Avenches Cossonay Echallens Grandson ## hk2 2 2 2 1 1 1 1 1 1 ## km2 3 3 3 2 2 2 2 2 2 ## Lausanne La Vallee Lavaux Morges Moudon Nyone Orbe Oron Payerne ## hk2 1 1 1 1 1 1 1 1 1 ## km2 1 1 2 2 2 2 2 2 2 ## Paysd&#39;enhaut Rolle Vevey Yverdon Conthey Entremont Herens Martigwy Monthey ## hk2 1 1 1 1 2 2 2 2 2 ## km2 2 2 1 2 3 3 3 3 3 ## St Maurice Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel Val de Ruz ## hk2 2 2 2 1 1 1 1 1 ## km2 3 3 3 2 1 1 1 2 ## ValdeTravers V. De Geneve Rive Droite Rive Gauche ## hk2 1 3 3 3 ## km2 1 1 1 1 When comparing different clusterings of the same data, we must keep in mind that a clustering is invariant to relabelling. That is, the cluster label or number has no meaning - a clusters meaning is determined by the observations in that cluster. This is anoter case of the invariance to rotation problem with FA that is common to all latent models. As a consequence, there exist special summaries for cluster similarity. One of the most commonly used is the Rand index. It is defined as the ratio of concordant pairs in all pairs of points. A pair of points is concordant if the points are either in the same cluster in both clusterings or in different clusters in both clusterings. Typicaly, we would use an exisitng implementation, but here we implement the basic Rand Index ourselves: concordant &lt;- 0 for (i in 2:length(hk2)) { for (j in 1:(i-1)) { if ((hk2[i] == hk2[j]) == (km2[i] == km2[j])) concordant &lt;- concordant + 1 } } rand_idx &lt;- concordant / choose(length(hk2), 2) round(rand_idx, 2) ## [1] 0.83 By definition, the Rand index is between 0 and 1. 8.6 Further reading and references For more information on multivariate methods, we recommend Hair, J. F., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2009). Multivariate Data Analysis 7th Edition Pearson Prentice Hall. The following chapters are particularly relevant to what we are trying to learn in this course: Chapter 1: Overview of multivariate methods. Chapter 2. Examining your data. And these two chapters provide more details on the multivariate techniques that we discussed: Chapter 3. Exploratory factor analysis. Chapter 8: Cluster Analysis Chapter 9: Multidimensional Scaling and Correspondence Analysis 8.7 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Recognize when a technique is appropriate and when it is not. Apply data summarization techiques to obtain insights from data. 8.8 Practice problems Download the Football Manager Players dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the multivariate summarization techniques from this chapter. "],
["databases.html", "Chapter 9 Relational databases 9.1 Introduction to SQL 9.2 SQLite 9.3 PostgreSQL 9.4 Further reading and references 9.5 Learning outcomes 9.6 Practice problems", " Chapter 9 Relational databases Organizations, researchers and other individuals have been using databases to store and work with data since they exist. Software that implements a specific database is called a database management system. It provides a complete software solution to work with data and implements security, consistency, simultaniety, administration, etc. There exist different types of database management systems, depending on the type of data they store - for example relational, object, document, XML, noSQL, etc. In this section we focus on the relational databases which provide a common interface using the SQL query language – a common basic query language, which we use for searching and manipulating data in relational databases. Example database management systems are Oracle, IBM DB2, MSSQL, MySQL, PostgreSQL or SQLite. Relational model implemented by the relational databases represents the database as a collection of relations. Relation as defined in theory is most easily represented as a table (note that a concept of a relation is not a table!). Tuple of data is represented as a row. Generally each row describes an entity, e.g. a person. Each tuple contains a number of columns/attributes. Each value is defined within a domain of possible values. Special fields (e.g. primary key) define relational integrity constraints, that is conditions which must be present for a valid relation. Constraints on the Relational database management system is mostly divided into three main categories are (a) domain constraints, (b) key constraints and (c) referential integrity constraints. An example of a relation representation is shown in the Figure below. Figure 6.1: Relation representation. 9.1 Introduction to SQL Structure Query Language (SQL), pronounced as “sequel” is a domain-specific language used in programming and designed for managing data held in a relational database management system. It is based on relational algebra and tuple relational calculus. SQL consists of many types of statements, such as a data manipulation language, a data definition language or a data control language. Although SQL is a declarative language, its extensions include also procedural elements. SQL became a standard in 1986 and has been revised a few times since to include new features. Despite the existence of standards, most SQL code is not completely portable among different database systems. For the data scientist, data modelling and querying are the most important. We will focus only on these two aspects of the SQL which are also the most common to all database systems. In order to use advanced mechanisms such as security management, procedural functionalities or locking, please review the documentation of your selected database system. 9.1.1 Data definition language Data definition language SQL sentences enable us to manipulate with the structure of the database. They define SQL data types and provide mechanisms to manipulate with database objects, data quality assertion mechanisms, transactions and definition of procedural objects such as procedures and functions. The data types of data columns in tables are defined as follows (ANSI SQL): Type SQL data types binary BIT, BIT VARYING time DATE, TIME, TIMESTAMP number DECIMAL, REAL, DOUBLE PRECISION, FLOAT, SMALLINT, INTEGER, INTERVAL text CHARACTER, CHARACTER VARYING, NATIONAL CHARACTER, NATIONAL CHARACTER VARYING Many database management systems can host multiple schemas or databases, so we need first to create the database using CREATE DATABASE DatabaseName. After that we need to explicity say which database will be used by the following commands using USE DatabaseName. If we do not issue that command, we need to addres every database object as DatabaseName.ObjectName. To create new tables, we use the CREATE TABLE statement, which looks as follows: CREATE TABLE TableName ( {colName dataType [NOT NULL] [UNIQUE] [DEFAULT defaultOption] [CHECK searchCondition] [,...]} [PRIMARY KEY (listOfColumns),] {[UNIQUE (listOfColumns),] [...,]} {[FOREIGN KEY (listOfFKColumns) REFERENCES ParentTableName [(listOfCKColumns)], [ON UPDATE referentialAction] [ON DELETE referentialAction ]] [,...]} {[CHECK (searchCondition)]}) Constraints are useful, because they allow the database’s referential integrity protection to take care of data correctness. With the command NULL or NOT NULL we define that a column may or may not contain a missing value. The parameter UNIQUE defines a constraint on a specific column that must not contain duplicated values in the whole table. We can also define it separately for more columns at once, similarly to the primary key syntax. Using this statement we can also define column specifics, such as data type, constraints, foreign keys or primary keys. All of these can be changed later using ALTER commands. For example, to drop an existing constraint or foreign key, we use: ALTER TABLE TableName DROP CONSTRAINT fkName; To add a new integer-type column that must not be undefined and has a specific default value if not set to database, we issue: ALTER TABLE TableName ADD NewColumnName INTEGER NOT NULL DEFAULT 0; When we create a table, an index is created for the columns that represent the primary key. There exist a number of different indexes, which are various and implemented differently across the database management systems. Indexes can improve our query performances but are costly to build and can consume a lot of space, so they need to be created wisely when needed. To create an index, we use the following syntax: CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name ON table_name (column1 [ASC|DESC], column2 [ASC|DESC], ...); To delete a database object, a DROP command is used. Using a DROP command we can delete any database object or a whole database. To delete a table, we use following command: DROP TABLE TableName [RESTRICT | CASCADE] Analogously, the command is used for constraints, indexes, users, databases, etc. In the example above we see two additional arguments - RESTRICT and CASCADE. They specifically define, whether the table can be deleted if there exist some columns in other tables that reference columns in the table we wish to delete. 9.1.2 Data manipulation language When a database and tables or other objects are created, we can focus on inserting, updating, deleting and querying data. 9.1.2.1 Inserting Statements for inserting rows into a database table looks as follows: INSERT INTO TableName [ (columnList) ] VALUES (dataValueList)[, (dataValueList), ...] The list of columns is not necessary if we provide all the columns in the same order that were defined at the table creation time. In the data values we need to provide at least all the values that are of NOT NULL type and do not have the default value set. The order of values in the list need correspond to the column list. To insert more rows at a time, multiple data value lists can be used. When doing a bulk inserts, this will speed up our insertion time. To insert enormous amounts of data into relational database, some database management systems provide their own mechanisms of bulk data importing, as well as for data export. 9.1.2.2 Updating The UPDATE statements allows us to change values within the existing data rows. It is defined as follows: UPDATE TableName SET columnName1 = dataValue1 [, columnName2 = dataValue2...] [WHERE searchCondition] We can update one or more column values in the selected table. The statement will update all the rows that initially satisfy the search condition. The search condition is optional but we must take care when updating or deleting rows as our command may change too many rows if search condition not defined correctly. 9.1.2.3 Deleting Statement for deleting rows is similar to updating without setting new values. The sentence deletes all the rows in a table that match the search condition is defined as follows: DELETE FROM TableName [WHERE searchCondition] 9.1.2.4 Querying Querying is enabled by the SELECT statement which is the most comprehensive as provides multiple mechanisms to access the data from a database: SELECT [DISTINCT | ALL] {* | [columnExpression [AS newName]] [,...] } FROM TableName [alias] [, ...] [WHERE condition] [GROUP BY columnList] [HAVING condition] [ORDER BY columnList [ASC|DESC]] In the SELECT part we can define whether the returned results must be distinct or not. Then we list columns that should be returned as the result. The columns may be named or changed using a special function provided by a database management system (DBMS). Some DBMS also support nested SELECT statements in this part when statement returns only one value. In the FROM part we list tables that are needed for the query to execute. It is possible to nest a SELECT statement instead of using a table - write a query within brackets and give it an alias. Depending on our needs we can use joins in this part to connect the rows between tables. To see which joins are supported by a specific DBMS, we need to check its documentation. In the WHERE part we provide conditions that need to be true for the row to be returned. In this part we can use equality operators (&gt;, &lt;, =, &lt;=, &gt;=, BETWEEN), text comparisons (LIKE), set comparisons (IN, ALL, ANY), null value comparisons (IS NULL, IS NOT NULL) or specific functions supported by our DBMS. The GROUP BY part groups together rows of the same value by the defined columns. On this group of columns we can run further aggregate functions. When using grouping, we can output only columns that are grouped by or aggregate functions of other columns in the SELECT part! Use with caution as some DBMS do not provide warnings if grouping used wrongly. Standard aggregate functions are COUNT, SUM, AVG, MIN and MAX. Each DBMS provides also additional grouping functions such as GROUP_CONCAT (MySQL specific, other databases name proprietary functions differently). The HAVING part can be used only together with GROUP BY. Its role is to provide filtering of rows based on the aggregate functions - these cannot be used in the WHERE part of the query. In the last part we define the order of returned results. By default, the order of rows is undefined. 9.2 SQLite In the previous part we pointed out some basics about data management in relational databases. In this part we will show SQL in practice using an SQLite database. SQLite is an open source, zero-configuration, self-contained, stand-alone, transaction relational database engine designed to be embedded into an application. This type of the database is used by our desktop applications, mobile applications, etc. and it does not need a special server-based database management system. Its structure is contained in a special file, which can be handled by a sqlite database management library. Sqlite seems like a simple database implementation. It lacks performance over large amounts of data, reliability, scalability, redundancy, concurrent access/transactions and other advanced features such as referential integrity, special indexing capabilities and functions, procedural SQL, etc. In the case of data science needs we often need a database only as a simple data storage that is better than storing the data in raw files. Therefore the choice of SQLite of our main database would be enough for the most of our needs. 9.2.1 Environment setup To install an SQLite database we just do need to download a binary version from the official SQLite web site. There exist also installation packages that install SQLite program to our computer and update the $PATH variable so that the sqlite command is directly accessible from command line. The sqlite 3 download package consists of three executables: sqldiff: The utility that displays the differences between SQLite databases. sqlite3: The main executable to create and manage SQLite databases. sqlite3_analyzer: The utility that outputs a human-readable ASCII text report which provides information on the space utilization of the database file. To start working with a database, just run the sqlite3 command in the command-line: $ sqlite3 SQLite version 3.23.1 2018-04-10 17:39:29 Enter &quot;.help&quot; for usage hints. Connected to a transient in-memory database. Use &quot;.open FILENAME&quot; to reopen on a persistent database. sqlite&gt; We exit the console by entering the CTRL+C command twice. The example above created just a temporary database in our memory, so no data was stored on disk. To work with or create a new database if not exists, we need to provide a filename when starting the executable - sqlite3 itds_course.db. When we change the defaults of the database or enter some data, the database file will be created. There exist a lot of standalone graphical user interfaces to work with SQLite database files. An example of a nice and lightweight software to create, open or manage SQLite databases is DB Browser for SQLite and is available for multiple operating systems. Figure 9.1: DB Browser for SQLite graphical user interface. 9.2.2 SQLite database usage In this section we show how to create a new SQLite database and manage it manually using SQL language directly. First we create a new database file called itds_course.db. We will create a new database model, presented in the Figure 9.2. Modeling database schemas can be a complex task, especially for larger databases consisting of many tables. The modeler needs to follow rules, needs to know how the database will be used, the approximate frequency of data insertion, most common queries, etc. Generally, we first prepare a conceptual model, which is easily understandable to laypersons. Then we create a logical model which contains specifics of a database - foreign keys, constraints, indexes, etc. Lastly, a pyhsical model or SQL script is created to instantiate a database on a selected database management system. Model changes in a live database later can be quite more difficult than at the beginning. Figure 9.2: Employee logical database model. Let’s first create the model from the Figure 9.2. The model consists of three tables - Branch, Employee and Salary. Employees must be employed at one of the branches and can get a salary each month. In the model some basic restrictions are defined, such as primary and foreign keys and data types. The data types are related to the SQLite database, which does not support many different types. Therefore time-based attributes are represented as text. SQLite supports foreign keys but it does not enforce their referential integrity if we do not say this explicitly at each database connection session. To enable checks, we need to issue the following command: -- Set the foreign keys checking on PRAGMA foreign_keys = ON; -- Check the foreign keys setting (1=ON, 0=OFF) PRAGMA foreign_keys; We observe that, by default, each SQL statement is terminated using a semicolon. Now we can continue and create all the tables: -- Creating Branch table CREATE TABLE Branch ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, address TEXT NOT NULL, telephone TEXT ); -- Employee CREATE TABLE Employee ( id INTEGER PRIMARY KEY AUTOINCREMENT, id_branch INTEGER NOT NULL, first_name TEXT NOT NULL, last_name TEXT NOT NULL, employment_start TEXT NOT NULL, employment_end TEXT, FOREIGN KEY (id_branch) REFERENCES Branch(id) ); -- Salary CREATE TABLE Salary ( year INTEGER, month INTEGER, id_employee INTEGER, amount REAL NOT NULL, PRIMARY KEY (year, month, id_employee), FOREIGN KEY (id_employee) REFERENCES Employee(id) ); When creating tables, we should be consistent in naming them - capitalization, singular/plural and similarly for the columns, especially foreign keys. The most readable tables have clearly identifiable primary key and foreign keys. Naming of the foreign key columns should reflect the name of the related table. In the Branch and Employee tables we defined the primary key columns as AUTOINCREMENT. This enables the database to automatically set the id of a new record and we do not need to define it explicitly. Now we can insert some data into our tables. The insertion into the Employee table could be done also using one statement only. For the still employed employees we would just set their employment_end value to NULL. -- Branches INSERT INTO Branch (name, address, telephone) VALUES (&quot;Grosuplje&#39;s Fried Chicken&quot;, &quot;Kolodvorska 1, Grosuplje&quot;, &quot;01/786-42-31&quot;); INSERT INTO Branch (name, address) VALUES (&quot;Hot Pork Wings&quot;, &quot;Cesta v Kranj 35, Ljubljana&quot;); -- Employees (still employed) INSERT INTO Employee (id_branch, first_name, last_name, employment_start) VALUES (1, &quot;Marko&quot;, &quot;Novaković&quot;, &quot;1972-01-25&quot;), (1, &quot;Jože&quot;, &quot;Novak&quot;, &quot;1995-08-22&quot;), (1, &quot;Janez&quot;, &quot;Gruden&quot;, &quot;1999-12-07&quot;), (1, &quot;Tine&quot;, &quot;Stegel&quot;, &quot;2002-04-28&quot;), (1, &quot;Nina&quot;, &quot;Gec&quot;, &quot;2010-03-08&quot;), (1, &quot;Petar&quot;, &quot;Stankovski&quot;, &quot;2018-10-01&quot;); -- Employees (retired) INSERT INTO Employee (id_branch, first_name, last_name, employment_start, employment_end) VALUES (1, &quot;Jana&quot;, &quot;Žitnik&quot;, &quot;1987-06-07&quot;, &quot;1987-06-12&quot;), (1, &quot;Patricio&quot;, &quot;Rozman&quot;, &quot;2010-04-22&quot;, &quot;2022-01-01&quot;), (1, &quot;Robert&quot;, &quot;Demšar&quot;, &quot;1996-12-06&quot;, &quot;1997-07-02&quot;); -- Salaries INSERT INTO Salary (year, month, id_employee, amount) VALUES (2010, 05, 2, 950.37), (2010, 06, 2, 955.2), (2010, 07, 2, 990.49), (2010, 08, 2, 1200.5), (2010, 05, 6, 980.43), (2010, 05, 7, 1320.2), (2010, 05, 8, 2100.67), (2010, 06, 8, 2140.99), (2010, 07, 8, 2099.11); Now the data are in the database and we can query them. Below we show just some basic queries. The command-line output does not include column names, so when they are not defined in the query, their order is as in the table. -- Selecting all columns SELECT * FROM Branch; 1|Grosuplje&#39;s Fried Chicken|Kolodvorska 1, Grosuplje|01/786-42-31 2|Hot Pork Wings|Cesta v Kranj 35, Ljubljana| -- Selecting a column with a condition SELECT employment_start FROM Employee WHERE last_name = &quot;Gec&quot;; 2010-03-08 -- Result value concatenation SELECT id, first_name || &#39; &#39; || last_name AS name FROM Employee WHERE employment_end IS NULL; 1|Marko Novaković 2|Joe Novak 3|Janez Gruden 4|Tine Stegel 5|Nina Gec 6|Petar Stankovski SQLite also supports advanced querying, for example: Show the number of employees and a maximum salary for each branch. -- Grouping and aggregations SELECT b.id, b.name, COUNT(e.id), MAX(s.amount) FROM Branch b, Employee e, Salary s WHERE e.id_branch = b.id AND s.id_employee = e.id GROUP BY b.id, b.name; 1|Grosuplje&#39;s Fried Chicken|9|2140.99 The query above does not return branches with no employees, which is not expected. To improve the query, we need to introduce outer joins. They enable outputs even if there are no matching records in a related table. You may think also of other solutions to solve the problem. -- Grouping and outer joins SELECT b.id, b.name, COUNT(e.id), MAX(s.amount) FROM Branch b LEFT OUTER JOIN Employee e ON e.id_branch = b.id LEFT OUTER JOIN Salary s ON s.id_employee = e.id GROUP BY b.id, b.name; 1|Grosuplje&#39;s Fried Chicken|14|2140.99 2|Hot Pork Wings|0| Select all the employees that got their salary even though they were not employed any more by a branch. Instead of using data from a table directly, we pre-prepare the data using a nested query which is used then in the same way as a new table in a query. -- Nested select statement SELECT * FROM (SELECT e.id, e.first_name, e.last_name, s.amount, e.employment_end, s.year, s.month FROM Employee e, Salary s WHERE e.id = s.id_employee AND e.employment_end IS NOT NULL) t WHERE strftime(&#39;%Y&#39;, t.employment_end) &lt; t.year OR (strftime(&#39;%Y&#39;, t.employment_end) = t.year AND strftime(&#39;%m&#39;, t.employment_end) &lt; t.month); 7|Jana|Žitnik|1320.2|1987-06-12|2010|5 Sometimes we would like our results of a query to be stored into a new table. We can achieve this using a CREATE AS statement. The example above prepares a table with all the employees and their salaries that we can further analyse using data science methods: CREATE TABLE SalaryAnalysis AS SELECT e.id, e.employment_start, e.employment_end, s.year || &#39;-&#39; || s.month || &#39;-01&#39; AS month, s.amount FROM Employee e INNER JOIN Salary s ON e.id = s.id_employee; SELECT * FROM SalaryAnalysis; 2|1995-08-22||2010-5-01|950.37 2|1995-08-22||2010-6-01|955.2 2|1995-08-22||2010-7-01|990.49 2|1995-08-22||2010-8-01|1200.5 6|2018-10-01||2010-5-01|980.43 7|1987-06-07|1987-06-12|2010-5-01|1320.2 8|2010-04-22|2022-01-01|2010-5-01|2100.67 8|2010-04-22|2022-01-01|2010-6-01|2140.99 8|2010-04-22|2022-01-01|2010-7-01|2099.11 Another option is to create a dynamic object called VIEW. The CREATE VIEW command assigns a name to a pre-packaged SELECT statement. Once the view is created, it can be used in the FROM clause of another SELECT in place of a table name. We can also rename columns when creating a view. We now use the same example as above to create a VIEW: CREATE VIEW SalaryAnalysisView (id, start, end, month, amount) AS SELECT e.id, e.employment_start, e.employment_end, s.year || &#39;-&#39; || s.month || &#39;-01&#39; AS month, s.amount FROM Employee e INNER JOIN Salary s ON e.id = s.id_employee; SELECT * FROM SalaryAnalysisView; 2|1995-08-22||2010-5-01|950.37 2|1995-08-22||2010-6-01|955.2 2|1995-08-22||2010-7-01|990.49 2|1995-08-22||2010-8-01|1200.5 6|2018-10-01||2010-5-01|980.43 7|1987-06-07|1987-06-12|2010-5-01|1320.2 8|2010-04-22|2022-01-01|2010-5-01|2100.67 8|2010-04-22|2022-01-01|2010-6-01|2140.99 8|2010-04-22|2022-01-01|2010-7-01|2099.11 There is also option to update our data using the UPDATE statement: -- Set new phone number to the Hot Pork Wings branch UPDATE Branch SET telephone = &quot;05/869-22-54&quot; WHERE name = &quot;Hot Pork Wings&quot;; -- Retire the Employee 3 starting today UPDATE Employee SET employment_end = date(&#39;now&#39;) WHERE id = 3; DELETE statements are similar to UPDATE statements. In both cases we must be careful about the WHERE condition as we may quickly delete or update too many rows. DELETE FROM Employee WHERE employment_end IS NOT NULL; The statement above could not delete Employees as there exist rows in the Salary table that refer to the rows in the Employee table. Therefore, we need to first delete the rows from the Salary table and then from the Employee table. More advanced databases support the CASCADE mechanism in the DELETE statement already which can be used for such scenarios. DELETE FROM Salary WHERE id_employee IN ( SELECT id FROM Employee WHERE employment_end IS NOT NULL ); DELETE FROM Employee WHERE employment_end IS NOT NULL; We presented just some basic examples of how to use an SQLite database. Feel free to investigate its possibilities further. 9.2.3 SQLite in Python SQLite can be used by many different programming languages. We show an example of how to use an SQLite database from Python. The database we prepared in the previous section is also available here. First we need to install library for SQLite database file manipulation. If you use Anaconda Python distribution, just issue the conda install -c anaconda sqlite command. Then we need to connect to a database. If the database file does not exist in the path provided, an empty database file will be created automatically. import sqlite3 conn = sqlite3.connect(&#39;itds_course.db&#39;) 9.2.3.1 Inserting data We assume that we are connected to the database from the previous section. Let’s first insert a record. c = conn.cursor() c.execute(&#39;&#39;&#39; INSERT INTO Employee (id_branch, first_name, last_name, employment_start) VALUES (2, &quot;Mija&quot;, &quot;Zblaznela&quot;, &quot;1999-07-12&quot;); &#39;&#39;&#39;) c.execute(&#39;&#39;&#39; INSERT INTO Salary (year, month, id_employee, amount) VALUES (2010, 05, last_insert_rowid(), 1243.67); &#39;&#39;&#39;) # Save (commit) the changes conn.commit() # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. # conn.close() After every change to a database, we must call the commit command which actually saves the results into the database. Now let’s examine if our insert statement was successful. You may have noticed the call to the function last_insert_rowid(). This function returns the id of the last inserted row when we are using AUTOINCREMENT option for the primary key. 9.2.3.2 Selecting data Let’s check, what is in the database. cursor = c.execute(&#39;&#39;&#39; SELECT b.id, b.name, COUNT(e.id), MAX(s.amount) FROM Branch b LEFT OUTER JOIN Employee e ON e.id_branch = b.id LEFT OUTER JOIN Salary s ON s.id_employee = e.id GROUP BY b.id, b.name; &#39;&#39;&#39;) for row in cursor: print(&quot;\\tBranch id: %d\\n\\t\\tBranch name: &#39;%s&#39;\\n\\t\\tEmployee number: \\ %s\\n\\t\\tMaximum salary: %s&quot; % (row[0], row[1], row[2], row[3])) # You should close the connection when stopped using the database. conn.close() We can observe that now also the Hot Pork Wings branch employs an employee. Branch id: 1 Branch name: &#39;Grosuplje&#39;s Fried Chicken&#39; Employee number: 14 Maximum salary: 2140.99 Branch id: 2 Branch name: &#39;Hot Pork Wings&#39; Employee number: 1 Maximum salary: 1243.67 9.3 PostgreSQL In some cases we need more powerful relational database features. In this case we propose to use a more advanced relational database management system, such as PostgreSQL, MS SQL, MySQL or others. In previous chapters we introduced Docker environment. Now we will use an official PostgreSQL database image and run it: docker run --name pg-server -e POSTGRES_PASSWORD=BeReSeMi.DataScience \\ -d -p 5432:5432 postgres:10 The command will run a PostgreSQL v10.0 database in a Docker container. Also, it will name the container pg-server, set the default database user’s postgres’ password to BeReSeMi.DataScience and share the port 5432 to the host machine. When the container is running, we can use our preffered database client and connect to the database server. We can also directly connect to the database using the following command: docker exec -it pg-server psql -U postgres -W First we need to create a database, called courseitds and connect to it: postgres=# create database courseitds; CREATE DATABASE postgres=# \\c courseitds You are now connected to database &quot;courseitds&quot; as user &quot;postgres&quot;. Now we can use the SQL statements from the example above (Section 9.2.2). PostgreSQL supports SQL a bit differently, so we need to adapt the following: Foreign key constraint checks are enabled by default so there is no need to enable them. The command AUTOINCREMENT is not supported. We can use a data type SERIAL which already includes the functionality. Foreign keys can still be of INTEGER type. Dates can used the data type DATE. Also functions for querying are different. For example, EXTRACT (YEAR FROM t.employment_end) extracts a year value from a date. We should not use double quotes for values. Therefore we change all double quotes to single quotes. Single quotes that are part of a value should be escaped with an additional single quote. We provided adapted SQL queries in a single SQL file. When running a database using Docker, do not forget about the data storage! Using the container storage directly is not safe, so we should use volumes instead. 9.4 Further reading and references SQLite official website and Sqlite 3 Python library documentation. There are lots of SQLite tutorial, such as SQLite tutorial.net. PostgreSQL official documentation. 9.5 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use SQL to model, query and update a relational database. Setup a relational database and use it from your programming language of choice. Import and export data to and from a database. 9.6 Practice problems Select one of the machine learning datasets, design a database for it and import it into a database. Use the Python visualization library and visualize the data queried directly from the database. Adapt the examples from above for use with another relational database management system - e.g. MySQL, MSSQL or Oracle. "],
["predictive.html", "Chapter 10 Predictive modelling 10.1 Introduction 10.2 Commonly used prediction models and paradigms 10.3 Model selection 10.4 Practical considerations 10.5 Putting it all together with R 10.6 Putting it all together with Python 10.7 Further reading and references 10.8 Learning outcomes 10.9 Practice problems", " Chapter 10 Predictive modelling 10.1 Introduction For the purposes of this text, we will define predictive modelling as a family of practical problems where the focus is on utilizing current and historical data to predict the outcome of future or unknown events. This is a very broad family - in fact, it encompasses the majority of practical data analysis problems, for example, classifying images, comparing groups, forecasting stock prices, automated translation, making recommendations, customer profiling, personalized advertising, etc. Note that predictive modelling is also referred to as supervised learning (learning a functional relationship based on example input-target variable pairs) in contrast with unsupervised learning (no variable is the target variable - our goal is to learn structure of the data - see Chapter 8 and semi-supervised learning (some observations are labelled and some are not and we want to utilize both). Note that predictive modeling is not restricted to machine learning models and techniques. Predictive modelling, while traditionally a more machine learning task, utilizes methods from statistics, machine learning, and computer science. Key terms Before we discuss predictive modelling in more detail, let us define some common terms. The main goal of predictive modelling is to define a relationship between known data and the unknown outcome that we are trying to predict. Looking at it from a more abstract perspective, we are trying to find a useful functional relationship between what we know and what we don’t know. We will call this functional relationship a model. While it might in some cases be possible to determine a model by hand, we will in most cases be using a training algorithm to produce a model from our data. This process is called training a model, learning, fitting a model or inference, depending on whether we are in a machine learning, statistical or some other community. Every training algoritm has some degree of freedom or flexibility to adapt to the given data - without that, learning would not be possible. We will refer to this flexibility as model parameters and we will make a distinction between parameters that will be fit automatically and parameters that the user has to set (tunable parameters). This distinction is purely practical - every parameter can be a tunable parameter or just a parameter, depending on how we set up our training process. For example, coefficients in linear regression (or weights in a neural network or splits in a decision tree) are typically trained on data and not set by the user, on the other hand, the regularization parameter in linear regression (or the number of layers in a neural network or the splitting criterion in a decision tree) is often set by the user. We’ll call the data that we use to train our model training data to distinguish them from data that are used to test the model’s performance - the test data or holdout data. When we introduce an intermediate step of setting tunable parameters automatically, we often refer to the data that are used to test the model’s performance for a particular setting of the tunable parameters as the validation data. Within the data we also distinguish between the known data (or data that will be known when making a prediction) and data that will be unknown (but are available in some historical cases, which we use to train the model). The latter are called target variables (or labels, classes, input variables, dependent variables) while the former are called input variables (or features, attributes, predictors, independent variables). We will refer to the basic unit of prediction (one set of inputs and the target) as an observation (or instance, example). An illustrative example Let’s reinforce the terms from the previous section with an illustrative example. We’ll use a simple data set with 200 observations and 2 variables, \\(X\\) and \\(Y\\). The task is to predict the value of \\(Y\\) from the value of \\(X\\). That is, we have a single input variable (\\(X\\)) and a single target variable (\\(Y\\)). Before we proceed, we’ll randomly split the dataset into two subsets of equal size. We’ll use one set to train our model and the other to test its performance. # Simple linear regression example --------------------------------------------- library(ggplot2) # read dataset dataset &lt;- read.table(&quot;./data/simpleReg.csv&quot;, sep = &quot;,&quot;, h = T) # train-test split set.seed(0) idx &lt;- sample(1:200, 100, rep = F) dataset$IsTest &lt;- T dataset$IsTest[idx] &lt;- F # plot the data g1 &lt;- ggplot(dataset, aes(x = X, y = Y, colour = IsTest)) + geom_point() plot(g1) Now we have to choose a training algorithm that we’ll use to produce a model for the process that generated this dataset. We’ll use simple least-squares linear regression. This tranining algorithm assumes that the functional relationship between \\(X\\) and \\(Y\\) is \\(Y_i = kX_i+n + \\epsilon_i\\). Or, in simpler terms, we assume that the points lie on a straight line and we account for the deviations with the residual terms \\(\\epsilon_i\\). Clearly, the data suggest a non-linear relationship and that linear regression is not the best choice, but we’ll ignore that for now. The next step is to run the linear regression algorithm to find the best values of our two model parameters slope (\\(k\\)) and intercept (\\(n\\)). In the case of least-squares linear regression, best is defined as the one that minimizes the sum of squared residuals \\(\\epsilon_i\\). That is, within the black-box of least-squares regression we’d find just a simple optimization algorithm: # train the model train_data &lt;- dataset[dataset$IsTest == F,] test_data &lt;- dataset[dataset$IsTest == T,] lr &lt;- lm(Y ~ X, data = train_data) n &lt;- lr$coefficients[1] k &lt;- lr$coefficients[2] cat(sprintf(&quot;n = %.2f, k = %.2f\\n&quot;, n, k)) ## n = -6.31, k = 6.38 g1 &lt;- ggplot(dataset, aes(x = X, y = Y, colour = IsTest)) + geom_point() + geom_abline(slope = k, intercept = n) plot(g1) We can see that the best linear model is probably not the best model for this data, but it still captures part of the relationship between \\(X\\) and \\(Y\\) and could be used to predict new values better than the naive approach of just predicting with the mean. Note that in practice the term model is used interchangeably for the training algorithm and the model it produces (in our case \\(Y_i = 6.65X_i - 8.60\\)). That is, the term linear regression model is often used to refer to the entire algorithm of assuming a liner relationship and finding the parameters using optimization. Now we’ll try to produce a better model. We’ll use the least-squares algorithm, but we’ll add the squared and cubic terms \\(X^2\\) and \\(X^3\\). This is also called polynomial regression: lr &lt;- lm(Y ~ poly(X, 3), data = train_data) x &lt;- seq(-10, 10, 0.1) y &lt;- predict(lr, newdata = data.frame(X = x)) g1 &lt;- ggplot(dataset) + geom_point(aes(x = X, y = Y, colour = IsTest)) + geom_abline(slope = k, intercept = n) + geom_line(data = data.frame(x = x, y = y), mapping = aes(x = x, y = y), colour = &quot;blue&quot;) plot(g1) The cubic polynomial appears to be a much better fit for the training data and the test data. Let’s check if that is indeed the case: lr &lt;- lm(Y ~ X, data = train_data) y &lt;- predict(lr, newdata = test_data) e1_te &lt;- mean((y - test_data$Y)^2) y &lt;- predict(lr, newdata = train_data) e1_tr &lt;- mean((y - train_data$Y)^2) lr &lt;- lm(Y ~ poly(X, 3), data = train_data) y &lt;- predict(lr, newdata = test_data) e2_te &lt;- mean((y - test_data$Y)^2) y &lt;- predict(lr, newdata = train_data) e2_tr &lt;- mean((y - train_data$Y)^2) cat(sprintf(&quot;test and training data mean squared error for line: %.2f / %.2f and poly: %.2f / %.2f\\n&quot;, e1_te, e1_tr, e2_te, e2_tr)) ## test and training data mean squared error for line: 462.06 / 477.82 and poly: 184.59 / 229.72 The numbers confirm that the polynomial regression model has lower training and test error. The latter implies that the polynomial regression model will better generalize to new or unseen examples and should be, in terms of predictive performance, used instead of the linear regression model. To illustrate another important concept, we’ll also use a 25th degree polynomial model. lr &lt;- lm(Y ~ poly(X, 3), data = train_data) x &lt;- seq(-10, 10, 0.1) y1 &lt;- predict(lr, newdata = data.frame(X = x)) lr &lt;- lm(Y ~ poly(X, 25, raw = T), data = train_data) y2 &lt;- predict(lr, newdata = data.frame(X = x)) g1 &lt;- ggplot(dataset) + geom_point(aes(x = X, y = Y, colour = IsTest)) + ylim(-100, 100) + geom_abline(slope = k, intercept = n) + geom_line(data = data.frame(x = x, y = y1), mapping = aes(x = x, y = y1), colour = &quot;blue&quot;) + geom_line(data = data.frame(x = x, y = y2), mapping = aes(x = x, y = y2), colour = &quot;red&quot;) plot(g1) ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 9 row(s) containing missing values (geom_path). ## Warning: Removed 4 row(s) containing missing values (geom_path). lr &lt;- lm(Y ~ poly(X, 25, raw = T), data = train_data) y &lt;- predict(lr, newdata = test_data) e1_te &lt;- mean((y - test_data$Y)^2) y &lt;- predict(lr, newdata = train_data) e1_tr &lt;- mean((y - train_data$Y)^2) cat(sprintf(&quot;poly (25): %.2f / %.2f\\n&quot;, e1_te, e1_tr)) ## poly (25): 71336.92 / 162.80 We can see that the 20th degree polynomial model, which has 17 more parameters than the 3rd degree polynomial model, is able to fit the training data better. However, its performance on the test data is worse! This is an important concept that we call overfitting - the model was too expressive for the amount of data we have and it started fitting not just the underlying true functional relationship but also the noise in the data. An extreme example of this would be using a 100th degree polynomial, which would be able to perfectly fit the training data, but it would perform very poorly on test data. Finally, let’s use a \\(k\\)-nearest neighbors model with \\(k = 10\\). In other words, we assume that the value of an observation is the average of the 10 nearest points: # we&#39;ll manually code k-nearest neighbors for this simple example tmp &lt;- dataset tmp$PredictedY &lt;- NA knearest &lt;- function(x, df, k) { mean(df[rank(abs(df$X - x)) &lt;= k,]$Y) } for (i in 1:nrow(tmp)) { tmp[i,]$PredictedY &lt;- knearest(tmp$X[i], train_data, 10) } y &lt;- tmp[tmp$IsTest == T,]$PredictedY e1_te &lt;- mean((y - test_data$Y)^2) y &lt;- tmp[tmp$IsTest == F,]$PredictedY e1_tr &lt;- mean((y - train_data$Y)^2) cat(sprintf(&quot;knn (10): %.2f / %.2f\\n&quot;, e1_te, e1_tr)) ## knn (10): 214.64 / 257.63 y &lt;- c() x &lt;- seq(-10, 10, 0.1) for (pt in x) { y &lt;- c(y, knearest(pt, train_data, 10)) } g1 &lt;- ggplot(dataset) + geom_point(aes(x = X, y = Y, colour = IsTest)) + ylim(-100, 100) + geom_line(data = data.frame(x = x, y = y), mapping = aes(x = x, y = y1)) plot(g1) ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 9 row(s) containing missing values (geom_path). The performance of the 10-nearest neighbor model is relatively good, only slightly worse on the test set than the cubic polynomial model. 10.1.1 The process of predictive modelling In general, predictive modelling is a complex task that requires mathematical, statistical, computational and programming knowledge and skills. However, in most practical scenarios we can get reasonably good results by following a simpler systematic process and utilizing existing tools and libraries. The process can be divided into three main steps: Analyze the given scenario and identify into which case(s) of predictive modelling it falls under. Based on desired predictive quality and other practical considerations, select candidate training algorithms. Estimate the error of the models produced by the training algorithms and select the best one. This process allows us to handle most practical predictive modelling scenarios and will almost always be used as a starting point. Steps (1) and (2) rely on our knowledge of and practical experience with different approaches to modelling - we will introduce a few in the next section. And later in the chapter we will see that we can handle (3) in a systematic and relatively general way, treating the training algorithms and models as black-boxes with tunable parameters. Before we proceed, let us briefly discuss some of the most common reasons why the process could fail. That is, why we would not be able to use existing algorithms and just choose the best one: None of the existing algorithms (or implementations) apply to our case: Some cases are very common in predictive modelling. So common, that they have special names and entire fields dedicated to them. For example, when the target variable is categorical (classification), numerical (regression) or ordinal (ordinal regression). If our problem fits into one of these standard paradigms, we’ll have many different training algorithms to choose from. However, if it doesn’t, for example, if we have non-standard input or target variables, such as a matrix or a graph, we will either have to translate our problem to a standard case or produce some methodological innovation. The latter typically requires in-depth machine learning, statistical, and/or computational knowledge and skills and is in the domain of data science experts and researchers. Models trained with existing algorithms make poor predictions, are computationally infeasible, not transparent enough, etc.: In the best-case scenario this again requires methodological innovation. In the worst-case scenario it might be infeasible or impossible to solve the problem with the data and resources at our disposal. To summarize - this engineering approach to predictive modelling that we’ll focus on is very systematic and can be applied in most cases. However, there are cases where it can’t be applied or where it doesn’t produce good-enough results. Such cases are a few orders of magnitude more difficult. 10.2 Commonly used prediction models and paradigms If we have some experience with statistics or machine learning, we already know that there exist many different algorithms for predictive modelling - linear regression, decision trees, neural networks, etc. This already hints at one of the fundamental principles of modelling - there is no single best algorithm or model! A well-known theorem, known as the no-free-lunch theorem for machine learning, states that no single algorithm that is better than all other algorithms over all possible problems. The theorem in a way assumes that all functional relationships are equally probable, so it does not apply to applied predictive modelling. In practice, in the real world, some functional relationships are more common than others. It does, however, imply, that in any scenario or for any dataset, a more specialized algorithm (one that exploits the structure of the problem) will outperform a more general algorithm. On a related note, an important thing to remember is that by selecting a particular training algorithm, we limit ourselves to a subset of possible models that the training algorithm can produce. For example, as we saw before, by choosing least-squares regression, we limit ourselves to a line or, more generally, a hyperplane. This is also known as the inductive bias of a training algorithm. Some algorithms have a very good inductive bias - the functional relationships they prefer are those that we more commonly encounter in practice, overall or in a particular application domain. These are the algorithms that are typically implemented in statistical or machine learning libraries. So, it is inevitable that we have many general-purpose predictive modelling training algorithms and a plethora of specialized methods, each with its own advantages and disadvantages. Taking into account that there are also other practical considerations, such as model simplicity, time complexity, etc. (covered in more detail later in the chapter), we can conclude that a practitioner must familiarize himself with many different training algorithms and models. Here we briefly discuss a few of the most common ones: Generalized linear models Linear regression is a special case of generalized linear models, which generalize the linear combination approach to other types of target variables, such as logistic regression for classification and Poisson regression for count data. Linear models are computationally simple, relatively easy to interpret and are less likely to overfit, due to their limited expressiveness. Linear models can also be used to model non-linear relationships by transforming the input space. This is the idea behind polynomial regression, spline models, etc. Decision trees Decision trees are a broad family of models, which are based on partitioning the input variable space into different regions. The prediction of a new observation is based on the values of other observations in its region. The model is typically derived via recursive partitioning of the input space and can be represented with a tree structure. This makes decision trees very useful when model interpretability is important. In fact, (small) decision trees are the most easy-to-understand representation for non-experts. Neural networks The prototypical artificial neural nework is the feed-forward neural network. In particular, the multi-layer perceptron. It is based on the input layer followed by zero or more hidden layers of interconnected neurons and the output layer. Every neuron is typically connected with one or more neurons from the next layer and the connections are weighted. Typically, every neuron also has a threshold or activation function that determines under what inputs the neuron will activate its output. In essence, a feed-forward neural network is a very flexible functional representation of the relationship between the inputs and outputs. In fact, they are capable of approximating almost all functional relationships we’ll encounter in practice, given enough data. Artificial neural networks are more complicated to apply and have more parameters to tune than most other models, including the topology (number of layers, how they are interconnected, etc.). Due to their complexity, they are also more prone to overfitting. Artificial neural networks have recently again grown in popularity with modern deep learning approaches such as convolutional neural networks. These approaches perform particularly well on tasks where we have large amounts of observations but very little understanding of what the functional relationship might be, such as image recognition, video analysis, speech recognition, natural language processing, etc. Kernel-based learning An alternative to explicitly specifying the functional relationship is to say that the target variable of new or unseen observations is likely to be similar to the target variables of nearby observations. In essence, similar observations should have similar target variables. The nearest-neighbors approach from the introductory example is a representative of this approach. Similarity-based learning is an elegant and powerful concept and the basis for many different training algorithms. Of course, in order to transfer this concept into practice, we must be able to define what we mean by similar (or a kernel). Many different kernels exist and while some are more popular than others, choosing the appropriate kernel and its parameters is problem-specific and should be treated as any other model tuning parameter. Kernel-based methods also include Support Vector Machines and Gaussian Processes. More traditional methods such as PCA and even linear regression can also be interpreted as kernel-based methods. Note that with this type of learning the functional relationship can become arbitrarily complex/refined as we get more observations. We call such models non-parameteric models. As opposed to parametric models such as linear models or neural networks, where the functional relationship remains fixed, only the parameters/weights change. Decision trees are also an example of a non-parametric model. Non-parametric models tend to require more data and are more likely to overfit, but they are also more convenient, because we don’t have to specify a functional form for the relationship between the inputs and outputs. Ensemble learning Instead of using a single model, we can often get a better result by combining several models or, as it is commonly known in machine learning circles, by ensemble learning. Different models can each learn better different parts of the relationship. Methods like random forests are based on deliberately training biased models on subsets of the observations and input data to reduce overall variability and improve prediction quality. Common predictive modelling paradigms Some distinct predictive modelling patterns (paradigms) are so common that we give them names. We already mentioned the two most common ones, standard classification (target variable is categorical) and standard regression (target variable is numerical), where by standard we refer to having several independent and identically distributed observations, each with one or more numerical input variables. If we can recognize that our predictive modelling case fits into a well-known paradigm, our job will typically be easier, because we can utilize existing methods and implementations. Note that common predictive models are designed for classification and/or regression (in fact all of the models that we listed in this section). This is also due to the fact that most paradigms can be reduced to classification or regression, albeit possibly with a loss of relevant problem structure. Other common paradigms include: multivariate analysis and prediction: When we have \\(k &gt; 1\\) target variables. This can be reduced to \\(k\\) independent classification/regression problems, but we lose information that is potentially contained in the dependency structure. multi-label classification: Predicting not just one category (classification) but a subset of categories (labels) associated with an observation. That is, predicting which labels the observation has. Again, this can be reduced to an independent binary (yes/no) classification problem for each label (we lose dependency structure) or a single classification problem over the cartesian product of the labels (computationally infeasible due to exponential growth in the number of labels). multi-instance learning: Instead of individual observations, observations are grouped in labeled bags, each with one or more observations. The label of the bag depends on the (unobserved) labels of its instances, for example, positive if at least one is positive and negative otherwise. The objective is to learn and predict labels of individual observations and/or bags. online learning: Sometimes we do not want to or even can’t learn from all the past observations together (batch training). For example, when storage is limited, when computational resources are limited or when we are dealing with a varying functional relationship. In such cases, we might prefer models that are able to update their functional relationship with a single observation. There exist online variants of many standard classification and regression models. time-series forecasting: When there is also a temporal component to the observations, we typically refer to the data as time-series data. Many specialized algorithms exist for such data, but time-series forecasting can also be viewed as a special case of classification/regression, where we can only use data that are available prior to the time point where the prediction has to be made. A special or related case is data stream mining. Data stream mining focuses on extracting patterns or making predictions from continuous and often rapid streams of data. Additional restrictions that typically apply are that data points can be read only once and that computing and storage capabilities are limited. Therefore, online learning approaches are often utilized in data stream mining. 10.3 Model selection In this section we will focus on the most important subtask, selecting the model that has the best predictions. Note that in practice, the best model for us might not be the one that gives the best predictions - we discuss this in the next section - but for now we focus on models’ predictive quality. First, we let us unwrap the statement selecting the model that has the best predictions. Best predictions where? And what do we mean by best? Making this statement precise is key to desiging a proper model selection process for the problem at hand. We can provide a very general answer to the where question - we want the model that will perform the best not on the data we have, but on future on unseen observations. The model that best generalizes the underlying functional relationship, not the observed relationships in the data. The what do we mean by best question, however, has to be answered on a case-by-case basis, because it depends highly on what we are trying to achieve. So, every model selection process has two main components: (1) choosing the appropriate measure(s) of quality and (2) estimating the expected value of those measures on future or unseen observations. Measuring predictive performance Our choice of measure of predictive preformance (also known as prediction error, loss, score) should depend on what we are trying to achieve. There are, however, some common scenarios and commonly used measures. For regression and when we are evaluating point predictions of the mean, mean squared error (MSE) is the natural choice. However, if we are interested in predicting the most likely values, MSE may be a very poor choice. To understand why we can revisit the Gamma distribution example from Chapter 8: if the error or noise in our target variable is very skewed, then there will be a large difference between the mean and the most likely value. Models that are trained by minimizing MSE will estimate the mean and therefore not the most likely value. In such cases it is better to use a measure that is less sensitive to extreme values, such as mean or median absolute error. A more general measure for regression is the logarithmic scoring rule or log-score. The log-score is proportional to the logarithm of the model’s density prediction for the true value of the target variable, so it is proportional to the log-likelihood of the model. Assuming that our error or noise is normally distributed, the log-score and MSE are equivalent. Unfortunately, the log-score for regression can only be used if the models we are evaluating give density predictions and not just point predictions. The MSE can be viewed as a reasonable approximation to the log-score when density predictions are not available. For classification and when the models output only the predicted target value (that is, crisp 1/0 and not probabilistic predictions), we are limited to computing the classification accuracy - the ratio of correctly predicted values. If not all mistakes are equal, however, we might prefer the false positive and false negative rate or, in general, a confusion matrix or measures derived from it. Unless we are restricted to crisp preditcions for some other reasons, we should rarely prefer accuracy over a measure that evaluates the probabilistic predictions output by the model. In particular, we should use the log-score or, alternatively, the Brier score or [quadratic scoring rule(https://en.wikipedia.org/wiki/Scoring_rule#Brier/quadratic_scoring_rule). Classification accuracy is flawed - computing it for classifiers that output probabilistic forecasts requires us to set a threshold that determines which probabilities consitute as 1s and which as 0s (0.5 being the natural choice in binary classification). This threshold may introduce a bias but more importantly, we can see how in order to get the optimal accuracy, we only need to be above the threshold when and only when the hypothetical optimal probabilistic classifier is above the threshold. That is, we can hypothetially take the optimal classifier and change its probabilities to any value and, as long as we don’t cross the threshold, we will retain the same classification accuracy. This implies that classification accuracy does not force classifiers to output optimal probabilistic forecasts in order to maximize their accuracy, which is not a desirable property. Other, more specialized predictive modeling paradigms may prefer other measures of predictive performance. When encountering a new measure or new scenario, we should always review what we are trying to achieve and if the chosen measure actually measures what we are trying to achieve. Estimating how performance will generalize We now assume that we have decided on the measure of predictive performance we want to use. Our goal now is to estimate how our model will perform on new or unseen examples. Our estimation is typically based on computing the measure on a finite number of test observations. For example, the average of squared errors if we are interested in estimating the MSE. The first and most important thing is that the model’s performance on test cases is just an estimate of its expected performance on new or unseen examples. Indeed, what we are faced with is a proper statistical estimation problem. The average performance on the test data will differ from the unknown expected performance that we are trying to estimate due to bias and variability. We have already pointed out one possible source of bias - if we test on the data we trained on, we introduce a positive bias - we will on average overestimate the model’s quality! This is very undesirable, so as a rule, we should never test on data that the model was trained on! In practice, we, once we select the best training algorithm, use that algorithm on all available data to produce the model that will be used in production. Note that such testing would have a negative bias - we will on average underestimate the model’s performance, because it uses less training data in testing than it will use in production. However, such negative bias typically of less concern than a positive bias. Even if we don’t have a background in statistics, we will agree that the more (independent) test observations we have, the less variability there will be in our estimate of the model’s performance. The same applies to model training - the more training examples we have, the less variability there will be in our estimates of model parameters. That puts us in a predicament - if we use more data for testing, we’ll get a better estimate of performance but for a model that is more likely to be far away from the model trained on all data. On the other hand, if we use more data for training, we’ll have better estimates of model parameters, but more uncertainty in how well the model will perform. So, unless we have large amounts of data, using a training-test split is not the best idea. Instead, we should use cross-validation - a process that fully utilizes all data for training and testing, without violating the rule of not using the same data for training and testing. The key idea is to randomly partition the data into \\(k\\)-folds of approximately equal size (hence \\(k\\)-fold cross-validation) and repeating \\(k\\)-times, once for each fold, the process of training on all but that fold and testing on that fold. Finally, we average the errors across all observations. Because there is some randomness involved in partitioning the data, it is important to use the same partitions for all models, so that we do not introduce unnecessary variability. Practitioners often also repeat \\(k\\)-fold cross-validation \\(m\\) times for different partitions, to further reduce variability. Non-standard cases: There are cases where cross-validation does not apply. In particular, when the observations are not independent, as is the case with temporal or spatial data. The latter is beyond the scope of this chapter, while in the former case we can rely on a rolling window approach - we test on every observation but for each observation we train only on observations that precede it temporally. 10.4 Practical considerations Although predictive model quality is typically synonymous with its prediction quality, there are many other practical things that we may need to consider when choosing the best model for the task at hand: Model interpretability: Being able to understand or explain what the model does is sometimes as important if not more important than its predictive quality. For example, in sensitive high-risk domains where the decision maker and person responsible is a non-technical non-data science expert, such as medicine or business. In such cases we might prefer decision tables, decision trees or linear models over neural networks or ensembles. Model explanation is an important and active subfield of machine learning and statistics (see Further reading and references). Computational complexity, computation time: We will often have limitations on how long we can run the training or prediction part of our model. Sometimes the issues can be resolved with more resources, if available. However, if the computational complexity of the algorithm prevents us from scaling to the size of our problem, we must either make a compromise (reduce the dimensionality of the problem) or choose another training algorithm. Cost of implementation and maintenance: The practical cost of implementing a novel method or integrating an unsupported third-party implementation is very high. When predictive quality is crucial and every improvement results in substantial gain, for example, algorithmic trading, the benefits of better predictive quality will outweigh the practical costs of implementing, integrating and/or buying the software. In most other cases, the benefits of having a well-supported and easy-to-use library outweigh some loss in predictive quality, transparency and/or speed compared to the state-of-the-art new but more difficult to use method. The above also applies when we are developing new methodology. We should always consider what is important to our user base and if we want others to use our methods, we must make them as easy to use as possible. 10.5 Putting it all together with R We’ll apply what we have learned to an interesting dataset from the paper Stengos, T., &amp; Zacharias, E. (2006). Intertemporal pricing and price discrimination: a semiparametric hedonic analysis of the personal computer market. Journal of Applied Econometrics, 21(3), 371-386. The dataset contains PC price and specifications from the 90: df &lt;- read.csv(&quot;./data/computers.csv&quot;, stringsAsFactors = T)[,1:7] summary(df) ## price speed hd ram ## Min. : 949 Min. : 25.00 Min. : 80.0 Min. : 2.000 ## 1st Qu.:1794 1st Qu.: 33.00 1st Qu.: 214.0 1st Qu.: 4.000 ## Median :2144 Median : 50.00 Median : 340.0 Median : 8.000 ## Mean :2220 Mean : 52.01 Mean : 416.6 Mean : 8.287 ## 3rd Qu.:2595 3rd Qu.: 66.00 3rd Qu.: 528.0 3rd Qu.: 8.000 ## Max. :5399 Max. :100.00 Max. :2100.0 Max. :32.000 ## screen cd multi ## Min. :14.00 no :3351 no :5386 ## 1st Qu.:14.00 yes:2908 yes: 873 ## Median :14.00 ## Mean :14.61 ## 3rd Qu.:15.00 ## Max. :17.00 Although this is probably not true, we’ll assume, for the purposes of this analysis, that all observations are independent observations from the distribution of computers in that time period. The goal is to predict PC price and the input variables we’ll use are …the clock speed (in MHz) of the 486 processor, the size of the hard drive (in MB), the size of the RAM (in MB), the size of the screen (in inches), the presence of a CD-ROM, the presence of a multi-media kit that includes speakers and a sound card... Now we are ready to proceed. First, before we do anything, we will set aside 1000 randomly chosen observations. These will be used as the final test set after we have decided on the best model. We can afford this, because we have a lot of observations (relative to the size of the input space). We will also recode the yes/no variables as binary: set.seed(0) idx &lt;- sample(1:nrow(df), 1000, rep = F) df$cd &lt;- as.numeric(df$cd) df$multi &lt;- as.numeric(df$multi) df.main &lt;- df[-idx,] df.test &lt;- df[ idx,] Next, we identify that the target variable is numerical, so we are dealing with a regression problem. Some input variables are numerical and some are binary, which means that we have a standard regression problem. To minimize the amount of effort, we’ll utilize the caret package for R. The package includes many functions and interfaces over 100 different classification and regression models. We will consider 3 different regression training algorithms - linear regression (no tunable parameters), a stochastic gradient boosting (we’ll run it over a range of values suggested by caret) and k-nearest neighbors (we’ll test it across several manually selected values of \\(k\\)). We’ll choose (root) mean squared error as our measure of quality, because our objective is to get the best predictor of the mean. And we’ll estimate how the models’ errors will generalize using 10 repetitions of 10-fold cross-validation: library(caret) ## Loading required package: lattice # setting-up the cross validation fitControl &lt;- trainControl(## 10-fold CV method = &quot;repeatedcv&quot;, number = 10, repeats = 10) all &lt;- NULL # gradient boosting set.seed(0) # set seed to get the same folds for all models fit_gb &lt;- train(price ~ ., data = df.main, method = &quot;gbm&quot;, trControl = fitControl, tuneLength = 2, verbose = F) tmp &lt;- fit_gb$results for (i in 1:nrow(tmp)) { all &lt;- rbind(all, data.frame(Model = paste(&quot;gb[&quot;, paste(tmp[i,1:4], collapse = &quot;,&quot;), &quot;]&quot;, sep = &quot;,&quot;), RMSE = tmp$RMSE[i], SD = tmp$RMSESD[i])) } # k-nn set.seed(0) grid &lt;- expand.grid(k= c(1,2,3,5,7,10)) fit_nn &lt;- train(price ~ ., data = df.main, method = &quot;knn&quot;, trControl = fitControl, tuneGrid = grid) tmp &lt;- fit_nn$results for (i in 1:nrow(tmp)) { all &lt;- rbind(all, data.frame(Model = paste(&quot;nn[&quot;, paste(tmp[i,1], collapse = &quot;,&quot;), &quot;]&quot;, sep = &quot;,&quot;), RMSE = tmp$RMSE[i], SD = tmp$RMSESD[i])) } # linear regression set.seed(0) fit_lm &lt;- train(price ~ ., data = df.main, method = &quot;lm&quot;, trControl = fitControl) tmp &lt;- fit_lm$results all &lt;- rbind(all, data.frame(Model = &quot;lm&quot;, RMSE = tmp$RMSE, SD = tmp$RMSESD)) Now we can summarize the results, ordered by predictive quality: all[order(all$RMSE),] ## Model RMSE SD ## 5 nn[,1,] 292.6001 15.38815 ## 6 nn[,2,] 299.6639 14.76814 ## 7 nn[,3,] 306.2012 14.37591 ## 8 nn[,5,] 319.4619 15.12788 ## 9 nn[,7,] 328.1494 15.58832 ## 10 nn[,10,] 338.8986 15.71369 ## 4 gb[,0.1,2,10,100,] 373.1816 16.46013 ## 2 gb[,0.1,2,10,50,] 397.5234 18.73404 ## 3 gb[,0.1,1,10,100,] 405.8163 18.78367 ## 1 gb[,0.1,1,10,50,] 422.1510 18.91172 ## 11 lm 422.7391 20.96899 When interpreting results of estimation, we should always keep in mind that estimates are not the underlying truth - there is a degree of uncertainty (variability) associated with them and possibly bias. Differences between models that are of the same size as the uncertainty or smaller are likely due to chance and not due to actual differences. The field of statistics offers many different approaches to estimation and quantifying uncertainty. Here, we’ll use a relatively simple but often sufficient approach - the standard deviation of the root mean squared error (SD). That is, how RMSE deviates across different folds of cross-validation. This deviation captures the variability due to models being trained and tested on different data and the variability of cross-validation itself, because we made 10 repetitions of 10-fold cross-validation. 1-nearest neighbors is estimated to be the best model. However, the differences are small (relative to the scale of random noise, measured by SD), so based solely on these numbers, we can’t be certain that k = 2 or k = 3 are not better models. We can, however, with reasonable certainty conclude that the best nearest neighbors model is better than the best gradient boosting model which is in turn better than linear regression. Once we are happy with our choice of model, we proceed with the final check - how the model performs on the held-out final test data. This final test is extremely important as it will catch any errors that we might have made in the estimation procedure. It will also catch any overfitting due to so-called user’s degrees of freedom. In practice, we typically don’t run the estimation procedure only once. Instead, it is an iterative process - if we aren’t happy with the results, we try to improve them by trying another model, setting different tuning parameters, inspecting and possiblty transforming the data, etc. With this we create a feedback loop where models and transformations with good cross-validation results are more likely to be included in the next iteration. This creates a positive bias - the estimate for the model we finally select is more likely than not to overestimate its predictive quality. In practice, this bias will typically be very small, unless we have very little data and we many different models. However, if we can afford a separate final test set, we should always use it. The problem from the previous paragraph can be illustrated by an extreme example. Imagine a training algorithm that creates a model that makes completely random predictions (its prediction for the same observation will always be the same, but the value is chosen at random for every observation). If we created a sufficient number of such models, we will eventually get a model that gets an estimated error of close to 0, even if we use cross-validation. But if we took that model and evaluated it on a held-out test set or in production, it would perform as poorly as random predictions. In practice, the problem is not nearly as extreme, but we should always be aware of the fact that cross-validation helps us detect models that overfit, but we can still overfit by repeating the process or by testing many different models. Finally, we test the estimated best (k = 1) nearest neighbors model on the held-out final test set: p &lt;- predict(fit_nn$finalModel, newdata = df.test[,-1]) round(sqrt(mean((p - df.test$price)^2)), 2) ## [1] 296.16 This estimate is even better than the one on the main data we used for cross-validation-based estimation. However, the difference is withing the above estimates of standard error of our measure of predictive quality, so there is no reason for concern. At this point we can conclude that our previous estimate of the generalized performance of the 1-nearest neighbor model is valid. We can also conclude that for this data predicting the price of the most similar PC performs at least as well as all the other models that we have tested. 10.6 Putting it all together with Python A substantial part of our predictive modelling scenario can be simplified by utilizing existing implementations of models and model selection procedures. The most commonly used Python libraries and packages are pandas, numpy, scikit-learn and matplotlib. You can read more about them also in Chapter 1. Below we show how to use the same type of the algorithms as we used in the R example above in Python. First we load and overview the dataset: import pandas as pd df = pd.read_csv(&quot;../data/computers.csv&quot;) df = df.filter(items=df.columns[0:7]) df.describe(include=&#39;all&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } price speed hd ram screen cd multi count 6259.00 6259.00 6259.00 6259.00 6259.00 6259 6259 unique NaN NaN NaN NaN NaN 2 2 top NaN NaN NaN NaN NaN no no freq NaN NaN NaN NaN NaN 3351 5386 mean 2219.58 52.01 416.60 8.29 14.61 NaN NaN std 580.80 21.16 258.55 5.63 0.91 NaN NaN min 949.00 25.00 80.00 2.00 14.00 NaN NaN 25% 1794.00 33.00 214.00 4.00 14.00 NaN NaN 50% 2144.00 50.00 340.00 8.00 14.00 NaN NaN 75% 2595.00 66.00 528.00 8.00 15.00 NaN NaN max 5399.00 100.00 2100.00 32.00 17.00 NaN NaN As some of the parameters are categorical (i.e. cd and multi), we need to add parameter to include all of them in to the output. Still, the values are still treated similar to numeric types, so if we want to see counts for each categor value, we need to output them separately: df[&#39;cd&#39;].value_counts() ## no 3351 ## yes 2908 ## Name: cd, dtype: int6 df[&#39;multi&#39;].value_counts() ## no 5386 ## yes 873 ## Name: multi, dtype: int64 Now we recode yes/no variables to binary and prepare the training and testing dataset. df[&#39;cd&#39;] = pd.Categorical(df[&#39;cd&#39;], categories=df[&#39;cd&#39;].unique()).codes df[&#39;multi&#39;] = pd.Categorical(df[&#39;multi&#39;], categories=df[&#39;multi&#39;].unique()).codes test_set = df.sample(n=1000, random_state=0) main_set = df.drop(test_set.index) Scikit-learn library works on numpy-based objects, so we need to transform the pandas data frames into numpy arrays (alternatively there also exist transformers to do this automatically by scikit-learn): X = main_set.loc[:, main_set.columns != &#39;price&#39;].to_numpy() y = main_set[&#39;price&#39;].to_numpy() X_test = test_set.loc[:, test_set.columns != &#39;price&#39;].to_numpy() y_test = test_set[&#39;price&#39;].to_numpy() Scikit-learn does not include RMSE scoring function by default, that is why we need to prepare it first. We also prepare a fold iterator for cross validation techniques and en empty data frame to store results. import numpy as np from sklearn.model_selection import RepeatedKFold, cross_validate, GridSearchCV, RandomizedSearchCV from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error, make_scorer from sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LinearRegression def mean_squared_error_(ground_truth, predictions): return np.sqrt(mean_squared_error(ground_truth, predictions)) RMSE = make_scorer(mean_squared_error_, greater_is_better=False) rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=0) all = pd.DataFrame() The R gradient boosting training was tuning the parameters n.trees and interaction.depth, while shrinkage and n.minobsinnode were left fixed. To tell the algorithm which parameters to tune, we need to prepare distributions to sample from and then run the cross validation method for a selected classifier. # gradient boosting clf = GradientBoostingRegressor() param_distributions = {&#39;learning_rate&#39;: [0.1], &#39;min_samples_leaf&#39;: [10], &#39;n_estimators&#39;: [50, 100], &#39;max_depth&#39;: [1, 2]} tmp = RandomizedSearchCV(clf, param_distributions, n_iter=4, scoring=RMSE, cv=rkf, random_state=0).fit(X, y) for param_id in range(0, len(tmp.cv_results_[&#39;params&#39;])): tmp_test_scores = [] for split_scores_id in range(0, 100): tmp_test_scores.append(tmp.cv_results_[f&#39;split{split_scores_id}_test_score&#39;][param_id]) params = tmp.cv_results_[&quot;params&quot;][param_id] params_list = [params[&#39;learning_rate&#39;], params[&#39;max_depth&#39;], params[&#39;min_samples_leaf&#39;], params[&#39;n_estimators&#39;]] all = all.append({&#39;Model&#39;: f&#39;gb[,{&quot;,&quot;.join(str(x) for x in params_list)},]&#39;, &#39;RMSE&#39;: -1*min(tmp_test_scores), &quot;SD&quot;: np.std(tmp_test_scores)}, ignore_index=True) # k-nn parameters = {&#39;n_neighbors&#39;: [1, 2, 3, 5, 7, 10]} tmp = GridSearchCV(KNeighborsRegressor(), parameters, cv=rkf, scoring=RMSE).fit(X, y) bestNNModel = tmp.best_estimator_ for param_id in range(0, len(tmp.cv_results_[&#39;params&#39;])): tmp_test_scores = [] for split_scores_id in range(0, 100): tmp_test_scores.append(tmp.cv_results_[f&#39;split{split_scores_id}_test_score&#39;][param_id]) all = all.append({&#39;Model&#39;: f&#39;nn[,{tmp.cv_results_[&quot;params&quot;][param_id][&quot;n_neighbors&quot;]},]&#39;, &#39;RMSE&#39;: -1*min(tmp_test_scores), &quot;SD&quot;: np.std(tmp_test_scores)}, ignore_index=True) # linear regression clf = LinearRegression() tmp = cross_validate(clf, X, y, cv=rkf, scoring=RMSE) all = all.append({&#39;Model&#39;: &#39;lm&#39;, &#39;RMSE&#39;: -1*min(tmp[&#39;test_score&#39;]), &quot;SD&quot;: np.std(tmp[&#39;test_score&#39;])}, ignore_index=True) Now we can summarize the results, ordered by predictive quality: all.sort_values(&#39;RMSE&#39;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Model RMSE SD 7 nn[,5,] 370.264823 14.634447 8 nn[,7,] 371.455809 15.485513 6 nn[,3,] 371.771094 14.135298 9 nn[,10,] 380.230663 16.406543 5 nn[,2,] 384.245843 15.759044 3 gb[,0.1,2,10,100,] 407.898817 15.229875 4 nn[,1,] 423.044016 18.534263 2 gb[,0.1,2,10,50,] 433.891617 17.185479 1 gb[,0.1,1,10,100,] 447.622174 17.133322 0 gb[,0.1,1,10,50,] 465.734108 17.891289 10 lm 474.878803 17.341011 Finally, we test the estimated best (k = 5) nearest neighbors model on the held-out final test set: y_predicted = bestNNModel.predict(X_test) round(np.sqrt(np.mean((y_predicted - y_test)**2)), 2) ## 320.45 10.7 Further reading and references An excellent introductory book that provides a balanced treatment of the machine learning, statistical, and computational aspects of predictive modeling: James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer. Predictive and/or computational performance of models can often be improved by selecting a subset of features or transforming the feature space. We covered the latter at least to some extent in Chapter 8, a good introduction to the former is: Guyon, I., &amp; Elisseeff, A. (2003). An introduction to variable and feature selection. Journal of machine learning research, 3(Mar), 1157-1182. Interpretability of models is an important aspect. There exist model-specific methods, but better-suited for practical purposesa re model-agnostic (or black-box) approaches that can be applied to all models (or a general class of models, such as all classifiers). A recent survey of this very active field in machine learning and statistics: Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., &amp; Pedreschi, D. (2019). A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5), 93. 10.8 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Recognize and describe the case of predictive modelling that applies to the task at hand. Use software tools to perform predictive modelling in standard cases. Understand the problem of overfitting and how to avoid it. Understand the advantages and disadvantages of common of predictive modelling algorithms. Choose the best training algorithm for the task at hand, depending on all the dimensions of quality that apply. 10.9 Practice problems Use cross-validation to find the optimal power of the polynomial regression on the toy 2-variable dataset from the beginning of this chapter. On the toy 2-variable dataset compare polynomial regression with a regression tree, random forests, a feed-forward neural network and another traning algorithm of choice. Comment on the models’ predictive performance, computation time (training and prediction) and interpretability. Consider 4 different classification training algorithms: logistic regression, k-nearest neighbors, a feed-forward neural network (with 1 hidden layer with no more than 10 neurons) and a decision tree (for example CART). Create 4 different binary classification datasets, each with two input variables \\(x \\in [0,1]\\) and \\(y \\in [0,1]\\) and a binary outcome \\(c \\in \\{0,1\\}\\), such that each of the four training algorithms will produce the best model for one of the datasets. That is, so that each dataset is most suited to the inductive bias of one of the training algorithms. "],
["missing-data.html", "Chapter 11 Dealing with missing data 11.1 The severity of the missing data problem 11.2 Visually exploring missingness 11.3 Deletion methods 11.4 Imputation methods 11.5 Summary 11.6 Further reading and references 11.7 Learning outcomes 11.8 Practice problems", " Chapter 11 Dealing with missing data In all of our data analyses so far we implicitly assumed that we don’t have any missing values in our data. In practice, that is often not the case. While some statistical and machine learning methods work with missing data, many commonly used methods can’t, so it is important to learn how to deal with missing values. In this chapter we will discuss a few of the most common methods. 11.1 The severity of the missing data problem If we judge by the imputation or data removing methods that are most commonly used in practice, we might conclude that missing data is a relatively simple problem that is secondary to the inference, predictive modelling, etc. that are the primary goal of our analysis. Unfortunately, that is not the case. Dealing with missing values is very challenging, often in itself a modelling problem. Three classes of missingness The choice of an appropriate method is inseparable from our understanding of or assumptions about the process that generated the missing values (the missingness mechanism). Based on the characteristics of this process we typically characterize the missing data problem as one of these three cases: MCAR (Missing Completely At Random): Whether or not a value is missing is independent of both the observed values and the missing (unobserved) values. For example, if we had temperature measuring devices at different locations and they occassionally and random intervals stopped working. Or, in surveys, where respondents don’t respond with a certain probability, independent of the characteristics that we are surveying. MAR (Missing At Random): Whether or not a value is missing is independent of the missing (unobserved) values but depends on the observed values. That is, there is a pattern to how the values are missing, but we could fully explain that pattern given only the observed data. For example, if our temperature measuring devices stopped working more often in certain locations than in others. Or, in surveys, if women are less likely to report their weight than men. MNAR (Missing Not At Random): Whether or not a value is missing also depends on the missing (unobserved) values in a way that can’t be explained by the observed values. That is, there is a pattern to how the values are missing, but we wouldn’t be able to fully explain it without observing the values that are missing. For example, if our temperature measuring device had a tendency to stop working when the temperature is very low. Or, in surveys, if a person was less likely to report their salary if their salary was high. Determining the missingness mechanism Every variable in our data might have a different missingness mechanism. So, how do we determine whether it is MCAR, MAR, or MNAR? Showing with a reasonable degree of certainty that the mechanism is not MCAR is equivalent to showing that the missingness (whether or not a value is missing) can be predicted from observed values. That is, it is a prediction problem and it is sufficient to show one way that missingness can be predicted. On the other hand, it is infeasible to show that the mechanism is MCAR, because that would require us to show that there is no way of predicting missingness from observed values. We can, however, rule out certain kinds of dependency (for example, linear dependency). For MNAR, the situation is even worse. In general, it is impossible to determine from the data the relationship between missingness and the value that is missing, because we don’t know what is missing. That is, unless we are able to somehow measure the values that are missing, we won’t be able to determine whether or not the missingness regime is MNAR. Getting our hands on the missing values, however, is in most cases impossible or infeasible. To summarize, we’ll often be able to show that our missingness regime is not MCAR and never that it is MCAR. Subsequently, we’ll often know that the missingness regime is at least MAR, but we’ll rarely be able to determine whether it is MAR or MNAR, unless we can get our hands on the missing data. Therefore, it becomes very important to utilize not only data but also domain-specific background knowledge, when applicable. In particular, known relationships between the variables in our data and what caused the values to be missing. Causes for missing data Understanding the cause for missing data can often help us identify the missingness mechanism and avoid introducing a bias. In general, we can split the causes into two classes: intentional or unintentional. Intentionally or systematically missing data are missing by design. For example: patients that did not experience pain were not asked to rate their pain, a patient that has not been discharged from the hospital doesn’t have a ‘days spent in hospital care’ data point (although we can infer a lower bound from the day of arrival) and a particular prediction algorithm’s performance was measured on datasets with fewer than 100 variables, due to its time complexity and some measurements were not made because it was too costly to make all of them. Unintentionally missing data were not planned. For example: data missing due to measurement error, a subject skipping a survey question, a patient prematurely dropping out of a study and other reasons not planned by and outside the control of the data collector. There is no general rule and further conclusions can only be made on a case-by-case basis, using domain specific knowledge. Measurement error can range from completly random to completely not-at-random, such a temperature sensor breaking down at high temperatures. Subjects typically do not drop out of studies at random, but that is also a possiblity. When not all measurements are made to reduce cost, they are often omitted completely at random, but sometimes a different design is used. Introducing bias The discussion in this section is very relevant for the remainder of this chapter. It will help us understand the limitations of the methods for dealing with missing data and the consequences of making the wrong choice. We will discuss two of the most common types of methods - deletion methods and imputation methods. Both types of methods can introduce a bias into our data. That is, they can result in data that are no longer a completely representative (simple random) sample from the process that generated the data and that is the focus of our analysis. This can have an adverse effect on our analysis, regardless of whether we are doing inference, clustering or prediction. Note that the extent of the bias depends the amount of missingness and the strength of the dependencies, so there is no general rule. Again, we must deal with such problems on a case-by-case basis. 11.2 Visually exploring missingness Throughout the remainder of the chapter we will be using a dataset from Pampaka, M., Hutcheson, G., &amp; Williams, J. (2016). Handling missing data: analysis of a challenging data set using multiple imputation. International Journal of Research &amp; Method in Education, 39(1), 19-37. as an illustrative example: dat &lt;- read.csv(&quot;./data/imputationDATASET.csv&quot;, stringsAsFactors = T) summary(dat) ## GCSEgrade Course Disposition SelfEfficacy ## 1_intC :201 ASTrad:1014 Min. :-6.840 Min. :-4.5500 ## 2_higherC: 80 UoM : 360 1st Qu.: 0.500 1st Qu.: 0.0000 ## 3_intB :354 Median : 2.830 Median : 0.5800 ## 4_higherB:353 Mean : 2.176 Mean : 0.7243 ## 5_A :294 3rd Qu.: 4.320 3rd Qu.: 1.3500 ## 6_A* : 92 Max. : 4.320 Max. : 6.0500 ## ## dropOUTretrieved dropOUToriginal Gender Language EMA ## no :780 no :289 female:513 BILINGUAL:273 no :617 ## yes:594 yes :206 male :860 ENGLISH :985 yes :698 ## NA&#39;s:879 NA&#39;s : 1 OTHER : 70 NA&#39;s: 59 ## NA&#39;s : 46 ## ## ## ## Ethnicity LPN HEFCE uniFAM ## ASIAN :251 NO :907 Min. :1.000 firstgeneration:497 ## BLACK :105 YES :302 1st Qu.:2.000 parents :342 ## CHINESE: 17 NA&#39;s:165 Median :3.000 siblings :499 ## OTHER : 58 Mean :3.843 NA&#39;s : 36 ## WHITE :449 3rd Qu.:6.000 ## NA&#39;s :494 Max. :6.000 ## NA&#39;s :167 In this analysis, the authors were interested in modelling dropout from mathematics courses (dropOUToriginal) which had a lot of missing values. The actual values were later retreived (dropOUTretrieved), but we will only use these to verify the quality of our methods. The other variables include their previous GCSE qualifications results in mathematics (GCSEgrade), type of course (Course), their disposition to study mathematics at a higher level (Disposition), their self-efficacy rating (SelfEfficacy), gender, language, whether the student was holding an Educational Maintenance Allowance (EMA), ethnicity, whether the student was from Low Participation Neighbourhood (LPN), socio-economic status (HEFCE) and whether the student was not first generation at HE (uniFAM). If our dataset does not have too many variables, a visual summary such as this one can be very effective at revealing the extend and patterns of missingness in our data: library(naniar) vis_miss(dat, warn_large_data = F) Another useful visualization is the frequency of different patterns of missingness: library(UpSetR) gg_miss_upset(dat) From the above plot we can see that dropout and ethnicity have the most missing values and that the other most common pattern is for both of them to be missing. If missingness of dropout and ethnicity were independent, we should expect about 64% of missing ethnicity rows to also have missing dropout. That is very likely not the case, as only about a half have missing dropout. That is, there is likely a pattern to missingness. 11.3 Deletion methods In this section we will cover deletion methods - dealing with missing values by deleting the columns and/or rows that have them. 11.3.1 Column deletion The most simple way of dealing with missing values is to delete the column that holds the variable. That is, to remove that variable from all observations. Such complete removal of potentially useful data is never the optimal choice in terms of available information. And it is not even an option if we are interested in doing inference with/about that variable. In our illustrative example we would, for example, be able to use only the GCSEgrade, Course, Disposition and SelfEfficacy variables to predict dropout. However, if we have reason to believe that the variable is not practically important for our analysis and/or the fraction of missing values is so large, it might be more easier to just remove the variable. That is, in some cases, the effort of dealing with missing values might outweigh the potential benefits of the extra information. By removing a variable we of course completely remove all information about the dependency of its (missing) values with other variables. So, regardless of what the missingness mechanism is, we will not introduce a bias to our analyses by removing a column. 11.3.2 Row deletion The most common deletion approach is to delete all rows with missing values. If we are not interested in those rows directly and there are not many such rows than this is a completely viable alternative to imputation methods. However, we must be aware that unless the missingness mechanism is MCAR, we will be introducing a bias into our data. Let’s estimate the dropout rate from the data that are available. That is, we remove all rows where we don’t know whether the student dropped out or not: stderr &lt;- function(x) { N &lt;- sum(!is.na(x), na.rm = T) sd(x, na.rm = T) / sqrt(N) } x &lt;- dat$dropOUToriginal == &quot;yes&quot; mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- stderr(x) cat(sprintf(&quot;%.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## 0.42 +/- 0.022 (n = 495) So, this suggest that we can be reasonably certain that the dropout rate is around 42%. However, this is only valid if the observed values are also a representative sample from our original data (we’re assuming that the original data are a representative sample from the population). That will always be the case if the missingness mechanism is MCAR - if missingness is completely random then row deletion will also be completely random. As we stated at the begining, there is no way of proving that the mechanism is MCAR, however, we can do our best to show that it is not and then account for it. We’ll do the latter in the imputation methods section. Here, we just show how incorrect conclusions could be reached if we rely only on row deletion. Observe that the relative frequency of missing values depends on ethnicity: x &lt;- is.na(dat$dropOUToriginal)[dat$Ethnicity == &quot;WHITE&quot;] mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- sd(x, na.rm = T) / sqrt(N) cat(sprintf(&quot;==WHITE: %.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## ==WHITE: 0.61 +/- 0.023 (n = 449) x &lt;- is.na(dat$dropOUToriginal)[dat$Ethnicity != &quot;WHITE&quot;] mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- stderr(x) cat(sprintf(&quot;!=WHITE: %.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## !=WHITE: 0.70 +/- 0.022 (n = 431) So, we can be reasonably certain that the missingness mechanism is at least MAR! Now, if those of white ethnicity would be more (less) prone to dropping out, our estimate of 42% from above would underestimate (overestimate) true dropout rate! Before using row deletion, we should check if dropout rate depends on ethnicity: x &lt;- (dat$dropOUToriginal == &quot;yes&quot;)[dat$Ethnicity == &quot;WHITE&quot;] mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- stderr(x) cat(sprintf(&quot;==WHITE: %.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## ==WHITE: 0.45 +/- 0.037 (n = 177) x &lt;- (dat$dropOUToriginal == &quot;yes&quot;)[dat$Ethnicity != &quot;WHITE&quot;] mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- stderr(x) cat(sprintf(&quot;!=WHITE: %.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## !=WHITE: 0.43 +/- 0.044 (n = 130) In this case, there is no discernible difference. So, the bias that we might be introducing, is small. Of course, we should do this check for every variable that can reasonably be dependent on/off missigness of dropout! If the missingness mechanism is MNAR, there is not much we can do. For example, it is not unreasonable to assume that people that dropped out might be less likely to report the dropout information. However, we could not be able to verify if that is the case unless we gathered some of the missing data. Our illustrative example is one of those rare exceptions - the authors gathered the true values hidden behind the missing values, so we can compare: x &lt;- dat$dropOUToriginal == &quot;yes&quot; mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- stderr(x) cat(sprintf(&quot;ORIGINAL: %.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## ORIGINAL: 0.42 +/- 0.022 (n = 495) x &lt;- dat[is.na(dat$dropOUToriginal),]$dropOUTretrieved == &quot;yes&quot; mu &lt;- mean(x, na.rm = T) N &lt;- sum(!is.na(x), na.rm = T) SE &lt;- stderr(x) cat(sprintf(&quot; MISSING: %.2f +/- %.3f (n = %d)\\n&quot;, mu, SE, N)) ## MISSING: 0.44 +/- 0.017 (n = 879) There is no discernible difference, so, for practical purposes, we’ll conclude that the missingness mechanism is MAR and not MNAR. To summarize this section: row deletion can be useful, but if we are deleting many rows, we should always check if the missingness mechanism is at least MAR and check if we need to account for dependencies between other variables and missingness. 11.3.3 Pairwise deletion Pairwise deletion is a special case of row deletion where we delete only the rows with missing values in the variables of interest for a particular part of the analysis. For example, suppose that we are interested in whether or not there is a dependency between dropout, language and ethnicity. First, lets investigate this on the subset of the three columns where we also drop all rows with missing values. We’ll use a Chi-squared test (see Basic summarization chapter for details) to test for dependency between the categorical variables: tmp &lt;- dat[,c(6,8,10)] tmp &lt;- tmp[complete.cases(tmp),] cat(sprintf(&quot;Rows before = %d, rows after deletion = %d\\n&quot;, nrow(dat), nrow(tmp))) ## Rows before = 1374, rows after deletion = 291 chisq.test(table(tmp$dropOUToriginal, tmp$Language), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: table(tmp$dropOUToriginal, tmp$Language) ## X-squared = 4.6808, df = NA, p-value = 0.08496 chisq.test(table(tmp$dropOUToriginal, tmp$Ethnicity), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: table(tmp$dropOUToriginal, tmp$Ethnicity) ## X-squared = 1.4154, df = NA, p-value = 0.8326 chisq.test(table(tmp$Ethnicity, tmp$Language), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: table(tmp$Ethnicity, tmp$Language) ## X-squared = 123.1, df = NA, p-value = 0.0004998 So, if we dropped all rows with missing values in any of the three columns, we’d be left with 291 observations. If we instead remove for each pair only the rows that are missing one of those values, we get 474, 307 and 841 observations, respectively. If we had used a 5% risk level, we’d in fact reject the null hypothesis with pairwise deletion but not with row deletion: tmp &lt;- dat[,c(6,8)] tmp &lt;- tmp[complete.cases(tmp),] cat(sprintf(&quot;Rows before = %d, rows after deletion = %d\\n&quot;, nrow(dat), nrow(tmp))) ## Rows before = 1374, rows after deletion = 474 chisq.test(table(tmp$dropOUToriginal, tmp$Language), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: table(tmp$dropOUToriginal, tmp$Language) ## X-squared = 5.8535, df = NA, p-value = 0.04998 tmp &lt;- dat[,c(6,10)] tmp &lt;- tmp[complete.cases(tmp),] cat(sprintf(&quot;Rows before = %d, rows after deletion = %d\\n&quot;, nrow(dat), nrow(tmp))) ## Rows before = 1374, rows after deletion = 307 chisq.test(table(tmp$dropOUToriginal, tmp$Ethnicity), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: table(tmp$dropOUToriginal, tmp$Ethnicity) ## X-squared = 0.72928, df = NA, p-value = 0.9585 tmp &lt;- dat[,c(8,10)] tmp &lt;- tmp[complete.cases(tmp),] cat(sprintf(&quot;Rows before = %d, rows after deletion = %d\\n&quot;, nrow(dat), nrow(tmp))) ## Rows before = 1374, rows after deletion = 841 chisq.test(table(tmp$Ethnicity, tmp$Language), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 ## replicates) ## ## data: table(tmp$Ethnicity, tmp$Language) ## X-squared = 285.5, df = NA, p-value = 0.0004998 So, pairwise deletion allows us to use more data. Note, however, that all that we’ve discussed for row deletion also applies to pairwise deletion. And with pairwise deletion, we have to deal with each sub-analysis separately. 11.4 Imputation methods The alternative to deleting rows or columns with missing values is to replace missing data with a value. When a single value is used, we refer to it as single imputation, when multiple values (a distribution) is used to replace a missing value, we refer to it as multiple imputation. In this section we’ll cover some of the most common variants of both single and multiple imputation. 11.4.1 Single imputation with the mean Single imputation with the mean is a simple procedure of replacing all missing values with the mean of the observed values. Imputation with the mean has the advantage of not introducing a bias automatically - changing all missing values to the mean will not change the sample mean across all variable values. However, similar to deletion methods, if the missingness mechanism is not MCAR, we risk introducing a bias. Replacing missing values with the median or mode (with categorical variables mode is the only reasonable central tendency) can be more representative of the underlying distribution, but we must be aware that it automatically introduces a bias. That is, we are inserting values that are not the mean and therefore by definition changing the expectation and introducing a bias. Note that for categorical variables we can also treat missing values as a new separate category and procede with inference and prediction - if we specify the model correctly, such data will be sufficient to account for bias, even if the missingness mechanism is MAR. A major disadvantage of single imputation is that it typically reduces the variance of the variable. Subsequently, it also reduces any covariance of that variable with other variables. Observe how the variability of the HEFCE variable and its correlation with SelfEfficacy reduce after mean imputation: x &lt;- dat$HEFCE mu &lt;- mean(x, na.rm = T) sd &lt;- sd(x, na.rm = T) pr &lt;- cor(x, dat$SelfEfficacy, use = &quot;complete.obs&quot;) cat(sprintf(&quot;row deletion : mu = %.3f sd = %.3f cor = %.3f \\n&quot;, mu, sd, pr)) ## row deletion : mu = 3.843 sd = 2.011 cor = -0.064 x &lt;- dat$HEFCE x[is.na(x)] &lt;- mean(x, na.rm = T) mu &lt;- mean(x, na.rm = T) sd &lt;- sd(x, na.rm = T) pr &lt;- cor(x, dat$SelfEfficacy, use = &quot;complete.obs&quot;) cat(sprintf(&quot;mean imputation: mu = %.3f sd = %.3f cor = %.3f \\n&quot;, mu, sd, pr)) ## mean imputation: mu = 3.843 sd = 1.885 cor = -0.060 Such underestimation of variability would also transfer to underestimating uncertainty in our predictions or parameter estimates, which is a serious problem if the goal is to understand uncertainty and not just point estimates. 11.4.2 Single imputation with prediction A generalization of imputation with the mean is to predict the missing value using all other available data. This translates to a fully fledged predictive modelling problem with all the complexity of model selection, etc. We demonstrate the approach by using logistic regression to predict missing dropout values from all variables without missing values: tmp &lt;- dat[,c(1:4, 6)] mod &lt;- glm(dropOUToriginal ~ ., tmp, family = &quot;binomial&quot;) # prediction model idx &lt;- is.na(dat$dropOUToriginal) pre &lt;- ifelse(predict(mod, newdata = tmp, type = &quot;response&quot;) &gt; 0.5, &quot;yes&quot;, &quot;no&quot;) # predicted values tmp$dropOUToriginal &lt;- ifelse(idx, pre, dat$dropOUToriginal) # accuracy on missing values, compared to relative freq. y &lt;- tmp$dropOUToriginal[idx] == dat$dropOUTretrieved[idx] cat(sprintf(&quot;%.2f +/- %.3f\\n&quot;, mean(y), stderr(y))) ## 0.65 +/- 0.016 round(mean(dat$dropOUTretrieved[idx] == &quot;no&quot;), 2) ## [1] 0.56 # The predictions are not very accurate, but still above the relative frequency of the mode of dropout (no), which is 0.56. We could take the extra step of imputing the remaining variables with mean/mode and using them in the model as well: mode &lt;- function(x) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } impute &lt;- function(x) { if (is.numeric(x)) { return (ifelse(is.na(x), mean(x, na.rm = T), x)) } else { return(ifelse(is.na(x), mode(x[!is.na(x)]), x)) } } tmp &lt;- dat[,-5] for (i in 1:ncol(tmp)) { if (names(tmp)[i] != &quot;dropOUToriginal&quot;) tmp[,i] &lt;- impute(tmp[,i]) } mod &lt;- glm(dropOUToriginal ~ ., tmp, family = &quot;binomial&quot;) # prediction model idx &lt;- is.na(dat$dropOUToriginal) pre &lt;- ifelse(predict(mod, newdata = tmp, type = &quot;response&quot;) &gt; 0.5, &quot;yes&quot;, &quot;no&quot;) # predicted values tmp$dropOUToriginal &lt;- ifelse(idx, pre, dat$dropOUToriginal) # accuracy on missing values, compared to relative freq. y &lt;- tmp$dropOUToriginal[idx] == dat$dropOUTretrieved[idx] cat(sprintf(&quot;%.2f +/- %.3f\\n&quot;, mean(y), stderr(y))) ## 0.67 +/- 0.016 Using the other variables did not lead to an improvement in accuracy. Either there is nothing more we can extract from the given variables or we would need to pick a better model or engineer better input variables. The advantage of a predictive approach over imputation with mean is that, if we manage to capture the relationships in the data, imputation with a predictive model can account for bias caused by a MAR missingness regime! However, as with any single imputation method, we will still underestimate variability. In order to fully capture variability, we must use a distribution or multiple values - multiple imputation. 11.4.3 Multiple imputation The imputation with prediction example from the previous section illustrates how imputation is in itself a prediction problem. Even more, it is potentially a set of many prediction problems, because we have to account for every possible combination of missing values when predicting the missing value of one variable. In the previous example we circumvented that problem by using imputation with mean, but that is not an ideal solution as it underestimates variability. To solve the problem in general, we would have to specify a generative model for all the variables. That is, to specify the data generation process in a way that can be used to generate values for missing data regardless of the pattern of missingness for that observation. This also solves the variability underestimation problem of single imputation - if we have a distribution over all possible missing values, we have a full characterization of the uncertainty or we can generate many datasets which as a whole capture that uncertainty (hence the term multiple imputation) Specifying a full generative model is the most difficult of all modelling problems. A common approach is to use a multivariate normal distribution, because under that modelling assumption all the conditional distributions are also multivariate normal and it is relatively easy to derive their parameters. In R, we can find it implemented in the amelia package. However, this approach can be used out-of-the-box only if all of our data are numerical. A popular alternative is multiple imputation with chained equations (MICE): First, we specify the type of model we want to use for each type of variable - packages that implement MICE often come with pre-specified models, such as if numerical, use linear regression, if categorical, use logistic regression, etc. Initially we replace all missing values with some simple procedure, such as mean imputation. We then cycle through all the variables and for each variable use its model from (1) to predict the values of its missing values from all the other variables. After we complete this step, all data that were initially missing values have received new values. We never change the observed values. We repeat (3) for several iterations. We do this to forget where we started so that the imputed values reflect the relationships in the data and not the initial, possibly very poor imputation. This generates one imputed dataset. We repeate steps (2-4) several times to generate multiple imputed datasets. Any further analysis we do should be based on all those datasets, because they capture the variability in the missing data - our uncertainty about what the missing data values might be. Those familiar with Gibbs sampling, a popular inference algorithm in Bayesian statistics, will quickly recognize that MICE is based on the same ideas. A full generative model can be specified by specifying just the full conditional distributions (a distribution of a variable with all other variables known) and we can use Markov Chain Monte Carlo (in this case Gibbs sampling) to generate samples from the posterior of that model. Implementations of MICE typically do all the work for us. We only need to specify the number of imputed datasets we need. We’ll use the R package mice to create 50 imputed datasets for our data. We then use logistic regression to predict dropout for each of those datasets and combine the predictions of all the models into a final prediction of dropout. First, we impute the predictor variables: tmp &lt;- dat[,-c(5,6)] tgt &lt;- dat$dropOUToriginal # multiple imputation library(mice) ## ## Attaching package: &#39;mice&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind # Uncomment the lines below to generate tmp data first time #imputed_tmp &lt;- mice(tmp, m = 50, seed = 0) #saveRDS(imputed_tmp, &quot;./data/imputed_tmp.rds&quot;) imputed_tmp &lt;- readRDS(&quot;./data/imputed_tmp.rds&quot;) # we load the precomputed data to save time We can inspect which types of models were used: print(imputed_tmp$method) ## GCSEgrade Course Disposition SelfEfficacy Gender Language ## &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;logreg&quot; &quot;polyreg&quot; ## EMA Ethnicity LPN HEFCE uniFAM ## &quot;logreg&quot; &quot;polyreg&quot; &quot;logreg&quot; &quot;pmm&quot; &quot;polyreg&quot; Now we iterate through all the imputed datasets, predict outcome and pool the predictions: pre &lt;- rep(0, length(x)) for (i in 1:50) { df &lt;- complete(imputed_tmp, 1) df$tgt &lt;- tgt # add dropout mod &lt;- glm(tgt ~ ., df, family = &quot;binomial&quot;) idx &lt;- is.na(dat$dropOUToriginal) pre &lt;- pre + predict(mod, newdata = df, type = &quot;response&quot;) / 50 } pre &lt;- ifelse(pre &gt; 0.5, &quot;yes&quot;, &quot;no&quot;) # predicted values tmp$tgt &lt;- ifelse(idx, pre, dat$dropOUToriginal) # accuracy on missing values, compared to relative freq. y &lt;- tmp$tgt[idx] == dat$dropOUTretrieved[idx] cat(sprintf(&quot;%.2f +/- %.3f\\n&quot;, mean(y), stderr(y))) ## 0.64 +/- 0.016 In this case MICE doesn’t seem to have helped. In general, MICE is a very robust procedure and it performs really well in many different situations. The main issue with MICE is that it is computationally very intensive. 11.5 Summary If we are to take away one thing from this chapter it should be that dealing with missing data is not an easy problem. Data will rarely be missing in a nice (completely random) way, so if we want to resort to more simple removal or imputation techniques, we must put reasonable effort into determining whether or not the dependencies between observed data and missingness are a cause for concern. If strong dependencies exist and/or there is a lot of data missing, the missing data problem becomes a prediction problem (or a series of prediction problems). We should also be aware of the possibility that missingness depends on the missing value in a way that can’t be explained by observed variables. This can also cause bias in our analyses and we will not be able to detect it unless we get our hands on some of the missing values. Note that in this chapter we focused on standard tabular data. There are many other types of data, such as time-series data, spatial data, images, sound, graphs, etc. The basic principles remain unchanged. We must be aware of the missingness mechanism and introducing bias. We can deal with missing values either by removing observations/variables or by imputing them. However, different, sometimes additional models and techniques will be appropriate. For example, temporal and spatial data lend themselves to interpolation of missing values. 11.6 Further reading and references A gentle introduction from a practitioners perspective: Blankers, M., Koeter, M. W., &amp; Schippers, G. M. (2010). Missing data approaches in eHealth research: simulation study and a tutorial for nonmathematically inclined researchers. Journal of medical Internet research, 12(5), e54. A great book on basic and some advance techniques: Allison, P. D. (2001). Missing data (Vol. 136). Sage publications. Another great book with many examples and case-studies: Van Buuren, S. (2018). Flexible imputation of missing data. Chapman and Hall/CRC. Understanding multiple imputation with chained equations (in R): Buuren, S. V., &amp; Groothuis-Oudshoorn, K. (2010). mice: Multivariate imputation by chained equations in R. Journal of statistical software, 1-68. When doing missing data value handling and prediction separately, we should be aware that certain types of prediction models might work better with certain methods for handling missing values. A paper that illustrate this: Yadav, M. L., &amp; Roychoudhury, B. (2018). Handling missing values: A study of popular imputation packages in R. Knowledge-Based Systems, 160, 104-118. 11.7 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Analyze the severity of their missing data problem. Recognize when a technique is appropriate and what are its limitations. 11.8 Practice problems We prepared a subset of the Football Manager Players dataset that contains 1000 randomly selected 19-year old players, their playing position, height, and 10 other attributes (football-manager-complete.rds). We then introduced missing values to this data based on various missingness mechanisms (football-manager-missing.rds) and in a way that could also have a reasonable practical explanation. Your task is to: Identify and, as much as it is possible, characterize missingness mechanisms and patterns in the data. Use information from (a) to impute missing values in all numerical variables (all variables except PositionsDesc, which can be ignored throughout this problem). Estimate the mean of all numerical variables. Only once you have completed your analysis, use football-manager-complete.rds to compare your estimated means with column averages on the complete dataset. Discuss which mechanisms you correctly detected and characterized. Discuss the discrepancies between your estimates and actual column means - was there anything you could have done better? In the same setting as (1) try to predict players’ playing position (PositionsDesc) using football-manager-missing.rds. once you have completed your analysis, use football-manager-complete.rds to evaluate your model on the observations that had missing playing position. Discuss what you could have done better. Aggarwal, Charu C, and ChengXiang Zhai. 2012. Mining Text Data. Springer Science &amp; Business Media. Christopher, D Manning, Raghavan Prabhakar, and Schacetzel Hinrich. 2008. “Introduction to Information Retrieval.” An Introduction to Information Retrieval 151 (177): 5. Deitel, Paul, Harvey Deitel, and Abbey Deitel. 2011. Internet &amp; World Wide Web: How to Program. 5th ed. Pearson. https://www.amazon.com/Internet-World-Wide-Web-Program/dp/0132151006. Ferrara, Emilio, Pasquale De Meo, Giacomo Fiumara, and Robert Baumgartner. 2014. “Web Data Extraction, Applications and Techniques: A Survey.” Knowledge-Based Systems 70: 301–23. Internet World Stats. 2017. “Usage and Population Statistics.” 2017. http://www.internetworldstats.com/stats.htm. Kushmerick, Nicholas, Daniel S Weld, and Robert Doorenbos. 1997. “Wrapper Induction for Information Extraction.” Liu, Bing. 2011. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data. 2nd ed. Springer Publishing Company, Incorporated. Noyes, Katherine. 2008. “Docker: A ’Shipping Container’ for Linux Code.” https://www.linux.com/news/docker-shipping-container-linux-code. Tai, Kuo-Chung. 1979. “The Tree-to-Tree Correction Problem.” J. ACM 26 (3): 422–33. https://doi.org/10.1145/322139.322143. "]
]
